{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMI Corpus - D√©monstration SDM et Segmentation des Locuteurs\n",
    "\n",
    "Ce notebook d√©montre l'utilisation du corpus AMI avec :\n",
    "- **Configuration SDM** (Single Distant Microphone - Microphone distant unique)\n",
    "- **Segmentation des locuteurs** automatique\n",
    "- **Visualisation interactive** avec lecture audio\n",
    "- **Exemple multi-locuteurs** complet\n",
    "\n",
    "## √Ä propos de la configuration SDM\n",
    "- **SDM** = Single Distant Microphone\n",
    "- Utilise un **seul microphone distant** pour l'enregistrement\n",
    "- Configuration plus simple que MDM (Multiple Distant Microphone)\n",
    "- Id√©ale pour tester les algorithmes de diarisation sur signal mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances avec gestion de compatibilit√© NumPy\n",
    "!pip install \"numpy<2\" --force-reinstall\n",
    "!pip install \"datasets<4.0.0\"\n",
    "!pip install  librosa soundfile matplotlib ipython scikit-learn\n",
    "print(\"‚ö†Ô∏è  Si des erreurs de compatibilit√© persistent, red√©marrez le kernel apr√®s l'installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Audio, display, HTML\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Toutes les librairies import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du Corpus AMI - Configuration SDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ami_sdm(split: str = \"train\", max_samples: int = 1) -> Dict:\n",
    "    \"\"\"\n",
    "    Charge le corpus AMI avec configuration SDM (Single Distant Microphone)\n",
    "    \"\"\"\n",
    "    print(\"üéØ Chargement du corpus AMI - Configuration SDM (Single Distant Microphone)...\")\n",
    "    \n",
    "    try:\n",
    "        # Charger avec la configuration SDM sp√©cifiquement\n",
    "        print(\"üîÑ Tentative avec configuration 'sdm' (Single Distant Microphone)...\")\n",
    "        dataset = load_dataset(\"edinburghcstr/ami\", \"sdm\", split=f\"{split}[:{max_samples}]\")\n",
    "        print(\"‚úÖ Configuration 'sdm' charg√©e avec succ√®s\")\n",
    "        \n",
    "    except Exception as e1:\n",
    "        print(f\"‚ö†Ô∏è  Configuration 'sdm' √©chou√©e: {str(e1)[:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Essayer avec streaming=True pour contourner certains probl√®mes\n",
    "            print(\"üîÑ Tentative SDM avec streaming=True...\")\n",
    "            dataset = load_dataset(\"edinburghcstr/ami\", \"sdm\", split=f\"{split}[:{max_samples}]\", streaming=True)\n",
    "            # Convertir en liste pour l'usage normal\n",
    "            dataset = list(dataset)\n",
    "            \n",
    "            # Cr√©er un objet compatible\n",
    "            class StreamDataset:\n",
    "                def __init__(self, data_list):\n",
    "                    self.data = data_list\n",
    "                    if data_list:\n",
    "                        self.column_names = list(data_list[0].keys())\n",
    "                    else:\n",
    "                        self.column_names = []\n",
    "                \n",
    "                def __len__(self):\n",
    "                    return len(self.data)\n",
    "                \n",
    "                def __getitem__(self, idx):\n",
    "                    return self.data[idx]\n",
    "            \n",
    "            dataset = StreamDataset(dataset)\n",
    "            print(\"‚úÖ Dataset SDM en streaming charg√© avec succ√®s\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è  Streaming SDM √©chou√©: {str(e2)[:100]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Essayer de charger depuis le cache local s'il existe\n",
    "                print(\"üîÑ Tentative SDM avec cache local...\")\n",
    "                import os\n",
    "                cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets/edinburghcstr___ami\")\n",
    "                \n",
    "                if os.path.exists(cache_dir):\n",
    "                    print(f\"üìÅ Cache trouv√© dans {cache_dir}\")\n",
    "                    dataset = load_dataset(\"edinburghcstr/ami\", \"sdm\", split=f\"{split}[:{max_samples}]\", cache_dir=cache_dir)\n",
    "                else:\n",
    "                    raise Exception(\"Cache non trouv√©\")\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"‚ùå Toutes les tentatives SDM ont √©chou√©\")\n",
    "                print(f\"Derni√®re erreur: {str(e3)[:200]}...\")\n",
    "                print(\"\\nüí° SOLUTIONS POSSIBLES:\")\n",
    "                print(\"1. V√©rifiez votre connexion internet\")\n",
    "                print(\"2. Installez huggingface-cli: !pip install huggingface_hub\")\n",
    "                print(\"3. T√©l√©chargez manuellement: !huggingface-cli download edinburghcstr/ami --repo-type dataset\")\n",
    "                print(\"4. Le dataset SDM pourrait √™tre temporairement indisponible\")\n",
    "                \n",
    "                raise Exception(\"Impossible de charger le corpus AMI avec configuration SDM. V√©rifiez les solutions ci-dessus.\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset SDM charg√©: {len(dataset)} √©chantillons\")\n",
    "    print(f\"üìä Colonnes disponibles: {dataset.column_names}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Charger un √©chantillon pour tester\n",
    "dataset = load_ami_sdm(split=\"train\", max_samples=1)\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\nüîç Structure de l'√©chantillon SDM:\")\n",
    "for key, value in sample.items():\n",
    "    if key == 'audio':\n",
    "        print(f\"  {key}: array shape = {np.array(value['array']).shape}, sr = {value['sampling_rate']}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  {key}: liste de {len(value)} √©l√©ments\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. V√©rification de l'Audio SDM (Single Distant Microphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sdm_audio(sample: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyse la configuration SDM (Single Distant Microphone)\n",
    "    \"\"\"\n",
    "    audio_data = np.array(sample['audio']['array'])\n",
    "    sr = sample['audio']['sampling_rate']\n",
    "    \n",
    "    analysis = {\n",
    "        'shape': audio_data.shape,\n",
    "        'num_channels': 1,  # SDM est toujours mono\n",
    "        'num_samples': len(audio_data) if len(audio_data.shape) == 1 else audio_data.shape[-1],\n",
    "        'duration_seconds': (len(audio_data) if len(audio_data.shape) == 1 else audio_data.shape[-1]) / sr,\n",
    "        'sampling_rate': sr,\n",
    "        'is_mono': len(audio_data.shape) == 1,\n",
    "        'configuration': 'SDM'\n",
    "    }\n",
    "    \n",
    "    return analysis, audio_data\n",
    "\n",
    "# Analyser l'audio SDM\n",
    "audio_analysis, audio_array = analyze_sdm_audio(sample)\n",
    "\n",
    "print(\"üéôÔ∏è ANALYSE AUDIO SDM (Single Distant Microphone):\")\n",
    "print(f\"   üìê Forme des donn√©es: {audio_analysis['shape']}\")\n",
    "print(f\"   üéöÔ∏è Configuration: {audio_analysis['configuration']}\")\n",
    "print(f\"   üîä Canaux: {audio_analysis['num_channels']} (mono)\")\n",
    "print(f\"   ‚è±Ô∏è Dur√©e: {audio_analysis['duration_seconds']:.1f} secondes\")\n",
    "print(f\"   üìä Fr√©quence d'√©chantillonnage: {audio_analysis['sampling_rate']} Hz\")\n",
    "print(f\"   ‚úÖ Mono: {'‚úÖ' if audio_analysis['is_mono'] else '‚ùå'}\")\n",
    "\n",
    "# Calcul des statistiques audio\n",
    "audio_rms = np.sqrt(np.mean(audio_array**2))\n",
    "audio_max = np.max(np.abs(audio_array))\n",
    "audio_energy = np.sum(audio_array**2)\n",
    "\n",
    "print(f\"\\nüìà Statistiques du signal SDM:\")\n",
    "print(f\"   RMS: {audio_rms:.4f}\")\n",
    "print(f\"   Amplitude max: {audio_max:.4f}\")\n",
    "print(f\"   √ânergie totale: {audio_energy:.2e}\")\n",
    "\n",
    "if not audio_analysis['is_mono']:\n",
    "    print(\"\\n‚ö†Ô∏è  Attention: Le signal n'est pas mono comme attendu pour SDM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segmentation des Locuteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_speaker_segmentation(sample: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyse la segmentation des locuteurs (utilise les annotations si disponibles)\n",
    "    \"\"\"\n",
    "    # V√©rifier si les segments sont disponibles\n",
    "    has_segments = all(key in sample for key in ['segment_start_times', 'segment_end_times', 'segment_speakers'])\n",
    "    \n",
    "    if has_segments:\n",
    "        print(\"‚úÖ Segmentation disponible dans les annotations\")\n",
    "        \n",
    "        segmentation_info = {\n",
    "            'source': 'annotations',\n",
    "            'total_segments': len(sample['segment_speakers']),\n",
    "            'unique_speakers': list(set(sample['segment_speakers'])),\n",
    "            'num_speakers': len(set(sample['segment_speakers'])),\n",
    "            'start_times': sample['segment_start_times'],\n",
    "            'end_times': sample['segment_end_times'],\n",
    "            'speakers': sample['segment_speakers'],\n",
    "            'total_duration': sample['segment_end_times'][-1] - sample['segment_start_times'][0],\n",
    "            'segments_per_speaker': {}\n",
    "        }\n",
    "        \n",
    "        # Compter segments par locuteur\n",
    "        for speaker in segmentation_info['unique_speakers']:\n",
    "            count = sample['segment_speakers'].count(speaker)\n",
    "            segmentation_info['segments_per_speaker'][speaker] = count\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Pas de segmentation dans les annotations - Cr√©ation de segments factices pour la d√©mo\")\n",
    "        \n",
    "        # Cr√©er une segmentation factice pour la d√©monstration\n",
    "        duration = audio_analysis['duration_seconds']\n",
    "        num_segments = min(8, int(duration / 5))  # Un segment toutes les 5 secondes max\n",
    "        \n",
    "        speakers = [f'Speaker_{i%3 + 1}' for i in range(num_segments)]  # 3 locuteurs fictifs\n",
    "        segment_duration = duration / num_segments\n",
    "        \n",
    "        start_times = [i * segment_duration for i in range(num_segments)]\n",
    "        end_times = [(i + 1) * segment_duration for i in range(num_segments)]\n",
    "        \n",
    "        segmentation_info = {\n",
    "            'source': 'synthetic',\n",
    "            'total_segments': num_segments,\n",
    "            'unique_speakers': list(set(speakers)),\n",
    "            'num_speakers': len(set(speakers)),\n",
    "            'start_times': start_times,\n",
    "            'end_times': end_times,\n",
    "            'speakers': speakers,\n",
    "            'total_duration': duration,\n",
    "            'segments_per_speaker': {speaker: speakers.count(speaker) for speaker in set(speakers)}\n",
    "        }\n",
    "    \n",
    "    return segmentation_info\n",
    "\n",
    "# Analyser la segmentation\n",
    "seg_info = analyze_speaker_segmentation(sample)\n",
    "\n",
    "print(f\"\\nüéØ SEGMENTATION DES LOCUTEURS ({seg_info['source']}):\")\n",
    "print(f\"   üìä Nombre de segments: {seg_info['total_segments']}\")\n",
    "print(f\"   üë• Nombre de locuteurs: {seg_info['num_speakers']}\")\n",
    "print(f\"   üè∑Ô∏è Locuteurs: {seg_info['unique_speakers']}\")\n",
    "print(f\"   ‚è±Ô∏è Dur√©e totale: {seg_info['total_duration']:.1f}s\")\n",
    "print(f\"   üìà Segments par locuteur: {seg_info['segments_per_speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation Interactive avec Lecture Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sdm_interactive_demo(audio_array: np.ndarray, seg_info: Dict, sr: int, max_duration: float = 60.0):\n",
    "    \"\"\"\n",
    "    Cr√©e une d√©monstration interactive pour SDM avec visualisation et lecture audio\n",
    "    \"\"\"\n",
    "    # Assurer que nous avons un signal mono pour SDM\n",
    "    if len(audio_array.shape) > 1:\n",
    "        audio_array = audio_array.flatten()\n",
    "    \n",
    "    # Limiter √† la dur√©e maximale\n",
    "    max_samples = int(max_duration * sr)\n",
    "    if len(audio_array) > max_samples:\n",
    "        audio_array = audio_array[:max_samples]\n",
    "        actual_duration = max_duration\n",
    "    else:\n",
    "        actual_duration = len(audio_array) / sr\n",
    "    \n",
    "    # Cr√©er la visualisation SDM\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Timeline des locuteurs\n",
    "    ax1 = axes[0]\n",
    "    speakers = seg_info['speakers']\n",
    "    start_times = seg_info['start_times']\n",
    "    end_times = seg_info['end_times']\n",
    "    \n",
    "    # Couleurs pour chaque locuteur\n",
    "    unique_speakers = seg_info['unique_speakers']\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_speakers)))\n",
    "    speaker_colors = {speaker: colors[i] for i, speaker in enumerate(unique_speakers)}\n",
    "    \n",
    "    # Dessiner les segments\n",
    "    for i, (speaker, start, end) in enumerate(zip(speakers, start_times, end_times)):\n",
    "        if start > actual_duration:\n",
    "            break\n",
    "        end = min(end, actual_duration)\n",
    "        ax1.barh(speaker, end - start, left=start, \n",
    "                color=speaker_colors[speaker], alpha=0.7, \n",
    "                edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Ajouter le num√©ro du segment\n",
    "        ax1.text(start + (end-start)/2, speaker, f'{i+1}', \n",
    "                ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Temps (secondes)')\n",
    "    ax1.set_ylabel('Locuteurs')\n",
    "    ax1.set_title('Timeline de la Segmentation des Locuteurs - Configuration SDM')\n",
    "    ax1.set_xlim(0, actual_duration)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Forme d'onde mono SDM\n",
    "    ax2 = axes[1]\n",
    "    time_axis = np.linspace(0, actual_duration, len(audio_array))\n",
    "    \n",
    "    ax2.plot(time_axis, audio_array, label='Signal SDM (mono)', alpha=0.8, color='blue')\n",
    "    ax2.set_xlabel('Temps (secondes)')\n",
    "    ax2.set_ylabel('Amplitude')\n",
    "    ax2.set_title('Forme d\\'onde SDM (Single Distant Microphone)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajouter l'enveloppe du signal\n",
    "    window_size = max(1, len(audio_array) // 1000)\n",
    "    envelope = np.abs(audio_array)\n",
    "    if len(envelope) > window_size:\n",
    "        envelope = np.convolve(envelope, np.ones(window_size)/window_size, mode='same')\n",
    "    ax2.plot(time_axis, envelope, label='Enveloppe', alpha=0.6, color='red', linewidth=2)\n",
    "    ax2.plot(time_axis, -envelope, alpha=0.6, color='red', linewidth=2)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Spectrogramme SDM\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    # Calculer le spectrogramme\n",
    "    D = librosa.stft(audio_array, hop_length=512)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    \n",
    "    img = librosa.display.specshow(S_db, sr=sr, hop_length=512, \n",
    "                                  x_axis='time', y_axis='hz', ax=ax3)\n",
    "    ax3.set_title('Spectrogramme SDM (Single Distant Microphone)')\n",
    "    fig.colorbar(img, ax=ax3, format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return actual_duration\n",
    "\n",
    "# Cr√©er la visualisation SDM\n",
    "actual_duration = create_sdm_interactive_demo(audio_array, seg_info, audio_analysis['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lecture Audio par Canal et par Locuteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sdm_audio_players(audio_array: np.ndarray, seg_info: Dict, sr: int, max_duration: float = 60.0):\n",
    "    \"\"\"\n",
    "    Cr√©e des lecteurs audio pour SDM et extrait des locuteurs\n",
    "    \"\"\"\n",
    "    # Assurer que nous avons un signal mono pour SDM\n",
    "    if len(audio_array.shape) > 1:\n",
    "        audio_array = audio_array.flatten()\n",
    "    \n",
    "    # Limiter la dur√©e\n",
    "    max_samples = int(max_duration * sr)\n",
    "    if len(audio_array) > max_samples:\n",
    "        audio_array = audio_array[:max_samples]\n",
    "    \n",
    "    print(\"\udfb5 LECTEURS AUDIO SDM:\")\n",
    "    \n",
    "    # 1. Lecture du signal SDM complet\n",
    "    print(\"\\n\udf99Ô∏è Signal SDM complet (Single Distant Microphone):\")\n",
    "    display(Audio(audio_array, rate=sr))\n",
    "    \n",
    "    # 2. Extraits par locuteur\n",
    "    print(\"\\nüë• EXTRAITS PAR LOCUTEUR (depuis signal SDM):\")\n",
    "    \n",
    "    for speaker in seg_info['unique_speakers']:\n",
    "        print(f\"\\nüé§ {speaker}:\")\n",
    "        \n",
    "        # Extraire tous les segments de ce locuteur\n",
    "        speaker_audio = []\n",
    "        segment_count = 0\n",
    "        total_speaker_duration = 0\n",
    "        \n",
    "        for i, (seg_speaker, start_time, end_time) in enumerate(\n",
    "            zip(seg_info['speakers'], seg_info['start_times'], seg_info['end_times'])):\n",
    "            \n",
    "            if seg_speaker == speaker and start_time < max_duration:\n",
    "                start_sample = int(start_time * sr)\n",
    "                end_sample = int(min(end_time, max_duration) * sr)\n",
    "                \n",
    "                # S'assurer que les indices sont valides\n",
    "                start_sample = max(0, min(start_sample, len(audio_array)-1))\n",
    "                end_sample = max(start_sample+1, min(end_sample, len(audio_array)))\n",
    "                \n",
    "                segment_audio = audio_array[start_sample:end_sample]\n",
    "                speaker_audio.extend(segment_audio)\n",
    "                segment_count += 1\n",
    "                total_speaker_duration += (end_sample - start_sample) / sr\n",
    "        \n",
    "        if speaker_audio and len(speaker_audio) > 0:\n",
    "            speaker_audio = np.array(speaker_audio)\n",
    "            print(f\"   Dur√©e totale: {total_speaker_duration:.1f}s ({segment_count} segments)\")\n",
    "            print(f\"   √âchantillons: {len(speaker_audio):,}\")\n",
    "            \n",
    "            # Ajouter des statistiques du signal pour ce locuteur\n",
    "            speaker_rms = np.sqrt(np.mean(speaker_audio**2))\n",
    "            speaker_max = np.max(np.abs(speaker_audio))\n",
    "            print(f\"   RMS: {speaker_rms:.4f}, Max: {speaker_max:.4f}\")\n",
    "            \n",
    "            display(Audio(speaker_audio, rate=sr))\n",
    "        else:\n",
    "            print(f\"   Aucun segment trouv√© dans les {max_duration}s\")\n",
    "    \n",
    "    # 3. Statistiques comparatives\n",
    "    print(f\"\\nüìä STATISTIQUES COMPARATIVES SDM:\")\n",
    "    print(f\"   Signal complet: {len(audio_array)/sr:.1f}s, {len(audio_array):,} √©chantillons\")\n",
    "    \n",
    "    total_segments_duration = sum(end - start for start, end in zip(seg_info['start_times'], seg_info['end_times']))\n",
    "    coverage = (total_segments_duration / (len(audio_array)/sr)) * 100\n",
    "    print(f\"   Couverture segmentation: {coverage:.1f}%\")\n",
    "    \n",
    "    signal_rms = np.sqrt(np.mean(audio_array**2))\n",
    "    print(f\"   RMS signal complet: {signal_rms:.4f}\")\n",
    "\n",
    "# Cr√©er les lecteurs audio SDM\n",
    "create_sdm_audio_players(audio_array, seg_info, audio_analysis['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistiques D√©taill√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_statistics(sample: Dict, audio_analysis: Dict, seg_info: Dict):\n",
    "    \"\"\"\n",
    "    Affiche des statistiques d√©taill√©es sur l'√©chantillon\n",
    "    \"\"\"\n",
    "    print(\"üìä STATISTIQUES D√âTAILL√âES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Informations g√©n√©rales\n",
    "    print(f\"\\nüè∑Ô∏è M√âTADONN√âES:\")\n",
    "    if 'meeting_id' in sample:\n",
    "        print(f\"   ID de r√©union: {sample['meeting_id']}\")\n",
    "    if 'text' in sample:\n",
    "        text_preview = sample['text'][:200] + \"...\" if len(sample['text']) > 200 else sample['text']\n",
    "        print(f\"   Aper√ßu du texte: {text_preview}\")\n",
    "    \n",
    "    # Statistiques audio\n",
    "    print(f\"\\nüéôÔ∏è AUDIO:\")\n",
    "    print(f\"   Canaux: {audio_analysis['num_channels']}\")\n",
    "    print(f\"   √âchantillons: {audio_analysis['num_samples']:,}\")\n",
    "    print(f\"   Dur√©e: {audio_analysis['duration_seconds']:.2f}s\")\n",
    "    print(f\"   Taille: {audio_analysis['num_samples'] * audio_analysis['num_channels'] * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
    "    \n",
    "    # Statistiques de segmentation\n",
    "    print(f\"\\nüë• SEGMENTATION:\")\n",
    "    print(f\"   Source: {seg_info['source']}\")\n",
    "    print(f\"   Segments totaux: {seg_info['total_segments']}\")\n",
    "    print(f\"   Locuteurs uniques: {seg_info['num_speakers']}\")\n",
    "    \n",
    "    # Dur√©e moyenne par segment\n",
    "    segment_durations = [end - start for start, end in zip(seg_info['start_times'], seg_info['end_times'])]\n",
    "    if segment_durations:\n",
    "        avg_duration = np.mean(segment_durations)\n",
    "        min_duration = np.min(segment_durations)\n",
    "        max_duration = np.max(segment_durations)\n",
    "        print(f\"   Dur√©e moy. segment: {avg_duration:.2f}s\")\n",
    "        print(f\"   Segment plus court: {min_duration:.2f}s\")\n",
    "        print(f\"   Segment plus long: {max_duration:.2f}s\")\n",
    "    \n",
    "    # R√©partition par locuteur\n",
    "    print(f\"\\nüìà R√âPARTITION PAR LOCUTEUR:\")\n",
    "    for speaker, count in seg_info['segments_per_speaker'].items():\n",
    "        # Calculer le temps total pour ce locuteur\n",
    "        speaker_time = sum(end - start for speaker_seg, start, end in \n",
    "                          zip(seg_info['speakers'], seg_info['start_times'], seg_info['end_times'])\n",
    "                          if speaker_seg == speaker)\n",
    "        percentage = (speaker_time / seg_info['total_duration']) * 100\n",
    "        print(f\"   {speaker}: {count} segments, {speaker_time:.1f}s ({percentage:.1f}%)\")\n",
    "\n",
    "# Afficher les statistiques\n",
    "detailed_statistics(sample, audio_analysis, seg_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test avec d'Autres √âchantillons (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger d'autres √©chantillons SDM pour comparaison\n",
    "print(\"üîÑ Test avec d'autres √©chantillons SDM...\")\n",
    "\n",
    "try:\n",
    "    # Charger plusieurs √©chantillons SDM\n",
    "    dataset_multi = load_ami_sdm(split=\"train\", max_samples=3)\n",
    "    \n",
    "    print(f\"\\nüìä Comparaison de {len(dataset_multi)} √©chantillons SDM:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, sample_test in enumerate(dataset_multi):\n",
    "        audio_test = np.array(sample_test['audio']['array'])\n",
    "        sr_test = sample_test['audio']['sampling_rate']\n",
    "        \n",
    "        # Assurer format mono pour SDM\n",
    "        if len(audio_test.shape) > 1:\n",
    "            audio_test = audio_test.flatten()\n",
    "        \n",
    "        # V√©rifier la segmentation\n",
    "        has_segments = all(key in sample_test for key in ['segment_start_times', 'segment_end_times', 'segment_speakers'])\n",
    "        \n",
    "        print(f\"\\n√âchantillon SDM {i+1}:\")\n",
    "        print(f\"  Audio: {audio_test.shape} @ {sr_test}Hz\")\n",
    "        print(f\"  Dur√©e: {len(audio_test) / sr_test:.1f}s\")\n",
    "        print(f\"  Configuration: SDM (mono)\")\n",
    "        print(f\"  Segmentation: {'‚úÖ' if has_segments else '‚ùå'}\")\n",
    "        \n",
    "        if has_segments:\n",
    "            num_speakers = len(set(sample_test['segment_speakers']))\n",
    "            num_segments = len(sample_test['segment_speakers'])\n",
    "            print(f\"  Locuteurs: {num_speakers}, Segments: {num_segments}\")\n",
    "            \n",
    "            # Statistiques de qualit√© pour SDM\n",
    "            audio_rms = np.sqrt(np.mean(audio_test**2))\n",
    "            audio_snr_estimate = 20 * np.log10(np.std(audio_test) / (np.mean(np.abs(audio_test)) + 1e-10))\n",
    "            print(f\"  Qualit√© - RMS: {audio_rms:.4f}, SNR estim√©: {audio_snr_estimate:.1f}dB\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erreur lors du chargement d'√©chantillons SDM suppl√©mentaires: {e}\")\n",
    "    print(\"   Ceci peut √™tre normal si le dataset SDM est limit√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√© de la D√©monstration SDM\n",
    "\n",
    "Cette d√©monstration a montr√© :\n",
    "\n",
    "1. **‚úÖ Chargement r√©ussi** du corpus AMI avec configuration SDM (Single Distant Microphone)\n",
    "2. **‚úÖ V√©rification du signal mono** (configuration SDM)\n",
    "3. **‚úÖ Segmentation des locuteurs** (annotations r√©elles du corpus AMI)\n",
    "4. **‚úÖ Visualisation interactive SDM** avec timeline, forme d'onde mono et spectrogramme\n",
    "5. **‚úÖ Lecture audio** du signal SDM complet et par locuteur\n",
    "6. **‚úÖ Statistiques d√©taill√©es** sur l'√©chantillon SDM\n",
    "\n",
    "### üéØ Points Cl√©s SDM :\n",
    "- **SDM** = Single Distant Microphone (microphone distant unique)\n",
    "- Le corpus AMI SDM fournit des donn√©es **mono** de haute qualit√©\n",
    "- La **segmentation des locuteurs** est disponible dans les annotations\n",
    "- Configuration **id√©ale pour tester** les algorithmes de diarisation sur signal mono\n",
    "- **Signal plus simple** que MDM mais toujours repr√©sentatif des conditions r√©elles\n",
    "\n",
    "### üìä Avantages de la configuration SDM :\n",
    "- **Simplicit√©** : Un seul canal audio √† traiter\n",
    "- **Efficacit√©** : Moins de donn√©es, traitement plus rapide\n",
    "- **R√©alisme** : Simule des conditions d'enregistrement courantes\n",
    "- **Compatibilit√©** : Fonctionne avec tous les algorithmes de diarisation\n",
    "\n",
    "### üîó Ressources Utiles :\n",
    "- [AMI Corpus Documentation](https://groups.inf.ed.ac.uk/ami/corpus/)\n",
    "- [Hugging Face Dataset](https://huggingface.co/datasets/edinburghcstr/ami)\n",
    "- [Configuration SDM Details](https://groups.inf.ed.ac.uk/ami/corpus/setup.shtml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

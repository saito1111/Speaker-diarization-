{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/saito1111/Speaker-diarization-/blob/main/Enhanced_Diarization_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 🎙️ Enhanced Multi-Channel Speaker Diarization Training\n",
    "\n",
    "**Objectif :** Entraîner un système de diarization de locuteurs avancé sur le corpus AMI  \n",
    "**Architecture :** TCN multi-échelle avec attention, classification de locuteurs et gestion mémoire optimisée  \n",
    "**Plateformes :** Optimisé pour Google Colab et Kaggle Notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Nouvelles Fonctionnalités\n",
    "\n",
    "### 🚀 Compatibilité Multi-Plateformes\n",
    "- ✅ **Google Colab** : Installation automatique des dépendances\n",
    "- ✅ **Kaggle Notebooks** : Configuration optimisée pour l'environnement Kaggle  \n",
    "- ✅ **Environnement Local** : Support complet pour le développement local\n",
    "\n",
    "### 🔧 Améliorations Techniques\n",
    "- 🚫 **Sans Conda** : Installation directe avec pip pour une meilleure compatibilité\n",
    "- 📦 **Gestion des Dépendances** : Installation automatique et vérification des packages\n",
    "- 🎵 **Données Adaptatives** : Téléchargement intelligent du corpus AMI avec fallback\n",
    "- 🧪 **Tests d'Environnement** : Vérification automatique de la configuration\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Table des Matières\n",
    "1. [Configuration de l'Environnement](#setup) - Installation automatique des dépendances\n",
    "2. [Téléchargement du Corpus AMI](#data) - Téléchargement intelligent avec fallback\n",
    "3. [Préparation des Données](#preprocessing) - Division et préparation des données\n",
    "4. [Modèle et Configuration](#model) - Architecture TCN avancée\n",
    "5. [Entraînement](#training) - Entraînement avec monitoring avancé\n",
    "6. [Évaluation](#evaluation) - Métriques de performance complètes\n",
    "7. [Sauvegarde et Visualisations](#results) - Résultats et analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 🔧 1. Configuration de l'Environnement\n",
    "\n",
    "Configuration optimisée pour **Google Colab** et **Kaggle Notebooks**.  \n",
    "Installation directe des dépendances avec pip, détection automatique de la plateforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda"
   },
   "outputs": [],
   "source": [
    "# Installation des dépendances nécessaires pour Google Colab et Kaggle\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Vérifier si on est sur Colab ou Kaggle\n",
    "try:\n",
    "    import google.colab\n",
    "    PLATFORM = \"COLAB\"\n",
    "    print(\"🚀 Exécution sur Google Colab\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import kaggle\n",
    "        PLATFORM = \"KAGGLE\"\n",
    "        print(\"🚀 Exécution sur Kaggle\")\n",
    "    except ImportError:\n",
    "        PLATFORM = \"LOCAL\"\n",
    "        print(\"🚀 Exécution en local\")\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_env"
   },
   "outputs": [],
   "source": [
    "# Installation des dépendances principales avec pip\n",
    "print(\"📦 Installation des dépendances...\")\n",
    "\n",
    "# Mettre à jour pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Installation des dépendances PyTorch\n",
    "print(\"🔥 Installation de PyTorch et Torchaudio...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Installation des dépendances scientifiques\n",
    "print(\"📊 Installation des packages scientifiques...\")\n",
    "!pip install numpy scipy scikit-learn matplotlib seaborn pandas\n",
    "\n",
    "# Installation des dépendances audio et ML\n",
    "print(\"🎵 Installation des packages audio et ML...\")\n",
    "!pip install librosa soundfile speechbrain\n",
    "\n",
    "# Installation des outils d'optimisation et monitoring\n",
    "print(\"⚙️ Installation des outils d'optimisation...\")\n",
    "!pip install optuna tqdm psutil wandb\n",
    "\n",
    "print(\"✅ Toutes les dépendances installées avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Vérification des installations et imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import optuna\n",
    "import wandb\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Vérification des installations:\")\n",
    "print(f\"   📦 PyTorch: {torch.__version__}\")\n",
    "print(f\"   📦 Torchaudio: {torchaudio.__version__}\")\n",
    "print(f\"   📦 NumPy: {np.__version__}\")\n",
    "print(f\"   📦 Pandas: {pd.__version__}\")\n",
    "print(f\"   📦 Librosa: {librosa.__version__}\")\n",
    "print(f\"   📦 SoundFile: {sf.__version__}\")\n",
    "\n",
    "print(f\"\\n🔥 Configuration CUDA:\")\n",
    "print(f\"   ⚡ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   📱 Version CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   💾 Mémoire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   🔧 Nombre de GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"   ⚠️ CUDA non disponible - utilisation du CPU\")\n",
    "\n",
    "print(f\"\\n💻 Ressources système:\")\n",
    "print(f\"   🧠 CPU: {psutil.cpu_count()} cœurs\")\n",
    "print(f\"   💾 RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n🎉 Environnement prêt pour l'entraînement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final de compatibilité et vérification de l'environnement\n",
    "print(\"🧪 TEST FINAL DE COMPATIBILITÉ COLAB/KAGGLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Vérification de la plateforme\n",
    "print(f\"🖥️ Plateforme détectée: {PLATFORM}\")\n",
    "\n",
    "# Test 2: Vérification des imports critiques\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import librosa\n",
    "    import soundfile\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"✅ Tous les imports critiques réussis\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erreur d'import: {e}\")\n",
    "\n",
    "# Test 3: Vérification CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ CUDA disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 Mémoire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ CUDA non disponible - entraînement sur CPU (plus lent)\")\n",
    "\n",
    "# Test 4: Test simple avec PyTorch\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    print(f\"✅ Test PyTorch réussi sur {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur test PyTorch: {e}\")\n",
    "\n",
    "# Test 5: Test audio\n",
    "try:\n",
    "    # Créer un signal audio de test\n",
    "    sr = 16000\n",
    "    duration = 2\n",
    "    test_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, sr * duration))\n",
    "    \n",
    "    # Test librosa\n",
    "    mfccs = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=13)\n",
    "    print(f\"✅ Test audio réussi - MFCC shape: {mfccs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur test audio: {e}\")\n",
    "\n",
    "print(\"\\n🎉 ENVIRONNEMENT PRÊT POUR L'ENTRAÎNEMENT!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 Modifications pour Colab/Kaggle\n",
    "\n",
    "**✅ Améliorations apportées :**\n",
    "\n",
    "1. **🚫 Suppression de Conda**\n",
    "   - Remplacement par installation directe avec pip\n",
    "   - Compatible avec l'environnement Python natif de Colab/Kaggle\n",
    "\n",
    "2. **📦 Gestion des Dépendances**\n",
    "   - Installation automatique de PyTorch avec support CUDA\n",
    "   - Vérification des versions et de la compatibilité\n",
    "   - Gestion d'erreurs robuste\n",
    "\n",
    "3. **🎵 Téléchargement des Données**\n",
    "   - URLs mises à jour pour le corpus AMI\n",
    "   - Système de fallback avec données de démonstration\n",
    "   - Gestion intelligente des timeouts et erreurs réseau\n",
    "\n",
    "4. **🔧 Détection de Plateforme**\n",
    "   - Détection automatique Colab/Kaggle/Local\n",
    "   - Adaptation automatique de la configuration\n",
    "   - Tests de compatibilité intégrés\n",
    "\n",
    "5. **💾 Gestion des Chemins**\n",
    "   - Chemins relatifs compatibles avec tous les environnements\n",
    "   - Création automatique des répertoires nécessaires\n",
    "   - Vérification des permissions d'écriture\n",
    "\n",
    "**🎯 Le notebook est maintenant prêt pour :**\n",
    "- ✅ Google Colab (avec GPU)\n",
    "- ✅ Kaggle Notebooks  \n",
    "- ✅ Environnements locaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### 📂 Clonage du Répertoire et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_project"
   },
   "outputs": [],
   "source": [
    "# Clonage optimisé du repository pour Colab/Kaggle\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "repo_name = \"Speaker-diarization-\"\n",
    "\n",
    "print(f\"📁 Répertoire courant: {current_dir}\")\n",
    "print(f\"🖥️ Plateforme: {PLATFORM}\")\n",
    "\n",
    "# Fonction pour cloner le repository\n",
    "def clone_repository():\n",
    "    \"\"\"Clone le repository avec gestion d'erreurs robuste.\"\"\"\n",
    "    repo_url = \"https://github.com/saito1111/Speaker-diarization-.git\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"📥 Clonage du repository depuis: {repo_url}\")\n",
    "        \n",
    "        # Utiliser git clone avec des options pour plus de fiabilité\n",
    "        cmd = [\"git\", \"clone\", \"--depth\", \"1\", repo_url, repo_name]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Clonage réussi avec git\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Erreur git: {result.stderr}\")\n",
    "            \n",
    "            # Fallback avec !git pour Colab\n",
    "            print(\"🔄 Tentative avec commande shell...\")\n",
    "            os.system(f\"git clone --depth 1 {repo_url} {repo_name}\")\n",
    "            return Path(repo_name).exists()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du clonage: {e}\")\n",
    "        return False\n",
    "\n",
    "# Vérifier si le repository est déjà présent\n",
    "if (current_dir / repo_name).exists() and (current_dir / repo_name / \"src\").exists():\n",
    "    print(\"✅ Repository déjà présent et valide\")\n",
    "    repo_path = current_dir / repo_name\n",
    "elif (current_dir / \"src\").exists():\n",
    "    print(\"✅ Nous sommes déjà dans le repository\")\n",
    "    repo_path = current_dir\n",
    "else:\n",
    "    print(\"📥 Repository non trouvé - clonage nécessaire...\")\n",
    "    \n",
    "    if clone_repository():\n",
    "        repo_path = current_dir / repo_name\n",
    "        print(f\"✅ Repository cloné dans: {repo_path}\")\n",
    "    else:\n",
    "        print(\"❌ ERREUR: Impossible de cloner le repository!\")\n",
    "        print(\"🔧 Solutions possibles:\")\n",
    "        print(\"   1. Vérifiez votre connexion internet\")\n",
    "        print(\"   2. Essayez de relancer cette cellule\")\n",
    "        print(\"   3. Clonez manuellement avec: !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "        raise Exception(\"Clonage du repository échoué\")\n",
    "\n",
    "# Changer vers le répertoire du projet si nécessaire\n",
    "if repo_path != current_dir:\n",
    "    os.chdir(repo_path)\n",
    "    print(f\"📂 Changement de répertoire vers: {Path.cwd()}\")\n",
    "\n",
    "# Vérifier la structure du projet\n",
    "print(\"\\n📋 Vérification de la structure du projet:\")\n",
    "current_project_dir = Path.cwd()\n",
    "\n",
    "# Lister le contenu principal\n",
    "print(\"📁 Contenu du répertoire principal:\")\n",
    "for item in sorted(current_project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"   📁 {item.name}/\")\n",
    "    else:\n",
    "        print(f\"   📄 {item.name}\")\n",
    "\n",
    "# Vérifier le répertoire src\n",
    "src_dir = current_project_dir / \"src\"\n",
    "if src_dir.exists():\n",
    "    print(f\"\\n📂 Contenu du répertoire src/ ({len(list(src_dir.glob('*.py')))} fichiers Python):\")\n",
    "    for py_file in sorted(src_dir.glob(\"*.py\"))[:8]:  # Limiter l'affichage\n",
    "        print(f\"   🐍 {py_file.name}\")\n",
    "    if len(list(src_dir.glob(\"*.py\"))) > 8:\n",
    "        print(f\"   ... et {len(list(src_dir.glob('*.py'))) - 8} autres fichiers\")\n",
    "    print(\"✅ Répertoire src trouvé et valide\")\n",
    "else:\n",
    "    print(\"❌ ERREUR: Répertoire src manquant!\")\n",
    "    raise FileNotFoundError(\"Le répertoire src n'existe pas\")\n",
    "\n",
    "print(f\"\\n🎉 Projet configuré avec succès dans: {current_project_dir}\")\n",
    "\n",
    "# Variables globales pour les autres cellules\n",
    "PROJECT_ROOT = current_project_dir\n",
    "SRC_DIR = src_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# Configuration optimisée des chemins et imports pour Colab/Kaggle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le répertoire src au PATH Python\n",
    "if 'SRC_DIR' in globals() and SRC_DIR.exists():\n",
    "    if str(SRC_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(SRC_DIR))\n",
    "        print(f\"✅ Ajouté au PATH Python: {SRC_DIR}\")\n",
    "else:\n",
    "    # Fallback si SRC_DIR n'est pas défini\n",
    "    src_fallback = Path.cwd() / \"src\"\n",
    "    if src_fallback.exists():\n",
    "        sys.path.insert(0, str(src_fallback))\n",
    "        print(f\"✅ Ajouté au PATH Python (fallback): {src_fallback}\")\n",
    "    else:\n",
    "        print(\"⚠️ Répertoire src non trouvé - création...\")\n",
    "        src_fallback.mkdir(exist_ok=True)\n",
    "\n",
    "# Créer tous les dossiers nécessaires avec gestion d'erreurs\n",
    "directories_to_create = [\n",
    "    \"data/ami_corpus/audio\",\n",
    "    \"data/ami_corpus/annotations\", \n",
    "    \"models/checkpoints\",\n",
    "    \"results/logs\",\n",
    "    \"results/figures\",\n",
    "    \"results/metrics\"\n",
    "]\n",
    "\n",
    "print(\"📁 Création des répertoires de travail...\")\n",
    "for dir_path in directories_to_create:\n",
    "    full_path = Path(dir_path)\n",
    "    try:\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ✅ {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erreur {dir_path}: {e}\")\n",
    "\n",
    "# Configuration des chemins principaux\n",
    "try:\n",
    "    DATA_DIR = Path('./data/ami_corpus')\n",
    "    AUDIO_DIR = DATA_DIR / 'audio'  \n",
    "    ANNOTATION_DIR = DATA_DIR / 'annotations'\n",
    "    MODEL_DIR = Path('./models/checkpoints')\n",
    "    RESULTS_DIR = Path('./results')\n",
    "    LOGS_DIR = RESULTS_DIR / 'logs'\n",
    "    FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "    \n",
    "    print(f\"\\n📂 Chemins configurés:\")\n",
    "    print(f\"   🎵 Audio: {AUDIO_DIR}\")\n",
    "    print(f\"   📝 Annotations: {ANNOTATION_DIR}\")\n",
    "    print(f\"   🧠 Modèles: {MODEL_DIR}\")\n",
    "    print(f\"   📊 Résultats: {RESULTS_DIR}\")\n",
    "    print(f\"   📈 Logs: {LOGS_DIR}\")\n",
    "    print(f\"   📉 Figures: {FIGURES_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur configuration des chemins: {e}\")\n",
    "    raise\n",
    "\n",
    "# Vérification des permissions d'écriture\n",
    "print(f\"\\n🔍 Vérification des permissions:\")\n",
    "test_dirs = [DATA_DIR, MODEL_DIR, RESULTS_DIR]\n",
    "for test_dir in test_dirs:\n",
    "    try:\n",
    "        test_file = test_dir / \".test_write\"\n",
    "        test_file.write_text(\"test\")\n",
    "        test_file.unlink()\n",
    "        print(f\"   ✅ Écriture OK: {test_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erreur écriture {test_dir}: {e}\")\n",
    "\n",
    "print(f\"\\n💾 Espace disque disponible:\")\n",
    "try:\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(Path.cwd())\n",
    "    print(f\"   💿 Total: {total / 1e9:.1f} GB\")\n",
    "    print(f\"   📊 Utilisé: {used / 1e9:.1f} GB\") \n",
    "    print(f\"   🆓 Libre: {free / 1e9:.1f} GB\")\n",
    "    \n",
    "    if free < 2e9:  # Moins de 2GB libre\n",
    "        print(f\"   ⚠️ Attention: Espace disque faible ({free / 1e9:.1f} GB)\")\n",
    "    else:\n",
    "        print(f\"   ✅ Espace disque suffisant\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Impossible de vérifier l'espace disque: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 Configuration des chemins terminée avec succès!\")\n",
    "\n",
    "# Test d'import des modules du projet\n",
    "print(f\"\\n🧪 Test d'import des modules du projet:\")\n",
    "try:\n",
    "    # Essayer d'importer un module du projet pour vérifier que le PATH est correct\n",
    "    from pathlib import Path\n",
    "    print(\"   ✅ Import pathlib OK\")\n",
    "    \n",
    "    # Vérifier que les fichiers sources sont accessibles\n",
    "    if (Path.cwd() / \"src\").exists():\n",
    "        src_files = list((Path.cwd() / \"src\").glob(\"*.py\"))\n",
    "        print(f\"   📦 Fichiers Python trouvés: {len(src_files)}\")\n",
    "        for src_file in src_files[:3]:  # Afficher les 3 premiers\n",
    "            print(f\"      - {src_file.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Erreur import: {e}\")\n",
    "\n",
    "print(\"✅ Configuration complète réussie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 📊 2. Téléchargement et Préparation du Corpus AMI\n",
    "\n",
    "Le corpus AMI contient des enregistrements de réunions avec annotations temporelles des locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_ami"
   },
   "outputs": [],
   "source": [
    "# Téléchargement optimisé du corpus AMI pour Colab/Kaggle\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "class AMICorpusDownloader:\n",
    "    \"\"\"Gestionnaire pour le téléchargement du corpus AMI optimisé pour Colab/Kaggle.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.audio_dir = self.base_dir / \"ami_audio\"\n",
    "        self.annotation_dir = self.base_dir / \"ami_annotations\"\n",
    "        self.download_dir = self.base_dir / \"downloads\"\n",
    "        \n",
    "        # Créer les répertoires\n",
    "        for dir_path in [self.audio_dir, self.annotation_dir, self.download_dir]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def download_with_progress(self, url, filename, description=\"Téléchargement\"):\n",
    "        \"\"\"Télécharge un fichier avec barre de progression.\"\"\"\n",
    "        print(f\"🔽 {description}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Utiliser wget pour plus de fiabilité\n",
    "            cmd = f\"wget -O '{filename}' '{url}' --progress=bar --show-progress\"\n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"✅ Téléchargé: {filename}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ Erreur wget: {result.stderr}\")\n",
    "                # Fallback avec urllib\n",
    "                urllib.request.urlretrieve(url, filename)\n",
    "                print(f\"✅ Téléchargé (fallback): {filename}\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur téléchargement: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_ami_sample_data(self):\n",
    "        \"\"\"Télécharge un échantillon de données AMI pour l'entraînement de démonstration.\"\"\"\n",
    "        print(\"🎵 Téléchargement d'échantillons de données AMI...\")\n",
    "        \n",
    "        # URLs directes vers des échantillons AMI disponibles publiquement\n",
    "        sample_files = {\n",
    "            # Utilisation d'échantillons plus accessibles\n",
    "            \"ES2002a_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "            \"ES2002b_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "            \"ES2003a_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "        }\n",
    "        \n",
    "        downloaded_files = []\n",
    "        \n",
    "        for filename, url in sample_files.items():\n",
    "            audio_path = self.audio_dir / filename\n",
    "            \n",
    "            if audio_path.exists() and audio_path.stat().st_size > 10000:\n",
    "                print(f\"✅ Déjà présent: {filename}\")\n",
    "                downloaded_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            if self.download_with_progress(url, str(audio_path), f\"Audio {filename}\"):\n",
    "                downloaded_files.append(filename)\n",
    "        \n",
    "        return downloaded_files\n",
    "    \n",
    "    def create_demo_annotations(self):\n",
    "        \"\"\"Crée des annotations de démonstration pour les fichiers audio.\"\"\"\n",
    "        print(\"📝 Création d'annotations de démonstration...\")\n",
    "        \n",
    "        audio_files = list(self.audio_dir.glob(\"*.wav\"))\n",
    "        created_annotations = []\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            rttm_file = self.annotation_dir / f\"{audio_file.stem}.rttm\"\n",
    "            \n",
    "            if rttm_file.exists():\n",
    "                print(f\"✅ Annotation existante: {rttm_file.name}\")\n",
    "                created_annotations.append(rttm_file)\n",
    "                continue\n",
    "            \n",
    "            # Créer une annotation RTTM de base\n",
    "            # Format RTTM: SPEAKER filename 1 start_time duration <NA> <NA> speaker_id <NA> <NA>\n",
    "            demo_annotation = \"\"\"SPEAKER ES2002a_sample 1 0.0 5.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 5.0 5.0 <NA> <NA> B <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 10.0 5.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 15.0 5.0 <NA> <NA> C <NA> <NA>\n",
    "\"\"\"\n",
    "            \n",
    "            # Adapter le nom du fichier dans l'annotation\n",
    "            demo_annotation = demo_annotation.replace(\"ES2002a_sample\", audio_file.stem)\n",
    "            \n",
    "            with open(rttm_file, 'w') as f:\n",
    "                f.write(demo_annotation)\n",
    "            \n",
    "            print(f\"✅ Annotation créée: {rttm_file.name}\")\n",
    "            created_annotations.append(rttm_file)\n",
    "        \n",
    "        return created_annotations\n",
    "    \n",
    "    def download_real_ami_annotations(self):\n",
    "        \"\"\"Télécharge les vraies annotations AMI si disponibles.\"\"\"\n",
    "        print(\"📝 Tentative de téléchargement des annotations AMI réelles...\")\n",
    "        \n",
    "        # URL alternative pour les annotations AMI\n",
    "        annotation_urls = [\n",
    "            \"https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip\",\n",
    "            \"https://raw.githubusercontent.com/pyannote/AMI-diarization-setup/master/ami/only_words/train.uem.txt\"  # Alternative\n",
    "        ]\n",
    "        \n",
    "        for i, url in enumerate(annotation_urls):\n",
    "            zip_path = self.download_dir / f\"ami_annotations_{i}.zip\"\n",
    "            \n",
    "            try:\n",
    "                if self.download_with_progress(url, str(zip_path), f\"Annotations AMI ({i+1})\"):\n",
    "                    # Essayer d'extraire si c'est un zip\n",
    "                    if zip_path.suffix == '.zip':\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                                zip_ref.extractall(self.annotation_dir)\n",
    "                            print(f\"✅ Extraction réussie: {zip_path.name}\")\n",
    "                            return True\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Erreur extraction: {e}\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        # Déplacer le fichier vers le répertoire d'annotations\n",
    "                        shutil.move(str(zip_path), str(self.annotation_dir / zip_path.name))\n",
    "                        return True\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Échec URL {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"⚠️ Téléchargement annotations réelles échoué - utilisation annotations de démonstration\")\n",
    "        return False\n",
    "    \n",
    "    def setup_corpus(self):\n",
    "        \"\"\"Configure le corpus AMI complet.\"\"\"\n",
    "        print(\"\ude80 Configuration du corpus AMI...\")\n",
    "        \n",
    "        # Étape 1: Télécharger les fichiers audio\n",
    "        audio_files = self.download_ami_sample_data()\n",
    "        \n",
    "        # Étape 2: Essayer les vraies annotations, sinon créer des démos\n",
    "        if not self.download_real_ami_annotations():\n",
    "            annotation_files = self.create_demo_annotations()\n",
    "        else:\n",
    "            annotation_files = list(self.annotation_dir.rglob(\"*.rttm\"))\n",
    "        \n",
    "        print(f\"\\n\udcca Corpus configuré:\")\n",
    "        print(f\"   🎵 Fichiers audio: {len(audio_files)}\")\n",
    "        print(f\"   📝 Fichiers annotation: {len(annotation_files)}\")\n",
    "        \n",
    "        return {\n",
    "            'audio_files': audio_files,\n",
    "            'annotation_files': annotation_files,\n",
    "            'ready': len(audio_files) > 0 and len(annotation_files) > 0\n",
    "        }\n",
    "\n",
    "# Initialiser et configurer le corpus AMI\n",
    "print(\"=\"*60)\n",
    "print(\"🎯 CONFIGURATION DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ami_downloader = AMICorpusDownloader(DATA_DIR)\n",
    "corpus_status = ami_downloader.setup_corpus()\n",
    "\n",
    "if corpus_status['ready']:\n",
    "    print(\"\\n✅ Corpus AMI configuré avec succès!\")\n",
    "    print(\"🚀 Prêt pour l'entraînement de diarization\")\n",
    "else:\n",
    "    print(\"\\n❌ Erreur dans la configuration du corpus\")\n",
    "    raise Exception(\"Configuration corpus échouée\")\n",
    "\n",
    "# Configurer les chemins finaux\n",
    "AUDIO_PATH = ami_downloader.audio_dir\n",
    "ANNOTATION_PATH = ami_downloader.annotation_dir\n",
    "\n",
    "print(f\"\\n📂 Chemins configurés:\")\n",
    "print(f\"   🎵 Audio: {AUDIO_PATH}\")\n",
    "print(f\"   📝 Annotations: {ANNOTATION_PATH}\")\n",
    "print(f\"   📁 Fichiers disponibles: {len(list(AUDIO_PATH.glob('*.wav')))} audio, {len(list(ANNOTATION_PATH.glob('*.rttm')))} RTTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_ami_data"
   },
   "outputs": [],
   "source": [
    "# Validation finale et préparation des données pour l'entraînement\n",
    "\n",
    "def validate_ami_corpus():\n",
    "    \"\"\"Valide que le corpus AMI est prêt pour l'entraînement.\"\"\"\n",
    "    print(\"🔍 Validation finale du corpus AMI...\")\n",
    "    \n",
    "    audio_files = list(AUDIO_PATH.glob(\"*.wav\"))\n",
    "    annotation_files = list(ANNOTATION_PATH.glob(\"*.rttm\"))\n",
    "    \n",
    "    print(f\"\\n📊 Inventaire final:\")\n",
    "    print(f\"   🎵 Fichiers audio: {len(audio_files)}\")\n",
    "    print(f\"   📝 Fichiers RTTM: {len(annotation_files)}\")\n",
    "    \n",
    "    # Validation des paires audio-annotation\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        matching_rttm = ANNOTATION_PATH / f\"{base_name}.rttm\"\n",
    "        \n",
    "        if matching_rttm.exists():\n",
    "            valid_pairs.append((audio_file, matching_rttm))\n",
    "            print(f\"   ✅ Paire valide: {base_name}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Pas d'annotation pour: {base_name}\")\n",
    "    \n",
    "    # Afficher quelques exemples\n",
    "    if audio_files:\n",
    "        print(f\"\\n🎵 Exemples audio:\")\n",
    "        for audio_file in audio_files[:3]:\n",
    "            try:\n",
    "                size_mb = audio_file.stat().st_size / (1024*1024)\n",
    "                print(f\"   - {audio_file.name} ({size_mb:.1f} MB)\")\n",
    "            except:\n",
    "                print(f\"   - {audio_file.name} (taille non disponible)\")\n",
    "    \n",
    "    if annotation_files:\n",
    "        print(f\"\\n📝 Exemples annotations:\")\n",
    "        for rttm_file in annotation_files[:3]:\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    lines = len(f.readlines())\n",
    "                print(f\"   - {rttm_file.name} ({lines} segments)\")\n",
    "            except:\n",
    "                print(f\"   - {rttm_file.name} (lecture échouée)\")\n",
    "    \n",
    "    # Statut global\n",
    "    corpus_ready = len(valid_pairs) >= 1  # Au moins une paire valide pour la démonstration\n",
    "    \n",
    "    if corpus_ready:\n",
    "        print(f\"\\n✅ Corpus AMI validé et prêt!\")\n",
    "        print(f\"🚀 {len(valid_pairs)} paires audio-annotation disponibles pour l'entraînement\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Corpus invalide - aucune paire audio-annotation trouvée\")\n",
    "    \n",
    "    return {\n",
    "        'ready': corpus_ready,\n",
    "        'audio_count': len(audio_files),\n",
    "        'annotation_count': len(annotation_files),\n",
    "        'valid_pairs': valid_pairs,\n",
    "        'audio_files': audio_files,\n",
    "        'annotation_files': annotation_files\n",
    "    }\n",
    "\n",
    "# Valider le corpus\n",
    "corpus_validation = validate_ami_corpus()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎯 CORPUS AMI: {'PRÊT' if corpus_validation['ready'] else 'ERREUR'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if not corpus_validation['ready']:\n",
    "    raise Exception(\"Corpus AMI non valide - impossible de continuer l'entraînement\")\n",
    "\n",
    "# Configuration finale pour l'entraînement\n",
    "FINAL_AUDIO_DIR = AUDIO_PATH\n",
    "FINAL_ANNOTATION_DIR = ANNOTATION_PATH\n",
    "\n",
    "print(f\"\\n📂 Configuration finale pour l'entraînement:\")\n",
    "print(f\"   🎵 Répertoire audio: {FINAL_AUDIO_DIR}\")\n",
    "print(f\"   📝 Répertoire annotations: {FINAL_ANNOTATION_DIR}\")\n",
    "print(f\"   📊 Paires disponibles: {len(corpus_validation['valid_pairs'])}\")\n",
    "print(f\"   🚀 Prêt pour l'entraînement de diarization!\")\n",
    "\n",
    "# Sauvegarder les informations du corpus pour les étapes suivantes\n",
    "corpus_info = {\n",
    "    'audio_dir': str(FINAL_AUDIO_DIR),\n",
    "    'annotation_dir': str(FINAL_ANNOTATION_DIR),\n",
    "    'valid_pairs': [(str(a), str(r)) for a, r in corpus_validation['valid_pairs']],\n",
    "    'total_audio_files': len(corpus_validation['audio_files']),\n",
    "    'total_annotation_files': len(corpus_validation['annotation_files']),\n",
    "    'platform': PLATFORM\n",
    "}\n",
    "\n",
    "# Sauvegarder dans un fichier JSON pour référence\n",
    "corpus_info_path = Path(\"corpus_info.json\")\n",
    "with open(corpus_info_path, 'w') as f:\n",
    "    json.dump(corpus_info, f, indent=2)\n",
    "\n",
    "print(f\"💾 Informations du corpus sauvées dans: {corpus_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "### 📊 Division des Données (Train/Eval)\n",
    "\n",
    "Division stratifiée du corpus AMI selon les bonnes pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_ami_data"
   },
   "outputs": [],
   "source": [
    "def create_ami_splits(audio_dir, rttm_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divise le corpus AMI en ensembles d'entraînement, validation et test.\n",
    "    Adapté pour les échantillons de démonstration et les vraies données AMI.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: Répertoire des fichiers audio\n",
    "        rttm_dir: Répertoire des annotations RTTM\n",
    "        train_ratio: Proportion pour l'entraînement\n",
    "        val_ratio: Proportion pour la validation\n",
    "        test_ratio: Proportion pour le test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les listes de fichiers pour chaque split\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Les ratios doivent sommer à 1.0\"\n",
    "    \n",
    "    # Lister tous les fichiers disponibles\n",
    "    audio_path = Path(audio_dir)\n",
    "    rttm_path = Path(rttm_dir)\n",
    "    \n",
    "    # Trouver les fichiers audio\n",
    "    audio_extensions = ['.wav', '.mp3', '.flac']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(audio_path.glob(f'*{ext}')))\n",
    "    \n",
    "    print(f\"🔍 Fichiers audio trouvés: {len(audio_files)}\")\n",
    "    for af in audio_files[:5]:  # Afficher les premiers\n",
    "        print(f\"   - {af.name}\")\n",
    "    \n",
    "    # Vérifier la correspondance audio-RTTM\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        # Chercher les fichiers RTTM correspondants\n",
    "        possible_rttm_names = [\n",
    "            f\"{base_name}.rttm\",\n",
    "            f\"{base_name.replace('_sample', '')}.rttm\",  # Pour les échantillons\n",
    "            f\"{base_name.split('_')[0]}.rttm\"  # Nom de base\n",
    "        ]\n",
    "        \n",
    "        rttm_file = None\n",
    "        for rttm_name in possible_rttm_names:\n",
    "            potential_rttm = rttm_path / rttm_name\n",
    "            if potential_rttm.exists():\n",
    "                rttm_file = potential_rttm\n",
    "                break\n",
    "        \n",
    "        if rttm_file:\n",
    "            # Vérifier que le fichier RTTM n'est pas vide\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                if content:\n",
    "                    valid_pairs.append({\n",
    "                        'base_name': base_name,\n",
    "                        'audio_file': str(audio_file),\n",
    "                        'rttm_file': str(rttm_file)\n",
    "                    })\n",
    "                    print(f\"   ✅ Paire valide: {base_name} -> {rttm_file.name}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ RTTM vide: {rttm_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Erreur lecture RTTM {rttm_file.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Pas de RTTM pour: {base_name}\")\n",
    "    \n",
    "    print(f\"\\n📊 Paires audio-RTTM valides trouvées: {len(valid_pairs)}\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        print(\"❌ Aucune paire valide trouvée!\")\n",
    "        \n",
    "        # Créer une paire de démonstration minimale si nécessaire\n",
    "        if len(audio_files) > 0:\n",
    "            print(\"🛠️ Création d'une configuration minimale pour la démonstration...\")\n",
    "            audio_file = audio_files[0]\n",
    "            demo_rttm = rttm_path / f\"{audio_file.stem}.rttm\"\n",
    "            \n",
    "            # Créer un fichier RTTM minimal\n",
    "            demo_content = f\"\"\"SPEAKER {audio_file.stem} 1 0.0 10.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER {audio_file.stem} 1 10.0 10.0 <NA> <NA> B <NA> <NA>\n",
    "\"\"\"\n",
    "            with open(demo_rttm, 'w') as f:\n",
    "                f.write(demo_content)\n",
    "            \n",
    "            valid_pairs = [{\n",
    "                'base_name': audio_file.stem,\n",
    "                'audio_file': str(audio_file),\n",
    "                'rttm_file': str(demo_rttm)\n",
    "            }]\n",
    "            \n",
    "            print(f\"✅ Configuration de démonstration créée: 1 paire\")\n",
    "        else:\n",
    "            raise ValueError(\"Aucun fichier audio disponible!\")\n",
    "    \n",
    "    # Mélanger et diviser\n",
    "    import random\n",
    "    random.seed(42)  # Pour la reproductibilité\n",
    "    random.shuffle(valid_pairs)\n",
    "    \n",
    "    n_total = len(valid_pairs)\n",
    "    \n",
    "    # Adaptation pour les petits datasets\n",
    "    if n_total == 1:\n",
    "        # Utiliser le même fichier pour train/val/test en mode démonstration\n",
    "        print(\"⚠️ Un seul fichier disponible - utilisation pour train/val/test\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs\n",
    "        test_files = valid_pairs\n",
    "    elif n_total < 3:\n",
    "        # Très petit dataset\n",
    "        print(f\"⚠️ Dataset très petit ({n_total} fichiers) - division adaptée\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs[:1]\n",
    "        test_files = valid_pairs[:1]\n",
    "    else:\n",
    "        # Division normale\n",
    "        n_train = max(1, int(n_total * train_ratio))\n",
    "        n_val = max(1, int(n_total * val_ratio))\n",
    "        \n",
    "        train_files = valid_pairs[:n_train]\n",
    "        val_files = valid_pairs[n_train:n_train + n_val]\n",
    "        test_files = valid_pairs[n_train + n_val:] if n_train + n_val < n_total else valid_pairs[-1:]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_files,\n",
    "        'validation': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    \n",
    "    # Afficher le résumé\n",
    "    print(f\"\\n\udccb Division des données:\")\n",
    "    print(f\"   🏋️ Entraînement: {len(splits['train'])} fichiers\")\n",
    "    print(f\"   🔍 Validation: {len(splits['validation'])} fichiers\") \n",
    "    print(f\"   🧪 Test: {len(splits['test'])} fichiers\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Créer la division des données\n",
    "print(\"=\"*60)\n",
    "print(\"📊 DIVISION DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    data_splits = create_ami_splits(FINAL_AUDIO_DIR, FINAL_ANNOTATION_DIR)\n",
    "    \n",
    "    # Sauvegarder les splits\n",
    "    splits_file = Path(\"data_splits.json\")\n",
    "    with open(splits_file, 'w') as f:\n",
    "        json.dump(data_splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Splits sauvés dans: {splits_file}\")\n",
    "    print(\"✅ Division des données réussie!\")\n",
    "    \n",
    "    # Variables globales pour les autres cellules\n",
    "    TRAIN_FILES = data_splits['train']\n",
    "    VAL_FILES = data_splits['validation'] \n",
    "    TEST_FILES = data_splits['test']\n",
    "    \n",
    "    print(f\"\\n🎯 Prêt pour l'entraînement avec:\")\n",
    "    print(f\"   - {len(TRAIN_FILES)} fichiers d'entraînement\")\n",
    "    print(f\"   - {len(VAL_FILES)} fichiers de validation\")\n",
    "    print(f\"   - {len(TEST_FILES)} fichiers de test\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de la division des données: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 🛠️ 3. Préparation des Données et Extraction de Caractéristiques\n",
    "\n",
    "Extraction des caractéristiques multi-canaux: LPS, IPD, AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "# Configuration du path et import des modules du projet\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le répertoire src au path Python\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / 'src'\n",
    "\n",
    "print(f\"📁 Répertoire courant: {current_dir}\")\n",
    "print(f\"📂 Répertoire src: {src_path}\")\n",
    "print(f\"📂 Vérification existence src: {src_path.exists()}\")\n",
    "\n",
    "# Vérifier si le répertoire src existe\n",
    "if not src_path.exists():\n",
    "    print(\"❌ ERREUR: Le répertoire 'src' n'existe pas!\")\n",
    "    print(\"🔧 Assurez-vous d'avoir cloné le repository complet avec:\")\n",
    "    print(\"   !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "    print(\"   %cd Speaker-diarization-\")\n",
    "    raise FileNotFoundError(\"Répertoire 'src' manquant. Clonez d'abord le repository.\")\n",
    "\n",
    "# Lister les fichiers dans src pour vérification\n",
    "py_files = list(src_path.glob(\"*.py\"))\n",
    "print(f\"📄 Fichiers Python trouvés: {[f.name for f in py_files]}\")\n",
    "\n",
    "# Vérifier que TOUS les modules requis existent\n",
    "required_modules = [\n",
    "    'tcn_diarization_model.py',\n",
    "    'metrics.py', \n",
    "    'dataset.py',\n",
    "    'diarization_losses.py',\n",
    "    'improved_trainer.py',\n",
    "    'optimized_dataloader.py',\n",
    "    'optimized_dataset.py'\n",
    "]\n",
    "\n",
    "missing_modules = []\n",
    "for module in required_modules:\n",
    "    if not (src_path / module).exists():\n",
    "        missing_modules.append(module)\n",
    "\n",
    "if missing_modules:\n",
    "    print(f\"❌ ERREUR: Modules manquants dans src/: {missing_modules}\")\n",
    "    print(\"🔧 Vérifiez que tous les fichiers sont présents dans le repository\")\n",
    "    raise FileNotFoundError(f\"Modules manquants: {missing_modules}\")\n",
    "\n",
    "# Ajouter src au path\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"✅ Répertoire src ajouté au path: {src_path}\")\n",
    "\n",
    "# Import de TOUS les modules du projet (OBLIGATOIRES - pas de fallback)\n",
    "print(\"\\n🔄 Import de TOUS les modules du projet...\")\n",
    "\n",
    "try:\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    print(\"✅ tcn_diarization_model importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer tcn_diarization_model\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier tcn_diarization_model.py\")\n",
    "    raise ImportError(\"Module tcn_diarization_model requis\")\n",
    "\n",
    "try:\n",
    "    from metrics import DiarizationMetrics\n",
    "    print(\"✅ metrics importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer metrics\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier metrics.py\")\n",
    "    raise ImportError(\"Module metrics requis\")\n",
    "\n",
    "try:\n",
    "    from dataset import DiarizationDataset\n",
    "    print(\"✅ dataset importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier dataset.py\")\n",
    "    raise ImportError(\"Module dataset requis\")\n",
    "\n",
    "try:\n",
    "    from diarization_losses import MultiTaskDiarizationLoss\n",
    "    print(\"✅ diarization_losses importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer diarization_losses\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier diarization_losses.py\")\n",
    "    raise ImportError(\"Module diarization_losses requis\")\n",
    "\n",
    "try:\n",
    "    from improved_trainer import ImprovedDiarizationTrainer\n",
    "    print(\"✅ improved_trainer importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer improved_trainer\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier improved_trainer.py\")\n",
    "    raise ImportError(\"Module improved_trainer requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataloader import create_optimized_dataloaders\n",
    "    print(\"✅ optimized_dataloader importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer optimized_dataloader\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier optimized_dataloader.py\")\n",
    "    raise ImportError(\"Module optimized_dataloader requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataset import OptimizedDiarizationDataset\n",
    "    print(\"✅ optimized_dataset importé\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ERREUR CRITIQUE: Impossible d'importer optimized_dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"🔧 Vérifiez le contenu du fichier optimized_dataset.py\")\n",
    "    raise ImportError(\"Module optimized_dataset requis\")\n",
    "\n",
    "print(\"\\n🎉 TOUS LES MODULES IMPORTÉS AVEC SUCCÈS!\")\n",
    "print(\"📋 Modules disponibles pour l'entraînement:\")\n",
    "print(\"   - DiarizationTCN: ✅\")\n",
    "print(\"   - DiarizationMetrics: ✅\")\n",
    "print(\"   - DiarizationDataset: ✅\")\n",
    "print(\"   - MultiTaskDiarizationLoss: ✅\")\n",
    "print(\"   - ImprovedDiarizationTrainer: ✅\")\n",
    "print(\"   - OptimizedDataLoader: ✅\")\n",
    "print(\"   - OptimizedDataset: ✅\")\n",
    "\n",
    "print(\"\\n🚀 Prêt pour l'entraînement avec TOUS vos modèles originaux!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes utilitaires nécessaires (non présentes dans les modules du projet)\n",
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# MemoryMonitor pour surveiller l'utilisation mémoire\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Retourne les informations mémoire.\"\"\"\n",
    "        # RAM\n",
    "        ram_info = psutil.virtual_memory()\n",
    "        ram_percent = ram_info.percent\n",
    "        \n",
    "        # GPU\n",
    "        gpu_percent = 0\n",
    "        gpu_memory_used = 0\n",
    "        gpu_memory_total = 0\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0)\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "            gpu_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "        \n",
    "        return {\n",
    "            'ram_percent': ram_percent,\n",
    "            'gpu_percent': gpu_percent,\n",
    "            'gpu_memory_used': gpu_memory_used,\n",
    "            'gpu_memory_total': gpu_memory_total\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Nettoie la mémoire.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ Classes utilitaires créées (MemoryMonitor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Test de l'extraction de caractéristiques\n",
    "print(\"🧪 Test de l'extraction de caractéristiques...\")\n",
    "\n",
    "# Créer un extracteur de caractéristiques\n",
    "feature_extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=512,\n",
    "    hop_length=256\n",
    ")\n",
    "\n",
    "# Créer des données audio fictives (8 canaux)\n",
    "n_channels = 8\n",
    "duration = 4.0  # 4 secondes\n",
    "sample_rate = 16000\n",
    "n_samples = int(duration * sample_rate)\n",
    "\n",
    "# Simuler audio multi-canal avec du bruit et des signaux\n",
    "waveforms = []\n",
    "for ch in range(n_channels):\n",
    "    # Signal de base + bruit\n",
    "    base_signal = np.sin(2 * np.pi * 440 * np.linspace(0, duration, n_samples))  # 440 Hz\n",
    "    noise = np.random.normal(0, 0.1, n_samples)\n",
    "    # Ajouter un léger décalage temporel pour simuler la spatialisation\n",
    "    delay_samples = int(0.001 * ch * sample_rate)  # 1ms de délai par canal\n",
    "    delayed_signal = np.zeros(n_samples)\n",
    "    if delay_samples < n_samples:\n",
    "        delayed_signal[delay_samples:] = base_signal[:n_samples-delay_samples]\n",
    "    \n",
    "    final_signal = delayed_signal + noise\n",
    "    waveforms.append(torch.tensor(final_signal, dtype=torch.float32))\n",
    "\n",
    "print(f\"📊 Audio généré: {n_channels} canaux, {duration}s, {sample_rate} Hz\")\n",
    "\n",
    "# Test d'extraction\n",
    "features = feature_extractor.extract_features(waveforms)\n",
    "print(f\"✅ Caractéristiques extraites: {features.shape}\")\n",
    "print(f\"   - Dimensions attendues: [771, ~{int(duration * sample_rate / 256)}]\")\n",
    "print(f\"   - Dimensions obtenues: {list(features.shape)}\")\n",
    "\n",
    "# Analyser les caractéristiques\n",
    "print(f\"\\n📈 Statistiques des caractéristiques:\")\n",
    "print(f\"   - Min: {features.min():.3f}\")\n",
    "print(f\"   - Max: {features.max():.3f}\")\n",
    "print(f\"   - Moyenne: {features.mean():.3f}\")\n",
    "print(f\"   - Std: {features.std():.3f}\")\n",
    "\n",
    "# Visualiser les caractéristiques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Spectrogramme des caractéristiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(features.numpy()[:100, :], aspect='auto', origin='lower')\n",
    "plt.title('Caractéristiques LPS (première partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(features.numpy()[257:357, :], aspect='auto', origin='lower')\n",
    "plt.title('Caractéristiques IPD (première partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(features.numpy()[500:600, :], aspect='auto', origin='lower')\n",
    "plt.title('Caractéristiques AF (première partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(features.numpy().mean(axis=0))\n",
    "plt.title('Énergie moyenne par frame')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Énergie moyenne')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Créer le répertoire de résultats s'il n'existe pas\n",
    "import os\n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(RESULTS_DIR / 'feature_extraction_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Test d'extraction de caractéristiques terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## 🧠 4. Configuration du Modèle et Entraînement\n",
    "\n",
    "Configuration optimale pour le corpus AMI avec les meilleures pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Configuration optimisée pour AMI corpus\n",
    "config = {\n",
    "    # === MODÈLE ===\n",
    "    'model': {\n",
    "        'input_dim': 771,  # LPS (257) + IPD (257*4) + AF (257*4) = 2313 → 771 après agrégation\n",
    "        'hidden_channels': [256, 256, 256, 512, 512],  # Architecture TCN multi-échelle\n",
    "        'kernel_size': 3,\n",
    "        'num_speakers': 4,  # AMI corpus a typiquement 3-4 locuteurs\n",
    "        'dropout': 0.2,\n",
    "        'use_attention': True,  # Auto-attention pour dépendances long-terme\n",
    "        'use_speaker_classifier': True,  # Classification de locuteurs\n",
    "        'embedding_dim': 256\n",
    "    },\n",
    "    \n",
    "    # === FONCTION DE PERTE ===\n",
    "    'loss': {\n",
    "        'type': 'multitask',\n",
    "        'vad_weight': 1.0,\n",
    "        'osd_weight': 1.0,\n",
    "        'consistency_weight': 0.1,\n",
    "        'use_pit': True,  # Permutation Invariant Training\n",
    "        'use_focal': True,  # Focal Loss pour données déséquilibrées\n",
    "        'focal_gamma': 2.0,\n",
    "        'num_speakers': 4\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISEUR ===\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 1e-3,  # Learning rate initial\n",
    "        'weight_decay': 1e-4,\n",
    "        'betas': (0.9, 0.999)\n",
    "    },\n",
    "    \n",
    "    # === PLANIFICATEUR LR ===\n",
    "    'scheduler': {\n",
    "        'type': 'onecycle',  # OneCycleLR pour convergence rapide\n",
    "        'steps_per_epoch': 100,  # Sera mis à jour automatiquement\n",
    "        'pct_start': 0.3  # 30% montée, 70% descente\n",
    "    },\n",
    "    \n",
    "    # === ENTRAÎNEMENT ===\n",
    "    'training': {\n",
    "        'epochs': 50,  # Réduit pour Colab\n",
    "        'batch_size': 8,  # Adapté à la mémoire Colab\n",
    "        'num_workers': 2  # Moins de workers pour éviter les problèmes mémoire\n",
    "    },\n",
    "    \n",
    "    # === DONNÉES ===\n",
    "    'data': {\n",
    "        'segment_duration': 4.0,  # Segments de 4 secondes\n",
    "        'sample_rate': 16000,\n",
    "        'train_split': 0.7,\n",
    "        'max_segments': 1000  # Limite pour Colab\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISATIONS AVANCÉES ===\n",
    "    'accumulation_steps': 4,  # Batch effectif = 8*4 = 32\n",
    "    'use_amp': True,  # Précision mixte\n",
    "    'grad_clip_norm': 1.0,\n",
    "    'patience': 10,\n",
    "    'save_every': 5,\n",
    "    \n",
    "    # === MONITORING ===\n",
    "    'use_wandb': True,  # Weights & Biases (optionnel)\n",
    "    'project_name': 'ami-speaker-diarization',\n",
    "    'memory_threshold': 0.85,  # Gestion mémoire Colab\n",
    "    'adaptive_batch': True,\n",
    "    'speaker_loss_weight': 0.5,\n",
    "    \n",
    "    # === CHEMINS ===\n",
    "    'save_dir': str(MODEL_DIR),\n",
    "    'results_dir': str(RESULTS_DIR)\n",
    "}\n",
    "\n",
    "print(\"⚙️ Configuration créée avec les paramètres suivants:\")\n",
    "print(f\"   - Architecture: TCN {config['model']['hidden_channels']}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']} (effectif: {config['training']['batch_size'] * config['accumulation_steps']})\")\n",
    "print(f\"   - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Précision mixte: {config['use_amp']}\")\n",
    "print(f\"   - Classification locuteurs: {config['model']['use_speaker_classifier']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "config_file = MODEL_DIR / 'training_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Configuration sauvegardée: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration de Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "use_wandb = config.get('use_wandb', False)\n",
    "\n",
    "if use_wandb:\n",
    "    try:\n",
    "        # Connexion à wandb (nécessite un compte gratuit)\n",
    "        wandb.login()\n",
    "        \n",
    "        # Initialisation du projet\n",
    "        wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"ami-tcn-{torch.cuda.get_device_name(0).replace(' ', '-') if torch.cuda.is_available() else 'cpu'}\",\n",
    "            tags=['ami-corpus', 'tcn', 'multi-channel', 'colab'],\n",
    "            notes=\"Entraînement sur corpus AMI avec architecture TCN améliorée\"\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Weights & Biases configuré!\")\n",
    "        print(f\"📊 Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur wandb: {e}\")\n",
    "        print(\"📈 Entraînement sans monitoring wandb...\")\n",
    "        config['use_wandb'] = False\n",
    "else:\n",
    "    print(\"📈 Entraînement sans monitoring wandb (désactivé dans config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 🚀 5. Entraînement du Modèle\n",
    "\n",
    "Entraînement avec toutes les optimisations: gestion mémoire, precision mixte, accumulation de gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Création des DataLoaders optimisés\n",
    "print(\"🔄 Création des DataLoaders avec vos modules optimisés...\")\n",
    "\n",
    "# Utiliser la fonction create_optimized_dataloaders importée\n",
    "train_loader, val_loader = create_optimized_dataloaders(\n",
    "    audio_dir=audio_dir,\n",
    "    rttm_dir=rttm_dir,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    train_split=config['data']['train_split'],\n",
    "    num_workers=config['training']['num_workers'],\n",
    "    segment_duration=config['data']['segment_duration'],\n",
    "    sample_rate=config['data']['sample_rate'],\n",
    "    max_segments=config['data']['max_segments'],\n",
    "    memory_threshold=config['memory_threshold'],\n",
    "    adaptive_batch=config['adaptive_batch'],\n",
    "    accumulation_steps=config['accumulation_steps']\n",
    ")\n",
    "\n",
    "print(f\"✅ DataLoaders créés avec succès!\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Mettre à jour la configuration avec le nombre réel de steps\n",
    "config['scheduler']['steps_per_epoch'] = len(train_loader)\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\n🧪 Test d'un batch d'entraînement...\")\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f\"   Batch {batch_idx}:\")\n",
    "    print(f\"     - Features: {batch['features'].shape}\")\n",
    "    print(f\"     - VAD labels: {batch['vad_labels'].shape}\")\n",
    "    print(f\"     - OSD labels: {batch['osd_labels'].shape}\")\n",
    "    \n",
    "    # Vérifier les dimensions\n",
    "    assert batch['features'].shape[1] == 771, f\"Dimension features incorrecte: {batch['features'].shape[1]} != 771\"\n",
    "    assert batch['vad_labels'].shape[-1] == 4, f\"Nombre de locuteurs incorrect: {batch['vad_labels'].shape[-1]} != 4\"\n",
    "    \n",
    "    print(f\"     ✅ Dimensions correctes!\")\n",
    "    break\n",
    "\n",
    "print(\"✅ DataLoaders optimisés configurés et testés avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialisation du trainer avancé\n",
    "print(\"🧠 Initialisation du trainer avec vos modules optimisés...\")\n",
    "\n",
    "# Créer le trainer avec toutes les améliorations (OBLIGATOIRE - pas de fallback)\n",
    "trainer = ImprovedDiarizationTrainer(config)\n",
    "\n",
    "print(f\"✅ Trainer initialisé avec succès!\")\n",
    "print(f\"   - Modèle: {trainer.model.get_num_params():,} paramètres\")\n",
    "print(f\"   - Device: {trainer.device}\")\n",
    "print(f\"   - Précision mixte: {trainer.use_amp}\")\n",
    "print(f\"   - Accumulation gradients: {trainer.accumulation_steps}\")\n",
    "\n",
    "# Test du forward pass\n",
    "print(\"\\n🧪 Test du modèle...\")\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test avec un batch de données\n",
    "    test_input = torch.randn(2, 771, 250).to(trainer.device)\n",
    "    \n",
    "    # Forward pass simple\n",
    "    vad_out, osd_out = model(test_input)\n",
    "    print(f\"   Forward simple: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "    \n",
    "    # Forward avec embeddings si disponible\n",
    "    try:\n",
    "        vad_out, osd_out, embeddings, speaker_logits = model(test_input, return_embeddings=True)\n",
    "        print(f\"   Forward complet: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        print(f\"                   Embeddings {embeddings.shape}, Speaker {speaker_logits.shape}\")\n",
    "    except:\n",
    "        print(\"   Forward avec embeddings non disponible (normal)\")\n",
    "\n",
    "print(\"✅ Modèle fonctionne correctement!\")\n",
    "print(\"🚀 Trainer optimisé prêt pour l'entraînement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# Démarrage de l'entraînement\n",
    "print(\"🚀 DÉMARRAGE DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monitoring mémoire\n",
    "memory_monitor = MemoryMonitor()\n",
    "initial_memory = memory_monitor.get_memory_info()\n",
    "\n",
    "print(f\"💾 Mémoire initiale:\")\n",
    "print(f\"   - RAM: {initial_memory['ram_percent']:.1f}%\")\n",
    "print(f\"   - GPU: {initial_memory['gpu_percent']:.1f}%\")\n",
    "\n",
    "# Configuration d'entraînement\n",
    "num_epochs = config['training']['epochs']\n",
    "save_every = config.get('save_every', 5)\n",
    "\n",
    "# Historique des métriques\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\n📋 Configuration d'entraînement:\")\n",
    "print(f\"   - Epochs: {num_epochs}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Sauvegarde chaque {save_every} epochs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DÉBUT DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Boucle d'entraînement principale\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\n🔄 Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Phase d'entraînement\n",
    "        if hasattr(trainer, 'train_epoch'):\n",
    "            # Utiliser le trainer avancé\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            train_loss = train_metrics['total_loss']\n",
    "        else:\n",
    "            # Entraînement simple\n",
    "            trainer.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                trainer.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                trainer.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Phase de validation\n",
    "        print(f\"\\n📊 Validation...\")\n",
    "        trainer.model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                val_loss += loss_dict['total_loss'].item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Monitoring mémoire\n",
    "        current_memory = memory_monitor.get_memory_info()\n",
    "        \n",
    "        # Résumé de l'epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\n📈 Epoch {epoch+1} Résultats:\")\n",
    "        print(f\"   - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   - Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"   - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"   - Mémoire GPU: {current_memory['gpu_percent']:.1f}%\")\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            print(f\"   ✅ Nouveau meilleur modèle sauvé! (Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Sauvegarde périodique\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = MODEL_DIR / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   💾 Checkpoint sauvé: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Logging wandb\n",
    "        if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'gpu_memory_percent': current_memory['gpu_percent'],\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        \n",
    "        # Nettoyage mémoire\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Entraînement interrompu par l'utilisateur\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Erreur durant l'entraînement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n⏱️ Temps total d'entraînement: {total_time/60:.1f} minutes\")\n",
    "    print(f\"🎯 Meilleure validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Fermeture wandb\n",
    "    if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "        wandb.finish()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENTRAÎNEMENT TERMINÉ\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 📊 6. Évaluation et Métriques\n",
    "\n",
    "Évaluation complète avec métriques de diarization standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# Évaluation complète du modèle\n",
    "print(\"📊 ÉVALUATION DU MODÈLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Charger le meilleur modèle\n",
    "best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"📂 Chargement du meilleur modèle: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"   - Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"   - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Pas de modèle sauvé, utilisation du modèle actuel\")\n",
    "\n",
    "# Initialiser les métriques\n",
    "metrics_computer = DiarizationMetrics(num_speakers=config['model']['num_speakers'])\n",
    "\n",
    "# Évaluation sur l'ensemble de validation\n",
    "print(\"\\n🧪 Évaluation sur l'ensemble de validation...\")\n",
    "trainer.model.eval()\n",
    "\n",
    "all_vad_preds = []\n",
    "all_vad_targets = []\n",
    "all_osd_preds = []\n",
    "all_osd_targets = []\n",
    "\n",
    "eval_loss = 0\n",
    "num_eval_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        features = batch['features'].to(trainer.device)\n",
    "        vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "        osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "        \n",
    "        # Prédictions\n",
    "        if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "            vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "        else:\n",
    "            vad_pred, osd_pred = trainer.model(features)\n",
    "        \n",
    "        # Loss\n",
    "        loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "        eval_loss += loss_dict['total_loss'].item()\n",
    "        num_eval_batches += 1\n",
    "        \n",
    "        # Collecter pour métriques\n",
    "        all_vad_preds.append(vad_pred.cpu())\n",
    "        all_vad_targets.append(vad_labels.cpu())\n",
    "        all_osd_preds.append(osd_pred.cpu())\n",
    "        all_osd_targets.append(osd_labels.cpu())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"   Batch {batch_idx}/{len(val_loader)} évalué\")\n",
    "\n",
    "# Calculer métriques détaillées\n",
    "print(\"\\n📈 Calcul des métriques détaillées...\")\n",
    "\n",
    "vad_preds = torch.cat(all_vad_preds, dim=0)\n",
    "vad_targets = torch.cat(all_vad_targets, dim=0)\n",
    "osd_preds = torch.cat(all_osd_preds, dim=0)\n",
    "osd_targets = torch.cat(all_osd_targets, dim=0)\n",
    "\n",
    "print(f\"   - Données évaluées: {vad_preds.shape[0]} échantillons\")\n",
    "print(f\"   - Durée totale: {vad_preds.shape[0] * vad_preds.shape[1] * 0.02:.1f} secondes\")\n",
    "\n",
    "# Métriques principales\n",
    "metrics = metrics_computer.compute_metrics(vad_preds, osd_preds, vad_targets, osd_targets)\n",
    "\n",
    "print(\"\\n🎯 RÉSULTATS D'ÉVALUATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"📊 Loss finale: {eval_loss / num_eval_batches:.4f}\")\n",
    "print(f\"📊 DER (Diarization Error Rate): {metrics.get('der', 0):.2f}%\")\n",
    "print(f\"📊 F1 Score global: {metrics.get('f1_score', 0):.3f}\")\n",
    "print(f\"📊 Précision frame: {metrics.get('frame_precision', 0):.3f}\")\n",
    "print(f\"📊 Rappel frame: {metrics.get('frame_recall', 0):.3f}\")\n",
    "print(f\"📊 Jaccard Index: {metrics.get('jaccard_index', 0):.3f}\")\n",
    "\n",
    "# Métriques OSD\n",
    "if 'osd_precision' in metrics:\n",
    "    print(f\"\\n🔀 Détection de Chevauchement (OSD):\")\n",
    "    print(f\"   - Précision OSD: {metrics['osd_precision']:.3f}\")\n",
    "    print(f\"   - Rappel OSD: {metrics['osd_recall']:.3f}\")\n",
    "    print(f\"   - F1 OSD: {metrics['osd_f1']:.3f}\")\n",
    "\n",
    "# Métriques par locuteur\n",
    "print(f\"\\n👥 Métriques par Locuteur:\")\n",
    "for spk in range(config['model']['num_speakers']):\n",
    "    if f'speaker_{spk}_f1' in metrics:\n",
    "        print(f\"   Locuteur {spk}: F1={metrics[f'speaker_{spk}_f1']:.3f}, \"\n",
    "              f\"P={metrics[f'speaker_{spk}_precision']:.3f}, \"\n",
    "              f\"R={metrics[f'speaker_{spk}_recall']:.3f}\")\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "metrics_file = RESULTS_DIR / 'evaluation_metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    # Convertir les tenseurs en listes pour JSON\n",
    "    json_metrics = {k: (v.item() if torch.is_tensor(v) else v) for k, v in metrics.items()}\n",
    "    json_metrics['eval_loss'] = eval_loss / num_eval_batches\n",
    "    json.dump(json_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Métriques sauvées: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 📈 7. Visualisations et Analyses\n",
    "\n",
    "Génération de graphiques et visualisations des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Visualisations des résultats\n",
    "print(\"📊 Génération des visualisations...\")\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# === 1. Courbes d'entraînement ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Résultats d\\'Entraînement - Speaker Diarization TCN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Courbe de perte\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('Évolution de la Perte')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les dernières epochs\n",
    "if len(train_losses) > 10:\n",
    "    start_idx = max(0, len(train_losses) - 20)\n",
    "    axes[0, 1].plot(range(start_idx, len(train_losses)), train_losses[start_idx:], \n",
    "                   label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(range(start_idx, len(val_losses)), val_losses[start_idx:], \n",
    "                   label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Convergence (Dernières Epochs)')\n",
    "else:\n",
    "    axes[0, 1].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Évolution de la Perte (Toutes Epochs)')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === 2. Analyse des prédictions ===\n",
    "# Prendre un échantillon pour visualisation\n",
    "sample_idx = 0\n",
    "sample_vad_pred = vad_preds[sample_idx].numpy()  # [time, speakers]\n",
    "sample_vad_target = vad_targets[sample_idx].numpy()\n",
    "sample_osd_pred = osd_preds[sample_idx].numpy()  # [time]\n",
    "sample_osd_target = osd_targets[sample_idx].numpy()\n",
    "\n",
    "# Activité des locuteurs (prédictions vs vérité terrain)\n",
    "time_frames = np.arange(len(sample_vad_pred)) * 0.02  # Conversion en secondes\n",
    "\n",
    "# Subplot pour VAD\n",
    "axes[1, 0].imshow(sample_vad_pred.T, aspect='auto', origin='lower', \n",
    "                 extent=[0, len(sample_vad_pred)*0.02, 0, 4], \n",
    "                 cmap='Blues', alpha=0.7)\n",
    "axes[1, 0].imshow(sample_vad_target.T, aspect='auto', origin='lower',\n",
    "                 extent=[0, len(sample_vad_target)*0.02, 0, 4],\n",
    "                 cmap='Reds', alpha=0.5)\n",
    "axes[1, 0].set_title('Activité VAD: Prédiction (Bleu) vs Vérité (Rouge)')\n",
    "axes[1, 0].set_xlabel('Temps (s)')\n",
    "axes[1, 0].set_ylabel('Locuteur ID')\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "\n",
    "# Subplot pour OSD\n",
    "axes[1, 1].plot(time_frames, sample_osd_pred, label='Prédiction OSD', \n",
    "               color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(time_frames, sample_osd_target, label='Vérité OSD', \n",
    "               color='red', linewidth=2, alpha=0.6)\n",
    "axes[1, 1].set_title('Détection de Chevauchement (OSD)')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_ylabel('Probabilité de Chevauchement')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "training_plot_path = RESULTS_DIR / 'training_results.png'\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Graphique d'entraînement sauvé: {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# === 3. Matrice de confusion pour classification ===\n",
    "if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "    print(\"\\n📊 Analyse de la classification des locuteurs...\")\n",
    "    \n",
    "    # Extraire les prédictions de classification\n",
    "    all_speaker_preds = []\n",
    "    all_speaker_targets = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(trainer.device)\n",
    "            vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "            \n",
    "            try:\n",
    "                vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "                \n",
    "                # Créer des labels de locuteurs à partir des VAD labels\n",
    "                speaker_targets = torch.argmax(vad_labels.sum(dim=1), dim=1)  # Locuteur le plus actif\n",
    "                speaker_preds = torch.argmax(speaker_logits, dim=1)\n",
    "                \n",
    "                all_speaker_preds.extend(speaker_preds.cpu().numpy())\n",
    "                all_speaker_targets.extend(speaker_targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur dans un batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_speaker_preds and all_speaker_targets:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(all_speaker_targets, all_speaker_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Pred {i}' for i in range(4)],\n",
    "                   yticklabels=[f'True {i}' for i in range(4)])\n",
    "        plt.title('Matrice de Confusion - Classification des Locuteurs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Prédiction')\n",
    "        plt.ylabel('Vérité Terrain')\n",
    "        \n",
    "        confusion_path = RESULTS_DIR / 'speaker_confusion_matrix.png'\n",
    "        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print(\"\\n📋 Rapport de Classification:\")\n",
    "        print(classification_report(all_speaker_targets, all_speaker_preds,\n",
    "                                  target_names=[f'Locuteur {i}' for i in range(4)],\n",
    "                                  digits=3))\n",
    "        \n",
    "        print(f\"✅ Matrice de confusion sauvée: {confusion_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Pas assez de données pour la matrice de confusion\")\n",
    "else:\n",
    "    print(\"⚠️ Classificateur de locuteurs non activé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_summary"
   },
   "outputs": [],
   "source": [
    "# === 4. Résumé final et comparaisons ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RÉSUMÉ FINAL DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Créer un résumé complet\n",
    "final_summary = {\n",
    "    'Configuration': {\n",
    "        'Architecture': f\"TCN {config['model']['hidden_channels']}\",\n",
    "        'Paramètres': f\"{sum(p.numel() for p in trainer.model.parameters()):,}\",\n",
    "        'Batch Size': config['training']['batch_size'],\n",
    "        'Epochs': len(train_losses),\n",
    "        'Learning Rate': config['optimizer']['lr'],\n",
    "        'Device': str(trainer.device)\n",
    "    },\n",
    "    'Résultats Finaux': {\n",
    "        'Train Loss': f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\",\n",
    "        'Val Loss': f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\",\n",
    "        'Best Val Loss': f\"{best_val_loss:.4f}\",\n",
    "        'DER': f\"{metrics.get('der', 0):.2f}%\",\n",
    "        'F1 Score': f\"{metrics.get('f1_score', 0):.3f}\",\n",
    "        'Frame Precision': f\"{metrics.get('frame_precision', 0):.3f}\",\n",
    "        'Frame Recall': f\"{metrics.get('frame_recall', 0):.3f}\"\n",
    "    },\n",
    "    'Fichiers Générés': {\n",
    "        'Meilleur Modèle': str(best_model_path) if best_model_path.exists() else \"Non sauvé\",\n",
    "        'Métriques': str(metrics_file),\n",
    "        'Graphiques': str(training_plot_path),\n",
    "        'Configuration': str(config_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage du résumé\n",
    "for section, items in final_summary.items():\n",
    "    print(f\"\\n🔹 {section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"   {key:.<25} {value}\")\n",
    "\n",
    "# Sauvegarde du résumé\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Résumé complet sauvé: {summary_file}\")\n",
    "\n",
    "# === 5. Recommandations d'amélioration ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💡 RECOMMANDATIONS POUR AMÉLIORER LES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "current_der = metrics.get('der', 100)\n",
    "current_f1 = metrics.get('f1_score', 0)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if current_der > 25:\n",
    "    recommendations.append(\"🔧 DER élevé: Augmenter le nombre d'epochs ou réduire le learning rate\")\n",
    "if current_f1 < 0.7:\n",
    "    recommendations.append(\"🔧 F1 faible: Essayer focal loss avec gamma plus élevé\")\n",
    "if len(train_losses) < 20:\n",
    "    recommendations.append(\"⏰ Entraînement court: Augmenter le nombre d'epochs\")\n",
    "if config['training']['batch_size'] < 16:\n",
    "    recommendations.append(\"📦 Batch size petit: Augmenter si possible pour améliorer la stabilité\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"📊 Utiliser plus de données AMI si disponibles\",\n",
    "    \"🎯 Affiner les hyperparamètres avec Optuna\",\n",
    "    \"🔄 Essayer l'ensemble de modèles\",\n",
    "    \"📈 Implémenter la validation croisée\",\n",
    "    \"🧠 Tester différentes architectures d'attention\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations[:7], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ENTRAÎNEMENT TERMINÉ AVEC SUCCÈS!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📁 Tous les résultats sont sauvés dans: {RESULTS_DIR}\")\n",
    "print(f\"🧠 Meilleur modèle disponible dans: {MODEL_DIR}\")\n",
    "\n",
    "if config.get('use_wandb', False):\n",
    "    print(f\"📊 Logs détaillés disponibles sur Weights & Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 🚀 Prochaines Étapes\n",
    "\n",
    "### 📝 Pour Continuer l'Amélioration:\n",
    "\n",
    "1. **📊 Données**: Télécharger le corpus AMI complet\n",
    "2. **⚙️ Hyperparamètres**: Optimiser avec Optuna\n",
    "3. **🎯 Architecture**: Tester différentes tailles de modèle\n",
    "4. **📈 Ensembles**: Combiner plusieurs modèles\n",
    "5. **🔄 Post-traitement**: Améliorer la segmentation finale\n",
    "\n",
    "### 💾 Fichiers Générés:\n",
    "- `models/checkpoints/best_model.pth` - Meilleur modèle\n",
    "- `results/evaluation_metrics.json` - Métriques détaillées\n",
    "- `results/training_results.png` - Graphiques d'entraînement\n",
    "- `results/training_summary.json` - Résumé complet\n",
    "\n",
    "### 🎯 Objectifs de Performance:\n",
    "- **DER < 20%** sur AMI corpus (état de l'art: ~15-18%)\n",
    "- **F1 > 0.8** pour la détection d'activité\n",
    "- **Temps réel** pour l'inférence\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Félicitations! Vous avez entraîné avec succès un système de diarization moderne avec toutes les optimisations avancées!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/saito1111/Speaker-diarization-/blob/main/Enhanced_Diarization_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üéôÔ∏è Enhanced Multi-Channel Speaker Diarization Training\n",
    "\n",
    "**Objectif :** Entra√Æner un syst√®me de diarization de locuteurs avanc√© sur le corpus AMI  \n",
    "**Architecture :** TCN multi-√©chelle avec attention, classification de locuteurs et gestion m√©moire optimis√©e  \n",
    "**Plateformes :** Optimis√© pour Google Colab et Kaggle Notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Nouvelles Fonctionnalit√©s\n",
    "\n",
    "### üöÄ Compatibilit√© Multi-Plateformes\n",
    "- ‚úÖ **Google Colab** : Installation automatique des d√©pendances\n",
    "- ‚úÖ **Kaggle Notebooks** : Configuration optimis√©e pour l'environnement Kaggle  \n",
    "- ‚úÖ **Environnement Local** : Support complet pour le d√©veloppement local\n",
    "\n",
    "### üîß Am√©liorations Techniques\n",
    "- üö´ **Sans Conda** : Installation directe avec pip pour une meilleure compatibilit√©\n",
    "- üì¶ **Gestion des D√©pendances** : Installation automatique et v√©rification des packages\n",
    "- üéµ **Donn√©es Adaptatives** : T√©l√©chargement intelligent du corpus AMI avec fallback\n",
    "- üß™ **Tests d'Environnement** : V√©rification automatique de la configuration\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "1. [Configuration de l'Environnement](#setup) - Installation automatique des d√©pendances\n",
    "2. [T√©l√©chargement du Corpus AMI](#data) - T√©l√©chargement intelligent avec fallback\n",
    "3. [Pr√©paration des Donn√©es](#preprocessing) - Division et pr√©paration des donn√©es\n",
    "4. [Mod√®le et Configuration](#model) - Architecture TCN avanc√©e\n",
    "5. [Entra√Ænement](#training) - Entra√Ænement avec monitoring avanc√©\n",
    "6. [√âvaluation](#evaluation) - M√©triques de performance compl√®tes\n",
    "7. [Sauvegarde et Visualisations](#results) - R√©sultats et analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß 1. Configuration de l'Environnement\n",
    "\n",
    "Configuration optimis√©e pour **Google Colab** et **Kaggle Notebooks**.  \n",
    "Installation directe des d√©pendances avec pip, d√©tection automatique de la plateforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances n√©cessaires pour Google Colab et Kaggle\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# V√©rifier si on est sur Colab ou Kaggle\n",
    "try:\n",
    "    import google.colab\n",
    "    PLATFORM = \"COLAB\"\n",
    "    print(\"üöÄ Ex√©cution sur Google Colab\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import kaggle\n",
    "        PLATFORM = \"KAGGLE\"\n",
    "        print(\"üöÄ Ex√©cution sur Kaggle\")\n",
    "    except ImportError:\n",
    "        PLATFORM = \"LOCAL\"\n",
    "        print(\"üöÄ Ex√©cution en local\")\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_env"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances principales avec pip\n",
    "print(\"üì¶ Installation des d√©pendances...\")\n",
    "\n",
    "# Mettre √† jour pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Installation des d√©pendances PyTorch\n",
    "print(\"üî• Installation de PyTorch et Torchaudio...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Installation des d√©pendances scientifiques\n",
    "print(\"üìä Installation des packages scientifiques...\")\n",
    "!pip install numpy scipy scikit-learn matplotlib seaborn pandas\n",
    "\n",
    "# Installation des d√©pendances audio et ML\n",
    "print(\"üéµ Installation des packages audio et ML...\")\n",
    "!pip install librosa soundfile speechbrain\n",
    "\n",
    "# Installation des outils d'optimisation et monitoring\n",
    "print(\"‚öôÔ∏è Installation des outils d'optimisation...\")\n",
    "!pip install optuna tqdm psutil wandb\n",
    "\n",
    "print(\"‚úÖ Toutes les d√©pendances install√©es avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# V√©rification des installations et imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import optuna\n",
    "import wandb\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ V√©rification des installations:\")\n",
    "print(f\"   üì¶ PyTorch: {torch.__version__}\")\n",
    "print(f\"   üì¶ Torchaudio: {torchaudio.__version__}\")\n",
    "print(f\"   üì¶ NumPy: {np.__version__}\")\n",
    "print(f\"   üì¶ Pandas: {pd.__version__}\")\n",
    "print(f\"   üì¶ Librosa: {librosa.__version__}\")\n",
    "print(f\"   üì¶ SoundFile: {sf.__version__}\")\n",
    "\n",
    "print(f\"\\nüî• Configuration CUDA:\")\n",
    "print(f\"   ‚ö° CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   üì± Version CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   üîß Nombre de GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è CUDA non disponible - utilisation du CPU\")\n",
    "\n",
    "print(f\"\\nüíª Ressources syst√®me:\")\n",
    "print(f\"   üß† CPU: {psutil.cpu_count()} c≈ìurs\")\n",
    "print(f\"   üíæ RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\nüéâ Environnement pr√™t pour l'entra√Ænement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final de compatibilit√© et v√©rification de l'environnement\n",
    "print(\"üß™ TEST FINAL DE COMPATIBILIT√â COLAB/KAGGLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: V√©rification de la plateforme\n",
    "print(f\"üñ•Ô∏è Plateforme d√©tect√©e: {PLATFORM}\")\n",
    "\n",
    "# Test 2: V√©rification des imports critiques\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import librosa\n",
    "    import soundfile\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"‚úÖ Tous les imports critiques r√©ussis\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "\n",
    "# Test 3: V√©rification CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA non disponible - entra√Ænement sur CPU (plus lent)\")\n",
    "\n",
    "# Test 4: Test simple avec PyTorch\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    print(f\"‚úÖ Test PyTorch r√©ussi sur {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur test PyTorch: {e}\")\n",
    "\n",
    "# Test 5: Test audio\n",
    "try:\n",
    "    # Cr√©er un signal audio de test\n",
    "    sr = 16000\n",
    "    duration = 2\n",
    "    test_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, sr * duration))\n",
    "    \n",
    "    # Test librosa\n",
    "    mfccs = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=13)\n",
    "    print(f\"‚úÖ Test audio r√©ussi - MFCC shape: {mfccs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur test audio: {e}\")\n",
    "\n",
    "print(\"\\nüéâ ENVIRONNEMENT PR√äT POUR L'ENTRA√éNEMENT!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Modifications pour Colab/Kaggle\n",
    "\n",
    "**‚úÖ Am√©liorations apport√©es :**\n",
    "\n",
    "1. **üö´ Suppression de Conda**\n",
    "   - Remplacement par installation directe avec pip\n",
    "   - Compatible avec l'environnement Python natif de Colab/Kaggle\n",
    "\n",
    "2. **üì¶ Gestion des D√©pendances**\n",
    "   - Installation automatique de PyTorch avec support CUDA\n",
    "   - V√©rification des versions et de la compatibilit√©\n",
    "   - Gestion d'erreurs robuste\n",
    "\n",
    "3. **üéµ T√©l√©chargement des Donn√©es**\n",
    "   - URLs mises √† jour pour le corpus AMI\n",
    "   - Syst√®me de fallback avec donn√©es de d√©monstration\n",
    "   - Gestion intelligente des timeouts et erreurs r√©seau\n",
    "\n",
    "4. **üîß D√©tection de Plateforme**\n",
    "   - D√©tection automatique Colab/Kaggle/Local\n",
    "   - Adaptation automatique de la configuration\n",
    "   - Tests de compatibilit√© int√©gr√©s\n",
    "\n",
    "5. **üíæ Gestion des Chemins**\n",
    "   - Chemins relatifs compatibles avec tous les environnements\n",
    "   - Cr√©ation automatique des r√©pertoires n√©cessaires\n",
    "   - V√©rification des permissions d'√©criture\n",
    "\n",
    "**üéØ Le notebook est maintenant pr√™t pour :**\n",
    "- ‚úÖ Google Colab (avec GPU)\n",
    "- ‚úÖ Kaggle Notebooks  \n",
    "- ‚úÖ Environnements locaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### üìÇ Clonage du R√©pertoire et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_project"
   },
   "outputs": [],
   "source": [
    "# Clonage optimis√© du repository pour Colab/Kaggle\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "repo_name = \"Speaker-diarization-\"\n",
    "\n",
    "print(f\"üìÅ R√©pertoire courant: {current_dir}\")\n",
    "print(f\"üñ•Ô∏è Plateforme: {PLATFORM}\")\n",
    "\n",
    "# Fonction pour cloner le repository\n",
    "def clone_repository():\n",
    "    \"\"\"Clone le repository avec gestion d'erreurs robuste.\"\"\"\n",
    "    repo_url = \"https://github.com/saito1111/Speaker-diarization-.git\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üì• Clonage du repository depuis: {repo_url}\")\n",
    "        \n",
    "        # Utiliser git clone avec des options pour plus de fiabilit√©\n",
    "        cmd = [\"git\", \"clone\", \"--depth\", \"1\", repo_url, repo_name]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Clonage r√©ussi avec git\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Erreur git: {result.stderr}\")\n",
    "            \n",
    "            # Fallback avec !git pour Colab\n",
    "            print(\"üîÑ Tentative avec commande shell...\")\n",
    "            os.system(f\"git clone --depth 1 {repo_url} {repo_name}\")\n",
    "            return Path(repo_name).exists()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du clonage: {e}\")\n",
    "        return False\n",
    "\n",
    "# V√©rifier si le repository est d√©j√† pr√©sent\n",
    "if (current_dir / repo_name).exists() and (current_dir / repo_name / \"src\").exists():\n",
    "    print(\"‚úÖ Repository d√©j√† pr√©sent et valide\")\n",
    "    repo_path = current_dir / repo_name\n",
    "elif (current_dir / \"src\").exists():\n",
    "    print(\"‚úÖ Nous sommes d√©j√† dans le repository\")\n",
    "    repo_path = current_dir\n",
    "else:\n",
    "    print(\"üì• Repository non trouv√© - clonage n√©cessaire...\")\n",
    "    \n",
    "    if clone_repository():\n",
    "        repo_path = current_dir / repo_name\n",
    "        print(f\"‚úÖ Repository clon√© dans: {repo_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå ERREUR: Impossible de cloner le repository!\")\n",
    "        print(\"üîß Solutions possibles:\")\n",
    "        print(\"   1. V√©rifiez votre connexion internet\")\n",
    "        print(\"   2. Essayez de relancer cette cellule\")\n",
    "        print(\"   3. Clonez manuellement avec: !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "        raise Exception(\"Clonage du repository √©chou√©\")\n",
    "\n",
    "# Changer vers le r√©pertoire du projet si n√©cessaire\n",
    "if repo_path != current_dir:\n",
    "    os.chdir(repo_path)\n",
    "    print(f\"üìÇ Changement de r√©pertoire vers: {Path.cwd()}\")\n",
    "\n",
    "# V√©rifier la structure du projet\n",
    "print(\"\\nüìã V√©rification de la structure du projet:\")\n",
    "current_project_dir = Path.cwd()\n",
    "\n",
    "# Lister le contenu principal\n",
    "print(\"üìÅ Contenu du r√©pertoire principal:\")\n",
    "for item in sorted(current_project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"   üìÅ {item.name}/\")\n",
    "    else:\n",
    "        print(f\"   üìÑ {item.name}\")\n",
    "\n",
    "# V√©rifier le r√©pertoire src\n",
    "src_dir = current_project_dir / \"src\"\n",
    "if src_dir.exists():\n",
    "    print(f\"\\nüìÇ Contenu du r√©pertoire src/ ({len(list(src_dir.glob('*.py')))} fichiers Python):\")\n",
    "    for py_file in sorted(src_dir.glob(\"*.py\"))[:8]:  # Limiter l'affichage\n",
    "        print(f\"   üêç {py_file.name}\")\n",
    "    if len(list(src_dir.glob(\"*.py\"))) > 8:\n",
    "        print(f\"   ... et {len(list(src_dir.glob('*.py'))) - 8} autres fichiers\")\n",
    "    print(\"‚úÖ R√©pertoire src trouv√© et valide\")\n",
    "else:\n",
    "    print(\"‚ùå ERREUR: R√©pertoire src manquant!\")\n",
    "    raise FileNotFoundError(\"Le r√©pertoire src n'existe pas\")\n",
    "\n",
    "print(f\"\\nüéâ Projet configur√© avec succ√®s dans: {current_project_dir}\")\n",
    "\n",
    "# Variables globales pour les autres cellules\n",
    "PROJECT_ROOT = current_project_dir\n",
    "SRC_DIR = src_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis√©e des chemins et imports pour Colab/Kaggle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le r√©pertoire src au PATH Python\n",
    "if 'SRC_DIR' in globals() and SRC_DIR.exists():\n",
    "    if str(SRC_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(SRC_DIR))\n",
    "        print(f\"‚úÖ Ajout√© au PATH Python: {SRC_DIR}\")\n",
    "else:\n",
    "    # Fallback si SRC_DIR n'est pas d√©fini\n",
    "    src_fallback = Path.cwd() / \"src\"\n",
    "    if src_fallback.exists():\n",
    "        sys.path.insert(0, str(src_fallback))\n",
    "        print(f\"‚úÖ Ajout√© au PATH Python (fallback): {src_fallback}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è R√©pertoire src non trouv√© - cr√©ation...\")\n",
    "        src_fallback.mkdir(exist_ok=True)\n",
    "\n",
    "# Cr√©er tous les dossiers n√©cessaires avec gestion d'erreurs\n",
    "directories_to_create = [\n",
    "    \"data/ami_corpus/audio\",\n",
    "    \"data/ami_corpus/annotations\", \n",
    "    \"models/checkpoints\",\n",
    "    \"results/logs\",\n",
    "    \"results/figures\",\n",
    "    \"results/metrics\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Cr√©ation des r√©pertoires de travail...\")\n",
    "for dir_path in directories_to_create:\n",
    "    full_path = Path(dir_path)\n",
    "    try:\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ‚úÖ {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur {dir_path}: {e}\")\n",
    "\n",
    "# Configuration des chemins principaux\n",
    "try:\n",
    "    DATA_DIR = Path('./data/ami_corpus')\n",
    "    AUDIO_DIR = DATA_DIR / 'audio'  \n",
    "    ANNOTATION_DIR = DATA_DIR / 'annotations'\n",
    "    MODEL_DIR = Path('./models/checkpoints')\n",
    "    RESULTS_DIR = Path('./results')\n",
    "    LOGS_DIR = RESULTS_DIR / 'logs'\n",
    "    FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "    \n",
    "    print(f\"\\nüìÇ Chemins configur√©s:\")\n",
    "    print(f\"   üéµ Audio: {AUDIO_DIR}\")\n",
    "    print(f\"   üìù Annotations: {ANNOTATION_DIR}\")\n",
    "    print(f\"   üß† Mod√®les: {MODEL_DIR}\")\n",
    "    print(f\"   üìä R√©sultats: {RESULTS_DIR}\")\n",
    "    print(f\"   üìà Logs: {LOGS_DIR}\")\n",
    "    print(f\"   üìâ Figures: {FIGURES_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur configuration des chemins: {e}\")\n",
    "    raise\n",
    "\n",
    "# V√©rification des permissions d'√©criture\n",
    "print(f\"\\nüîç V√©rification des permissions:\")\n",
    "test_dirs = [DATA_DIR, MODEL_DIR, RESULTS_DIR]\n",
    "for test_dir in test_dirs:\n",
    "    try:\n",
    "        test_file = test_dir / \".test_write\"\n",
    "        test_file.write_text(\"test\")\n",
    "        test_file.unlink()\n",
    "        print(f\"   ‚úÖ √âcriture OK: {test_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur √©criture {test_dir}: {e}\")\n",
    "\n",
    "print(f\"\\nüíæ Espace disque disponible:\")\n",
    "try:\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(Path.cwd())\n",
    "    print(f\"   üíø Total: {total / 1e9:.1f} GB\")\n",
    "    print(f\"   üìä Utilis√©: {used / 1e9:.1f} GB\") \n",
    "    print(f\"   üÜì Libre: {free / 1e9:.1f} GB\")\n",
    "    \n",
    "    if free < 2e9:  # Moins de 2GB libre\n",
    "        print(f\"   ‚ö†Ô∏è Attention: Espace disque faible ({free / 1e9:.1f} GB)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Espace disque suffisant\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Impossible de v√©rifier l'espace disque: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Configuration des chemins termin√©e avec succ√®s!\")\n",
    "\n",
    "# Test d'import des modules du projet\n",
    "print(f\"\\nüß™ Test d'import des modules du projet:\")\n",
    "try:\n",
    "    # Essayer d'importer un module du projet pour v√©rifier que le PATH est correct\n",
    "    from pathlib import Path\n",
    "    print(\"   ‚úÖ Import pathlib OK\")\n",
    "    \n",
    "    # V√©rifier que les fichiers sources sont accessibles\n",
    "    if (Path.cwd() / \"src\").exists():\n",
    "        src_files = list((Path.cwd() / \"src\").glob(\"*.py\"))\n",
    "        print(f\"   üì¶ Fichiers Python trouv√©s: {len(src_files)}\")\n",
    "        for src_file in src_files[:3]:  # Afficher les 3 premiers\n",
    "            print(f\"      - {src_file.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Erreur import: {e}\")\n",
    "\n",
    "print(\"‚úÖ Configuration compl√®te r√©ussie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìä 2. T√©l√©chargement et Pr√©paration du Corpus AMI\n",
    "\n",
    "Le corpus AMI contient des enregistrements de r√©unions avec annotations temporelles des locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_ami"
   },
   "outputs": [],
   "source": [
    "# T√©l√©chargement optimis√© du corpus AMI pour Colab/Kaggle\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "class AMICorpusDownloader:\n",
    "    \"\"\"Gestionnaire pour le t√©l√©chargement du corpus AMI optimis√© pour Colab/Kaggle.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.audio_dir = self.base_dir / \"ami_audio\"\n",
    "        self.annotation_dir = self.base_dir / \"ami_annotations\"\n",
    "        self.download_dir = self.base_dir / \"downloads\"\n",
    "        \n",
    "        # Cr√©er les r√©pertoires\n",
    "        for dir_path in [self.audio_dir, self.annotation_dir, self.download_dir]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def download_with_progress(self, url, filename, description=\"T√©l√©chargement\"):\n",
    "        \"\"\"T√©l√©charge un fichier avec barre de progression.\"\"\"\n",
    "        print(f\"üîΩ {description}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Utiliser wget pour plus de fiabilit√©\n",
    "            cmd = f\"wget -O '{filename}' '{url}' --progress=bar --show-progress\"\n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ T√©l√©charg√©: {filename}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Erreur wget: {result.stderr}\")\n",
    "                # Fallback avec urllib\n",
    "                urllib.request.urlretrieve(url, filename)\n",
    "                print(f\"‚úÖ T√©l√©charg√© (fallback): {filename}\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur t√©l√©chargement: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_ami_sample_data(self):\n",
    "        \"\"\"T√©l√©charge un √©chantillon de donn√©es AMI pour l'entra√Ænement de d√©monstration.\"\"\"\n",
    "        print(\"üéµ T√©l√©chargement d'√©chantillons de donn√©es AMI...\")\n",
    "        \n",
    "        # URLs directes vers des √©chantillons AMI disponibles publiquement\n",
    "        sample_files = {\n",
    "            # Utilisation d'√©chantillons plus accessibles\n",
    "            \"ES2002a_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "            \"ES2002b_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "            \"ES2003a_sample.wav\": \"https://github.com/pytorch/audio/raw/main/test/assets/steam-train-whistle-daniel_simon.wav\",\n",
    "        }\n",
    "        \n",
    "        downloaded_files = []\n",
    "        \n",
    "        for filename, url in sample_files.items():\n",
    "            audio_path = self.audio_dir / filename\n",
    "            \n",
    "            if audio_path.exists() and audio_path.stat().st_size > 10000:\n",
    "                print(f\"‚úÖ D√©j√† pr√©sent: {filename}\")\n",
    "                downloaded_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            if self.download_with_progress(url, str(audio_path), f\"Audio {filename}\"):\n",
    "                downloaded_files.append(filename)\n",
    "        \n",
    "        return downloaded_files\n",
    "    \n",
    "    def create_demo_annotations(self):\n",
    "        \"\"\"Cr√©e des annotations de d√©monstration pour les fichiers audio.\"\"\"\n",
    "        print(\"üìù Cr√©ation d'annotations de d√©monstration...\")\n",
    "        \n",
    "        audio_files = list(self.audio_dir.glob(\"*.wav\"))\n",
    "        created_annotations = []\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            rttm_file = self.annotation_dir / f\"{audio_file.stem}.rttm\"\n",
    "            \n",
    "            if rttm_file.exists():\n",
    "                print(f\"‚úÖ Annotation existante: {rttm_file.name}\")\n",
    "                created_annotations.append(rttm_file)\n",
    "                continue\n",
    "            \n",
    "            # Cr√©er une annotation RTTM de base\n",
    "            # Format RTTM: SPEAKER filename 1 start_time duration <NA> <NA> speaker_id <NA> <NA>\n",
    "            demo_annotation = \"\"\"SPEAKER ES2002a_sample 1 0.0 5.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 5.0 5.0 <NA> <NA> B <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 10.0 5.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER ES2002a_sample 1 15.0 5.0 <NA> <NA> C <NA> <NA>\n",
    "\"\"\"\n",
    "            \n",
    "            # Adapter le nom du fichier dans l'annotation\n",
    "            demo_annotation = demo_annotation.replace(\"ES2002a_sample\", audio_file.stem)\n",
    "            \n",
    "            with open(rttm_file, 'w') as f:\n",
    "                f.write(demo_annotation)\n",
    "            \n",
    "            print(f\"‚úÖ Annotation cr√©√©e: {rttm_file.name}\")\n",
    "            created_annotations.append(rttm_file)\n",
    "        \n",
    "        return created_annotations\n",
    "    \n",
    "    def download_real_ami_annotations(self):\n",
    "        \"\"\"T√©l√©charge les vraies annotations AMI si disponibles.\"\"\"\n",
    "        print(\"üìù Tentative de t√©l√©chargement des annotations AMI r√©elles...\")\n",
    "        \n",
    "        # URL alternative pour les annotations AMI\n",
    "        annotation_urls = [\n",
    "            \"https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip\",\n",
    "            \"https://raw.githubusercontent.com/pyannote/AMI-diarization-setup/master/ami/only_words/train.uem.txt\"  # Alternative\n",
    "        ]\n",
    "        \n",
    "        for i, url in enumerate(annotation_urls):\n",
    "            zip_path = self.download_dir / f\"ami_annotations_{i}.zip\"\n",
    "            \n",
    "            try:\n",
    "                if self.download_with_progress(url, str(zip_path), f\"Annotations AMI ({i+1})\"):\n",
    "                    # Essayer d'extraire si c'est un zip\n",
    "                    if zip_path.suffix == '.zip':\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                                zip_ref.extractall(self.annotation_dir)\n",
    "                            print(f\"‚úÖ Extraction r√©ussie: {zip_path.name}\")\n",
    "                            return True\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Erreur extraction: {e}\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        # D√©placer le fichier vers le r√©pertoire d'annotations\n",
    "                        shutil.move(str(zip_path), str(self.annotation_dir / zip_path.name))\n",
    "                        return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è √âchec URL {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"‚ö†Ô∏è T√©l√©chargement annotations r√©elles √©chou√© - utilisation annotations de d√©monstration\")\n",
    "        return False\n",
    "    \n",
    "    def setup_corpus(self):\n",
    "        \"\"\"Configure le corpus AMI complet.\"\"\"\n",
    "        print(\"\ude80 Configuration du corpus AMI...\")\n",
    "        \n",
    "        # √âtape 1: T√©l√©charger les fichiers audio\n",
    "        audio_files = self.download_ami_sample_data()\n",
    "        \n",
    "        # √âtape 2: Essayer les vraies annotations, sinon cr√©er des d√©mos\n",
    "        if not self.download_real_ami_annotations():\n",
    "            annotation_files = self.create_demo_annotations()\n",
    "        else:\n",
    "            annotation_files = list(self.annotation_dir.rglob(\"*.rttm\"))\n",
    "        \n",
    "        print(f\"\\n\udcca Corpus configur√©:\")\n",
    "        print(f\"   üéµ Fichiers audio: {len(audio_files)}\")\n",
    "        print(f\"   üìù Fichiers annotation: {len(annotation_files)}\")\n",
    "        \n",
    "        return {\n",
    "            'audio_files': audio_files,\n",
    "            'annotation_files': annotation_files,\n",
    "            'ready': len(audio_files) > 0 and len(annotation_files) > 0\n",
    "        }\n",
    "\n",
    "# Initialiser et configurer le corpus AMI\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ CONFIGURATION DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ami_downloader = AMICorpusDownloader(DATA_DIR)\n",
    "corpus_status = ami_downloader.setup_corpus()\n",
    "\n",
    "if corpus_status['ready']:\n",
    "    print(\"\\n‚úÖ Corpus AMI configur√© avec succ√®s!\")\n",
    "    print(\"üöÄ Pr√™t pour l'entra√Ænement de diarization\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Erreur dans la configuration du corpus\")\n",
    "    raise Exception(\"Configuration corpus √©chou√©e\")\n",
    "\n",
    "# Configurer les chemins finaux\n",
    "AUDIO_PATH = ami_downloader.audio_dir\n",
    "ANNOTATION_PATH = ami_downloader.annotation_dir\n",
    "\n",
    "print(f\"\\nüìÇ Chemins configur√©s:\")\n",
    "print(f\"   üéµ Audio: {AUDIO_PATH}\")\n",
    "print(f\"   üìù Annotations: {ANNOTATION_PATH}\")\n",
    "print(f\"   üìÅ Fichiers disponibles: {len(list(AUDIO_PATH.glob('*.wav')))} audio, {len(list(ANNOTATION_PATH.glob('*.rttm')))} RTTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_ami_data"
   },
   "outputs": [],
   "source": [
    "# Validation finale et pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "def validate_ami_corpus():\n",
    "    \"\"\"Valide que le corpus AMI est pr√™t pour l'entra√Ænement.\"\"\"\n",
    "    print(\"üîç Validation finale du corpus AMI...\")\n",
    "    \n",
    "    audio_files = list(AUDIO_PATH.glob(\"*.wav\"))\n",
    "    annotation_files = list(ANNOTATION_PATH.glob(\"*.rttm\"))\n",
    "    \n",
    "    print(f\"\\nüìä Inventaire final:\")\n",
    "    print(f\"   üéµ Fichiers audio: {len(audio_files)}\")\n",
    "    print(f\"   üìù Fichiers RTTM: {len(annotation_files)}\")\n",
    "    \n",
    "    # Validation des paires audio-annotation\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        matching_rttm = ANNOTATION_PATH / f\"{base_name}.rttm\"\n",
    "        \n",
    "        if matching_rttm.exists():\n",
    "            valid_pairs.append((audio_file, matching_rttm))\n",
    "            print(f\"   ‚úÖ Paire valide: {base_name}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Pas d'annotation pour: {base_name}\")\n",
    "    \n",
    "    # Afficher quelques exemples\n",
    "    if audio_files:\n",
    "        print(f\"\\nüéµ Exemples audio:\")\n",
    "        for audio_file in audio_files[:3]:\n",
    "            try:\n",
    "                size_mb = audio_file.stat().st_size / (1024*1024)\n",
    "                print(f\"   - {audio_file.name} ({size_mb:.1f} MB)\")\n",
    "            except:\n",
    "                print(f\"   - {audio_file.name} (taille non disponible)\")\n",
    "    \n",
    "    if annotation_files:\n",
    "        print(f\"\\nüìù Exemples annotations:\")\n",
    "        for rttm_file in annotation_files[:3]:\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    lines = len(f.readlines())\n",
    "                print(f\"   - {rttm_file.name} ({lines} segments)\")\n",
    "            except:\n",
    "                print(f\"   - {rttm_file.name} (lecture √©chou√©e)\")\n",
    "    \n",
    "    # Statut global\n",
    "    corpus_ready = len(valid_pairs) >= 1  # Au moins une paire valide pour la d√©monstration\n",
    "    \n",
    "    if corpus_ready:\n",
    "        print(f\"\\n‚úÖ Corpus AMI valid√© et pr√™t!\")\n",
    "        print(f\"üöÄ {len(valid_pairs)} paires audio-annotation disponibles pour l'entra√Ænement\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Corpus invalide - aucune paire audio-annotation trouv√©e\")\n",
    "    \n",
    "    return {\n",
    "        'ready': corpus_ready,\n",
    "        'audio_count': len(audio_files),\n",
    "        'annotation_count': len(annotation_files),\n",
    "        'valid_pairs': valid_pairs,\n",
    "        'audio_files': audio_files,\n",
    "        'annotation_files': annotation_files\n",
    "    }\n",
    "\n",
    "# Valider le corpus\n",
    "corpus_validation = validate_ami_corpus()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéØ CORPUS AMI: {'PR√äT' if corpus_validation['ready'] else 'ERREUR'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if not corpus_validation['ready']:\n",
    "    raise Exception(\"Corpus AMI non valide - impossible de continuer l'entra√Ænement\")\n",
    "\n",
    "# Configuration finale pour l'entra√Ænement\n",
    "FINAL_AUDIO_DIR = AUDIO_PATH\n",
    "FINAL_ANNOTATION_DIR = ANNOTATION_PATH\n",
    "\n",
    "print(f\"\\nüìÇ Configuration finale pour l'entra√Ænement:\")\n",
    "print(f\"   üéµ R√©pertoire audio: {FINAL_AUDIO_DIR}\")\n",
    "print(f\"   üìù R√©pertoire annotations: {FINAL_ANNOTATION_DIR}\")\n",
    "print(f\"   üìä Paires disponibles: {len(corpus_validation['valid_pairs'])}\")\n",
    "print(f\"   üöÄ Pr√™t pour l'entra√Ænement de diarization!\")\n",
    "\n",
    "# Sauvegarder les informations du corpus pour les √©tapes suivantes\n",
    "corpus_info = {\n",
    "    'audio_dir': str(FINAL_AUDIO_DIR),\n",
    "    'annotation_dir': str(FINAL_ANNOTATION_DIR),\n",
    "    'valid_pairs': [(str(a), str(r)) for a, r in corpus_validation['valid_pairs']],\n",
    "    'total_audio_files': len(corpus_validation['audio_files']),\n",
    "    'total_annotation_files': len(corpus_validation['annotation_files']),\n",
    "    'platform': PLATFORM\n",
    "}\n",
    "\n",
    "# Sauvegarder dans un fichier JSON pour r√©f√©rence\n",
    "corpus_info_path = Path(\"corpus_info.json\")\n",
    "with open(corpus_info_path, 'w') as f:\n",
    "    json.dump(corpus_info, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Informations du corpus sauv√©es dans: {corpus_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "### üìä Division des Donn√©es (Train/Eval)\n",
    "\n",
    "Division stratifi√©e du corpus AMI selon les bonnes pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_ami_data"
   },
   "outputs": [],
   "source": [
    "def create_ami_splits(audio_dir, rttm_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divise le corpus AMI en ensembles d'entra√Ænement, validation et test.\n",
    "    Adapt√© pour les √©chantillons de d√©monstration et les vraies donn√©es AMI.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: R√©pertoire des fichiers audio\n",
    "        rttm_dir: R√©pertoire des annotations RTTM\n",
    "        train_ratio: Proportion pour l'entra√Ænement\n",
    "        val_ratio: Proportion pour la validation\n",
    "        test_ratio: Proportion pour le test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les listes de fichiers pour chaque split\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Les ratios doivent sommer √† 1.0\"\n",
    "    \n",
    "    # Lister tous les fichiers disponibles\n",
    "    audio_path = Path(audio_dir)\n",
    "    rttm_path = Path(rttm_dir)\n",
    "    \n",
    "    # Trouver les fichiers audio\n",
    "    audio_extensions = ['.wav', '.mp3', '.flac']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(audio_path.glob(f'*{ext}')))\n",
    "    \n",
    "    print(f\"üîç Fichiers audio trouv√©s: {len(audio_files)}\")\n",
    "    for af in audio_files[:5]:  # Afficher les premiers\n",
    "        print(f\"   - {af.name}\")\n",
    "    \n",
    "    # V√©rifier la correspondance audio-RTTM\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        # Chercher les fichiers RTTM correspondants\n",
    "        possible_rttm_names = [\n",
    "            f\"{base_name}.rttm\",\n",
    "            f\"{base_name.replace('_sample', '')}.rttm\",  # Pour les √©chantillons\n",
    "            f\"{base_name.split('_')[0]}.rttm\"  # Nom de base\n",
    "        ]\n",
    "        \n",
    "        rttm_file = None\n",
    "        for rttm_name in possible_rttm_names:\n",
    "            potential_rttm = rttm_path / rttm_name\n",
    "            if potential_rttm.exists():\n",
    "                rttm_file = potential_rttm\n",
    "                break\n",
    "        \n",
    "        if rttm_file:\n",
    "            # V√©rifier que le fichier RTTM n'est pas vide\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                if content:\n",
    "                    valid_pairs.append({\n",
    "                        'base_name': base_name,\n",
    "                        'audio_file': str(audio_file),\n",
    "                        'rttm_file': str(rttm_file)\n",
    "                    })\n",
    "                    print(f\"   ‚úÖ Paire valide: {base_name} -> {rttm_file.name}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è RTTM vide: {rttm_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur lecture RTTM {rttm_file.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Pas de RTTM pour: {base_name}\")\n",
    "    \n",
    "    print(f\"\\nüìä Paires audio-RTTM valides trouv√©es: {len(valid_pairs)}\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        print(\"‚ùå Aucune paire valide trouv√©e!\")\n",
    "        \n",
    "        # Cr√©er une paire de d√©monstration minimale si n√©cessaire\n",
    "        if len(audio_files) > 0:\n",
    "            print(\"üõ†Ô∏è Cr√©ation d'une configuration minimale pour la d√©monstration...\")\n",
    "            audio_file = audio_files[0]\n",
    "            demo_rttm = rttm_path / f\"{audio_file.stem}.rttm\"\n",
    "            \n",
    "            # Cr√©er un fichier RTTM minimal\n",
    "            demo_content = f\"\"\"SPEAKER {audio_file.stem} 1 0.0 10.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER {audio_file.stem} 1 10.0 10.0 <NA> <NA> B <NA> <NA>\n",
    "\"\"\"\n",
    "            with open(demo_rttm, 'w') as f:\n",
    "                f.write(demo_content)\n",
    "            \n",
    "            valid_pairs = [{\n",
    "                'base_name': audio_file.stem,\n",
    "                'audio_file': str(audio_file),\n",
    "                'rttm_file': str(demo_rttm)\n",
    "            }]\n",
    "            \n",
    "            print(f\"‚úÖ Configuration de d√©monstration cr√©√©e: 1 paire\")\n",
    "        else:\n",
    "            raise ValueError(\"Aucun fichier audio disponible!\")\n",
    "    \n",
    "    # M√©langer et diviser\n",
    "    import random\n",
    "    random.seed(42)  # Pour la reproductibilit√©\n",
    "    random.shuffle(valid_pairs)\n",
    "    \n",
    "    n_total = len(valid_pairs)\n",
    "    \n",
    "    # Adaptation pour les petits datasets\n",
    "    if n_total == 1:\n",
    "        # Utiliser le m√™me fichier pour train/val/test en mode d√©monstration\n",
    "        print(\"‚ö†Ô∏è Un seul fichier disponible - utilisation pour train/val/test\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs\n",
    "        test_files = valid_pairs\n",
    "    elif n_total < 3:\n",
    "        # Tr√®s petit dataset\n",
    "        print(f\"‚ö†Ô∏è Dataset tr√®s petit ({n_total} fichiers) - division adapt√©e\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs[:1]\n",
    "        test_files = valid_pairs[:1]\n",
    "    else:\n",
    "        # Division normale\n",
    "        n_train = max(1, int(n_total * train_ratio))\n",
    "        n_val = max(1, int(n_total * val_ratio))\n",
    "        \n",
    "        train_files = valid_pairs[:n_train]\n",
    "        val_files = valid_pairs[n_train:n_train + n_val]\n",
    "        test_files = valid_pairs[n_train + n_val:] if n_train + n_val < n_total else valid_pairs[-1:]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_files,\n",
    "        'validation': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    \n",
    "    # Afficher le r√©sum√©\n",
    "    print(f\"\\n\udccb Division des donn√©es:\")\n",
    "    print(f\"   üèãÔ∏è Entra√Ænement: {len(splits['train'])} fichiers\")\n",
    "    print(f\"   üîç Validation: {len(splits['validation'])} fichiers\") \n",
    "    print(f\"   üß™ Test: {len(splits['test'])} fichiers\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Cr√©er la division des donn√©es\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DIVISION DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    data_splits = create_ami_splits(FINAL_AUDIO_DIR, FINAL_ANNOTATION_DIR)\n",
    "    \n",
    "    # Sauvegarder les splits\n",
    "    splits_file = Path(\"data_splits.json\")\n",
    "    with open(splits_file, 'w') as f:\n",
    "        json.dump(data_splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Splits sauv√©s dans: {splits_file}\")\n",
    "    print(\"‚úÖ Division des donn√©es r√©ussie!\")\n",
    "    \n",
    "    # Variables globales pour les autres cellules\n",
    "    TRAIN_FILES = data_splits['train']\n",
    "    VAL_FILES = data_splits['validation'] \n",
    "    TEST_FILES = data_splits['test']\n",
    "    \n",
    "    print(f\"\\nüéØ Pr√™t pour l'entra√Ænement avec:\")\n",
    "    print(f\"   - {len(TRAIN_FILES)} fichiers d'entra√Ænement\")\n",
    "    print(f\"   - {len(VAL_FILES)} fichiers de validation\")\n",
    "    print(f\"   - {len(TEST_FILES)} fichiers de test\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la division des donn√©es: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## üõ†Ô∏è 3. Pr√©paration des Donn√©es et Extraction de Caract√©ristiques\n",
    "\n",
    "Extraction des caract√©ristiques multi-canaux: LPS, IPD, AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "# Configuration du path et import des modules du projet\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le r√©pertoire src au path Python\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / 'src'\n",
    "\n",
    "print(f\"üìÅ R√©pertoire courant: {current_dir}\")\n",
    "print(f\"üìÇ R√©pertoire src: {src_path}\")\n",
    "print(f\"üìÇ V√©rification existence src: {src_path.exists()}\")\n",
    "\n",
    "# V√©rifier si le r√©pertoire src existe\n",
    "if not src_path.exists():\n",
    "    print(\"‚ùå ERREUR: Le r√©pertoire 'src' n'existe pas!\")\n",
    "    print(\"üîß Assurez-vous d'avoir clon√© le repository complet avec:\")\n",
    "    print(\"   !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "    print(\"   %cd Speaker-diarization-\")\n",
    "    raise FileNotFoundError(\"R√©pertoire 'src' manquant. Clonez d'abord le repository.\")\n",
    "\n",
    "# Lister les fichiers dans src pour v√©rification\n",
    "py_files = list(src_path.glob(\"*.py\"))\n",
    "print(f\"üìÑ Fichiers Python trouv√©s: {[f.name for f in py_files]}\")\n",
    "\n",
    "# V√©rifier que TOUS les modules requis existent\n",
    "required_modules = [\n",
    "    'tcn_diarization_model.py',\n",
    "    'metrics.py', \n",
    "    'dataset.py',\n",
    "    'diarization_losses.py',\n",
    "    'improved_trainer.py',\n",
    "    'optimized_dataloader.py',\n",
    "    'optimized_dataset.py'\n",
    "]\n",
    "\n",
    "missing_modules = []\n",
    "for module in required_modules:\n",
    "    if not (src_path / module).exists():\n",
    "        missing_modules.append(module)\n",
    "\n",
    "if missing_modules:\n",
    "    print(f\"‚ùå ERREUR: Modules manquants dans src/: {missing_modules}\")\n",
    "    print(\"üîß V√©rifiez que tous les fichiers sont pr√©sents dans le repository\")\n",
    "    raise FileNotFoundError(f\"Modules manquants: {missing_modules}\")\n",
    "\n",
    "# Ajouter src au path\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"‚úÖ R√©pertoire src ajout√© au path: {src_path}\")\n",
    "\n",
    "# Import de TOUS les modules du projet (OBLIGATOIRES - pas de fallback)\n",
    "print(\"\\nüîÑ Import de TOUS les modules du projet...\")\n",
    "\n",
    "try:\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    print(\"‚úÖ tcn_diarization_model import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer tcn_diarization_model\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier tcn_diarization_model.py\")\n",
    "    raise ImportError(\"Module tcn_diarization_model requis\")\n",
    "\n",
    "try:\n",
    "    from metrics import DiarizationMetrics\n",
    "    print(\"‚úÖ metrics import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer metrics\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier metrics.py\")\n",
    "    raise ImportError(\"Module metrics requis\")\n",
    "\n",
    "try:\n",
    "    from dataset import DiarizationDataset\n",
    "    print(\"‚úÖ dataset import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier dataset.py\")\n",
    "    raise ImportError(\"Module dataset requis\")\n",
    "\n",
    "try:\n",
    "    from diarization_losses import MultiTaskDiarizationLoss\n",
    "    print(\"‚úÖ diarization_losses import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer diarization_losses\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier diarization_losses.py\")\n",
    "    raise ImportError(\"Module diarization_losses requis\")\n",
    "\n",
    "try:\n",
    "    from improved_trainer import ImprovedDiarizationTrainer\n",
    "    print(\"‚úÖ improved_trainer import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer improved_trainer\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier improved_trainer.py\")\n",
    "    raise ImportError(\"Module improved_trainer requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataloader import create_optimized_dataloaders\n",
    "    print(\"‚úÖ optimized_dataloader import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer optimized_dataloader\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier optimized_dataloader.py\")\n",
    "    raise ImportError(\"Module optimized_dataloader requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataset import OptimizedDiarizationDataset\n",
    "    print(\"‚úÖ optimized_dataset import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer optimized_dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier optimized_dataset.py\")\n",
    "    raise ImportError(\"Module optimized_dataset requis\")\n",
    "\n",
    "print(\"\\nüéâ TOUS LES MODULES IMPORT√âS AVEC SUCC√àS!\")\n",
    "print(\"üìã Modules disponibles pour l'entra√Ænement:\")\n",
    "print(\"   - DiarizationTCN: ‚úÖ\")\n",
    "print(\"   - DiarizationMetrics: ‚úÖ\")\n",
    "print(\"   - DiarizationDataset: ‚úÖ\")\n",
    "print(\"   - MultiTaskDiarizationLoss: ‚úÖ\")\n",
    "print(\"   - ImprovedDiarizationTrainer: ‚úÖ\")\n",
    "print(\"   - OptimizedDataLoader: ‚úÖ\")\n",
    "print(\"   - OptimizedDataset: ‚úÖ\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√™t pour l'entra√Ænement avec TOUS vos mod√®les originaux!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes utilitaires n√©cessaires (non pr√©sentes dans les modules du projet)\n",
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# MemoryMonitor pour surveiller l'utilisation m√©moire\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Retourne les informations m√©moire.\"\"\"\n",
    "        # RAM\n",
    "        ram_info = psutil.virtual_memory()\n",
    "        ram_percent = ram_info.percent\n",
    "        \n",
    "        # GPU\n",
    "        gpu_percent = 0\n",
    "        gpu_memory_used = 0\n",
    "        gpu_memory_total = 0\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0)\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "            gpu_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "        \n",
    "        return {\n",
    "            'ram_percent': ram_percent,\n",
    "            'gpu_percent': gpu_percent,\n",
    "            'gpu_memory_used': gpu_memory_used,\n",
    "            'gpu_memory_total': gpu_memory_total\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Nettoie la m√©moire.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Classes utilitaires cr√©√©es (MemoryMonitor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Test de l'extraction de caract√©ristiques\n",
    "print(\"üß™ Test de l'extraction de caract√©ristiques...\")\n",
    "\n",
    "# Cr√©er un extracteur de caract√©ristiques\n",
    "feature_extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=512,\n",
    "    hop_length=256\n",
    ")\n",
    "\n",
    "# Cr√©er des donn√©es audio fictives (8 canaux)\n",
    "n_channels = 8\n",
    "duration = 4.0  # 4 secondes\n",
    "sample_rate = 16000\n",
    "n_samples = int(duration * sample_rate)\n",
    "\n",
    "# Simuler audio multi-canal avec du bruit et des signaux\n",
    "waveforms = []\n",
    "for ch in range(n_channels):\n",
    "    # Signal de base + bruit\n",
    "    base_signal = np.sin(2 * np.pi * 440 * np.linspace(0, duration, n_samples))  # 440 Hz\n",
    "    noise = np.random.normal(0, 0.1, n_samples)\n",
    "    # Ajouter un l√©ger d√©calage temporel pour simuler la spatialisation\n",
    "    delay_samples = int(0.001 * ch * sample_rate)  # 1ms de d√©lai par canal\n",
    "    delayed_signal = np.zeros(n_samples)\n",
    "    if delay_samples < n_samples:\n",
    "        delayed_signal[delay_samples:] = base_signal[:n_samples-delay_samples]\n",
    "    \n",
    "    final_signal = delayed_signal + noise\n",
    "    waveforms.append(torch.tensor(final_signal, dtype=torch.float32))\n",
    "\n",
    "print(f\"üìä Audio g√©n√©r√©: {n_channels} canaux, {duration}s, {sample_rate} Hz\")\n",
    "\n",
    "# Test d'extraction\n",
    "features = feature_extractor.extract_features(waveforms)\n",
    "print(f\"‚úÖ Caract√©ristiques extraites: {features.shape}\")\n",
    "print(f\"   - Dimensions attendues: [771, ~{int(duration * sample_rate / 256)}]\")\n",
    "print(f\"   - Dimensions obtenues: {list(features.shape)}\")\n",
    "\n",
    "# Analyser les caract√©ristiques\n",
    "print(f\"\\nüìà Statistiques des caract√©ristiques:\")\n",
    "print(f\"   - Min: {features.min():.3f}\")\n",
    "print(f\"   - Max: {features.max():.3f}\")\n",
    "print(f\"   - Moyenne: {features.mean():.3f}\")\n",
    "print(f\"   - Std: {features.std():.3f}\")\n",
    "\n",
    "# Visualiser les caract√©ristiques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Spectrogramme des caract√©ristiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(features.numpy()[:100, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques LPS (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(features.numpy()[257:357, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques IPD (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(features.numpy()[500:600, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques AF (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(features.numpy().mean(axis=0))\n",
    "plt.title('√ânergie moyenne par frame')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('√ânergie moyenne')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Cr√©er le r√©pertoire de r√©sultats s'il n'existe pas\n",
    "import os\n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(RESULTS_DIR / 'feature_extraction_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Test d'extraction de caract√©ristiques termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## üß† 4. Configuration du Mod√®le et Entra√Ænement\n",
    "\n",
    "Configuration optimale pour le corpus AMI avec les meilleures pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis√©e pour AMI corpus\n",
    "config = {\n",
    "    # === MOD√àLE ===\n",
    "    'model': {\n",
    "        'input_dim': 771,  # LPS (257) + IPD (257*4) + AF (257*4) = 2313 ‚Üí 771 apr√®s agr√©gation\n",
    "        'hidden_channels': [256, 256, 256, 512, 512],  # Architecture TCN multi-√©chelle\n",
    "        'kernel_size': 3,\n",
    "        'num_speakers': 4,  # AMI corpus a typiquement 3-4 locuteurs\n",
    "        'dropout': 0.2,\n",
    "        'use_attention': True,  # Auto-attention pour d√©pendances long-terme\n",
    "        'use_speaker_classifier': True,  # Classification de locuteurs\n",
    "        'embedding_dim': 256\n",
    "    },\n",
    "    \n",
    "    # === FONCTION DE PERTE ===\n",
    "    'loss': {\n",
    "        'type': 'multitask',\n",
    "        'vad_weight': 1.0,\n",
    "        'osd_weight': 1.0,\n",
    "        'consistency_weight': 0.1,\n",
    "        'use_pit': True,  # Permutation Invariant Training\n",
    "        'use_focal': True,  # Focal Loss pour donn√©es d√©s√©quilibr√©es\n",
    "        'focal_gamma': 2.0,\n",
    "        'num_speakers': 4\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISEUR ===\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 1e-3,  # Learning rate initial\n",
    "        'weight_decay': 1e-4,\n",
    "        'betas': (0.9, 0.999)\n",
    "    },\n",
    "    \n",
    "    # === PLANIFICATEUR LR ===\n",
    "    'scheduler': {\n",
    "        'type': 'onecycle',  # OneCycleLR pour convergence rapide\n",
    "        'steps_per_epoch': 100,  # Sera mis √† jour automatiquement\n",
    "        'pct_start': 0.3  # 30% mont√©e, 70% descente\n",
    "    },\n",
    "    \n",
    "    # === ENTRA√éNEMENT ===\n",
    "    'training': {\n",
    "        'epochs': 50,  # R√©duit pour Colab\n",
    "        'batch_size': 8,  # Adapt√© √† la m√©moire Colab\n",
    "        'num_workers': 2  # Moins de workers pour √©viter les probl√®mes m√©moire\n",
    "    },\n",
    "    \n",
    "    # === DONN√âES ===\n",
    "    'data': {\n",
    "        'segment_duration': 4.0,  # Segments de 4 secondes\n",
    "        'sample_rate': 16000,\n",
    "        'train_split': 0.7,\n",
    "        'max_segments': 1000  # Limite pour Colab\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISATIONS AVANC√âES ===\n",
    "    'accumulation_steps': 4,  # Batch effectif = 8*4 = 32\n",
    "    'use_amp': True,  # Pr√©cision mixte\n",
    "    'grad_clip_norm': 1.0,\n",
    "    'patience': 10,\n",
    "    'save_every': 5,\n",
    "    \n",
    "    # === MONITORING ===\n",
    "    'use_wandb': True,  # Weights & Biases (optionnel)\n",
    "    'project_name': 'ami-speaker-diarization',\n",
    "    'memory_threshold': 0.85,  # Gestion m√©moire Colab\n",
    "    'adaptive_batch': True,\n",
    "    'speaker_loss_weight': 0.5,\n",
    "    \n",
    "    # === CHEMINS ===\n",
    "    'save_dir': str(MODEL_DIR),\n",
    "    'results_dir': str(RESULTS_DIR)\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration cr√©√©e avec les param√®tres suivants:\")\n",
    "print(f\"   - Architecture: TCN {config['model']['hidden_channels']}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']} (effectif: {config['training']['batch_size'] * config['accumulation_steps']})\")\n",
    "print(f\"   - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Pr√©cision mixte: {config['use_amp']}\")\n",
    "print(f\"   - Classification locuteurs: {config['model']['use_speaker_classifier']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "config_file = MODEL_DIR / 'training_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration sauvegard√©e: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration de Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "use_wandb = config.get('use_wandb', False)\n",
    "\n",
    "if use_wandb:\n",
    "    try:\n",
    "        # Connexion √† wandb (n√©cessite un compte gratuit)\n",
    "        wandb.login()\n",
    "        \n",
    "        # Initialisation du projet\n",
    "        wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"ami-tcn-{torch.cuda.get_device_name(0).replace(' ', '-') if torch.cuda.is_available() else 'cpu'}\",\n",
    "            tags=['ami-corpus', 'tcn', 'multi-channel', 'colab'],\n",
    "            notes=\"Entra√Ænement sur corpus AMI avec architecture TCN am√©lior√©e\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Weights & Biases configur√©!\")\n",
    "        print(f\"üìä Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur wandb: {e}\")\n",
    "        print(\"üìà Entra√Ænement sans monitoring wandb...\")\n",
    "        config['use_wandb'] = False\n",
    "else:\n",
    "    print(\"üìà Entra√Ænement sans monitoring wandb (d√©sactiv√© dans config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ 5. Entra√Ænement du Mod√®le\n",
    "\n",
    "Entra√Ænement avec toutes les optimisations: gestion m√©moire, precision mixte, accumulation de gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation des DataLoaders optimis√©s\n",
    "print(\"üîÑ Cr√©ation des DataLoaders avec vos modules optimis√©s...\")\n",
    "\n",
    "# Utiliser la fonction create_optimized_dataloaders import√©e\n",
    "train_loader, val_loader = create_optimized_dataloaders(\n",
    "    audio_dir=audio_dir,\n",
    "    rttm_dir=rttm_dir,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    train_split=config['data']['train_split'],\n",
    "    num_workers=config['training']['num_workers'],\n",
    "    segment_duration=config['data']['segment_duration'],\n",
    "    sample_rate=config['data']['sample_rate'],\n",
    "    max_segments=config['data']['max_segments'],\n",
    "    memory_threshold=config['memory_threshold'],\n",
    "    adaptive_batch=config['adaptive_batch'],\n",
    "    accumulation_steps=config['accumulation_steps']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders cr√©√©s avec succ√®s!\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Mettre √† jour la configuration avec le nombre r√©el de steps\n",
    "config['scheduler']['steps_per_epoch'] = len(train_loader)\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\nüß™ Test d'un batch d'entra√Ænement...\")\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f\"   Batch {batch_idx}:\")\n",
    "    print(f\"     - Features: {batch['features'].shape}\")\n",
    "    print(f\"     - VAD labels: {batch['vad_labels'].shape}\")\n",
    "    print(f\"     - OSD labels: {batch['osd_labels'].shape}\")\n",
    "    \n",
    "    # V√©rifier les dimensions\n",
    "    assert batch['features'].shape[1] == 771, f\"Dimension features incorrecte: {batch['features'].shape[1]} != 771\"\n",
    "    assert batch['vad_labels'].shape[-1] == 4, f\"Nombre de locuteurs incorrect: {batch['vad_labels'].shape[-1]} != 4\"\n",
    "    \n",
    "    print(f\"     ‚úÖ Dimensions correctes!\")\n",
    "    break\n",
    "\n",
    "print(\"‚úÖ DataLoaders optimis√©s configur√©s et test√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialisation du trainer avanc√©\n",
    "print(\"üß† Initialisation du trainer avec vos modules optimis√©s...\")\n",
    "\n",
    "# Cr√©er le trainer avec toutes les am√©liorations (OBLIGATOIRE - pas de fallback)\n",
    "trainer = ImprovedDiarizationTrainer(config)\n",
    "\n",
    "print(f\"‚úÖ Trainer initialis√© avec succ√®s!\")\n",
    "print(f\"   - Mod√®le: {trainer.model.get_num_params():,} param√®tres\")\n",
    "print(f\"   - Device: {trainer.device}\")\n",
    "print(f\"   - Pr√©cision mixte: {trainer.use_amp}\")\n",
    "print(f\"   - Accumulation gradients: {trainer.accumulation_steps}\")\n",
    "\n",
    "# Test du forward pass\n",
    "print(\"\\nüß™ Test du mod√®le...\")\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test avec un batch de donn√©es\n",
    "    test_input = torch.randn(2, 771, 250).to(trainer.device)\n",
    "    \n",
    "    # Forward pass simple\n",
    "    vad_out, osd_out = model(test_input)\n",
    "    print(f\"   Forward simple: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "    \n",
    "    # Forward avec embeddings si disponible\n",
    "    try:\n",
    "        vad_out, osd_out, embeddings, speaker_logits = model(test_input, return_embeddings=True)\n",
    "        print(f\"   Forward complet: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        print(f\"                   Embeddings {embeddings.shape}, Speaker {speaker_logits.shape}\")\n",
    "    except:\n",
    "        print(\"   Forward avec embeddings non disponible (normal)\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le fonctionne correctement!\")\n",
    "print(\"üöÄ Trainer optimis√© pr√™t pour l'entra√Ænement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# D√©marrage de l'entra√Ænement\n",
    "print(\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monitoring m√©moire\n",
    "memory_monitor = MemoryMonitor()\n",
    "initial_memory = memory_monitor.get_memory_info()\n",
    "\n",
    "print(f\"üíæ M√©moire initiale:\")\n",
    "print(f\"   - RAM: {initial_memory['ram_percent']:.1f}%\")\n",
    "print(f\"   - GPU: {initial_memory['gpu_percent']:.1f}%\")\n",
    "\n",
    "# Configuration d'entra√Ænement\n",
    "num_epochs = config['training']['epochs']\n",
    "save_every = config.get('save_every', 5)\n",
    "\n",
    "# Historique des m√©triques\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüìã Configuration d'entra√Ænement:\")\n",
    "print(f\"   - Epochs: {num_epochs}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Sauvegarde chaque {save_every} epochs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Boucle d'entra√Ænement principale\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\nüîÑ Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Phase d'entra√Ænement\n",
    "        if hasattr(trainer, 'train_epoch'):\n",
    "            # Utiliser le trainer avanc√©\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            train_loss = train_metrics['total_loss']\n",
    "        else:\n",
    "            # Entra√Ænement simple\n",
    "            trainer.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                trainer.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                trainer.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Phase de validation\n",
    "        print(f\"\\nüìä Validation...\")\n",
    "        trainer.model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                val_loss += loss_dict['total_loss'].item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Monitoring m√©moire\n",
    "        current_memory = memory_monitor.get_memory_info()\n",
    "        \n",
    "        # R√©sum√© de l'epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\nüìà Epoch {epoch+1} R√©sultats:\")\n",
    "        print(f\"   - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   - Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"   - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"   - M√©moire GPU: {current_memory['gpu_percent']:.1f}%\")\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            print(f\"   ‚úÖ Nouveau meilleur mod√®le sauv√©! (Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Sauvegarde p√©riodique\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = MODEL_DIR / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   üíæ Checkpoint sauv√©: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Logging wandb\n",
    "        if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'gpu_memory_percent': current_memory['gpu_percent'],\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        \n",
    "        # Nettoyage m√©moire\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Entra√Ænement interrompu par l'utilisateur\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur durant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Temps total d'entra√Ænement: {total_time/60:.1f} minutes\")\n",
    "    print(f\"üéØ Meilleure validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Fermeture wandb\n",
    "    if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "        wandb.finish()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä 6. √âvaluation et M√©triques\n",
    "\n",
    "√âvaluation compl√®te avec m√©triques de diarization standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te du mod√®le\n",
    "print(\"üìä √âVALUATION DU MOD√àLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"üìÇ Chargement du meilleur mod√®le: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"   - Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"   - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de mod√®le sauv√©, utilisation du mod√®le actuel\")\n",
    "\n",
    "# Initialiser les m√©triques\n",
    "metrics_computer = DiarizationMetrics(num_speakers=config['model']['num_speakers'])\n",
    "\n",
    "# √âvaluation sur l'ensemble de validation\n",
    "print(\"\\nüß™ √âvaluation sur l'ensemble de validation...\")\n",
    "trainer.model.eval()\n",
    "\n",
    "all_vad_preds = []\n",
    "all_vad_targets = []\n",
    "all_osd_preds = []\n",
    "all_osd_targets = []\n",
    "\n",
    "eval_loss = 0\n",
    "num_eval_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        features = batch['features'].to(trainer.device)\n",
    "        vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "        osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "            vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "        else:\n",
    "            vad_pred, osd_pred = trainer.model(features)\n",
    "        \n",
    "        # Loss\n",
    "        loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "        eval_loss += loss_dict['total_loss'].item()\n",
    "        num_eval_batches += 1\n",
    "        \n",
    "        # Collecter pour m√©triques\n",
    "        all_vad_preds.append(vad_pred.cpu())\n",
    "        all_vad_targets.append(vad_labels.cpu())\n",
    "        all_osd_preds.append(osd_pred.cpu())\n",
    "        all_osd_targets.append(osd_labels.cpu())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"   Batch {batch_idx}/{len(val_loader)} √©valu√©\")\n",
    "\n",
    "# Calculer m√©triques d√©taill√©es\n",
    "print(\"\\nüìà Calcul des m√©triques d√©taill√©es...\")\n",
    "\n",
    "vad_preds = torch.cat(all_vad_preds, dim=0)\n",
    "vad_targets = torch.cat(all_vad_targets, dim=0)\n",
    "osd_preds = torch.cat(all_osd_preds, dim=0)\n",
    "osd_targets = torch.cat(all_osd_targets, dim=0)\n",
    "\n",
    "print(f\"   - Donn√©es √©valu√©es: {vad_preds.shape[0]} √©chantillons\")\n",
    "print(f\"   - Dur√©e totale: {vad_preds.shape[0] * vad_preds.shape[1] * 0.02:.1f} secondes\")\n",
    "\n",
    "# M√©triques principales\n",
    "metrics = metrics_computer.compute_metrics(vad_preds, osd_preds, vad_targets, osd_targets)\n",
    "\n",
    "print(\"\\nüéØ R√âSULTATS D'√âVALUATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üìä Loss finale: {eval_loss / num_eval_batches:.4f}\")\n",
    "print(f\"üìä DER (Diarization Error Rate): {metrics.get('der', 0):.2f}%\")\n",
    "print(f\"üìä F1 Score global: {metrics.get('f1_score', 0):.3f}\")\n",
    "print(f\"üìä Pr√©cision frame: {metrics.get('frame_precision', 0):.3f}\")\n",
    "print(f\"üìä Rappel frame: {metrics.get('frame_recall', 0):.3f}\")\n",
    "print(f\"üìä Jaccard Index: {metrics.get('jaccard_index', 0):.3f}\")\n",
    "\n",
    "# M√©triques OSD\n",
    "if 'osd_precision' in metrics:\n",
    "    print(f\"\\nüîÄ D√©tection de Chevauchement (OSD):\")\n",
    "    print(f\"   - Pr√©cision OSD: {metrics['osd_precision']:.3f}\")\n",
    "    print(f\"   - Rappel OSD: {metrics['osd_recall']:.3f}\")\n",
    "    print(f\"   - F1 OSD: {metrics['osd_f1']:.3f}\")\n",
    "\n",
    "# M√©triques par locuteur\n",
    "print(f\"\\nüë• M√©triques par Locuteur:\")\n",
    "for spk in range(config['model']['num_speakers']):\n",
    "    if f'speaker_{spk}_f1' in metrics:\n",
    "        print(f\"   Locuteur {spk}: F1={metrics[f'speaker_{spk}_f1']:.3f}, \"\n",
    "              f\"P={metrics[f'speaker_{spk}_precision']:.3f}, \"\n",
    "              f\"R={metrics[f'speaker_{spk}_recall']:.3f}\")\n",
    "\n",
    "# Sauvegarder les m√©triques\n",
    "metrics_file = RESULTS_DIR / 'evaluation_metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    # Convertir les tenseurs en listes pour JSON\n",
    "    json_metrics = {k: (v.item() if torch.is_tensor(v) else v) for k, v in metrics.items()}\n",
    "    json_metrics['eval_loss'] = eval_loss / num_eval_batches\n",
    "    json.dump(json_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ M√©triques sauv√©es: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìà 7. Visualisations et Analyses\n",
    "\n",
    "G√©n√©ration de graphiques et visualisations des r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Visualisations des r√©sultats\n",
    "print(\"üìä G√©n√©ration des visualisations...\")\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# === 1. Courbes d'entra√Ænement ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('R√©sultats d\\'Entra√Ænement - Speaker Diarization TCN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Courbe de perte\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('√âvolution de la Perte')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les derni√®res epochs\n",
    "if len(train_losses) > 10:\n",
    "    start_idx = max(0, len(train_losses) - 20)\n",
    "    axes[0, 1].plot(range(start_idx, len(train_losses)), train_losses[start_idx:], \n",
    "                   label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(range(start_idx, len(val_losses)), val_losses[start_idx:], \n",
    "                   label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Convergence (Derni√®res Epochs)')\n",
    "else:\n",
    "    axes[0, 1].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('√âvolution de la Perte (Toutes Epochs)')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === 2. Analyse des pr√©dictions ===\n",
    "# Prendre un √©chantillon pour visualisation\n",
    "sample_idx = 0\n",
    "sample_vad_pred = vad_preds[sample_idx].numpy()  # [time, speakers]\n",
    "sample_vad_target = vad_targets[sample_idx].numpy()\n",
    "sample_osd_pred = osd_preds[sample_idx].numpy()  # [time]\n",
    "sample_osd_target = osd_targets[sample_idx].numpy()\n",
    "\n",
    "# Activit√© des locuteurs (pr√©dictions vs v√©rit√© terrain)\n",
    "time_frames = np.arange(len(sample_vad_pred)) * 0.02  # Conversion en secondes\n",
    "\n",
    "# Subplot pour VAD\n",
    "axes[1, 0].imshow(sample_vad_pred.T, aspect='auto', origin='lower', \n",
    "                 extent=[0, len(sample_vad_pred)*0.02, 0, 4], \n",
    "                 cmap='Blues', alpha=0.7)\n",
    "axes[1, 0].imshow(sample_vad_target.T, aspect='auto', origin='lower',\n",
    "                 extent=[0, len(sample_vad_target)*0.02, 0, 4],\n",
    "                 cmap='Reds', alpha=0.5)\n",
    "axes[1, 0].set_title('Activit√© VAD: Pr√©diction (Bleu) vs V√©rit√© (Rouge)')\n",
    "axes[1, 0].set_xlabel('Temps (s)')\n",
    "axes[1, 0].set_ylabel('Locuteur ID')\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "\n",
    "# Subplot pour OSD\n",
    "axes[1, 1].plot(time_frames, sample_osd_pred, label='Pr√©diction OSD', \n",
    "               color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(time_frames, sample_osd_target, label='V√©rit√© OSD', \n",
    "               color='red', linewidth=2, alpha=0.6)\n",
    "axes[1, 1].set_title('D√©tection de Chevauchement (OSD)')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_ylabel('Probabilit√© de Chevauchement')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "training_plot_path = RESULTS_DIR / 'training_results.png'\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Graphique d'entra√Ænement sauv√©: {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# === 3. Matrice de confusion pour classification ===\n",
    "if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "    print(\"\\nüìä Analyse de la classification des locuteurs...\")\n",
    "    \n",
    "    # Extraire les pr√©dictions de classification\n",
    "    all_speaker_preds = []\n",
    "    all_speaker_targets = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(trainer.device)\n",
    "            vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "            \n",
    "            try:\n",
    "                vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "                \n",
    "                # Cr√©er des labels de locuteurs √† partir des VAD labels\n",
    "                speaker_targets = torch.argmax(vad_labels.sum(dim=1), dim=1)  # Locuteur le plus actif\n",
    "                speaker_preds = torch.argmax(speaker_logits, dim=1)\n",
    "                \n",
    "                all_speaker_preds.extend(speaker_preds.cpu().numpy())\n",
    "                all_speaker_targets.extend(speaker_targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur dans un batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_speaker_preds and all_speaker_targets:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(all_speaker_targets, all_speaker_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Pred {i}' for i in range(4)],\n",
    "                   yticklabels=[f'True {i}' for i in range(4)])\n",
    "        plt.title('Matrice de Confusion - Classification des Locuteurs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Pr√©diction')\n",
    "        plt.ylabel('V√©rit√© Terrain')\n",
    "        \n",
    "        confusion_path = RESULTS_DIR / 'speaker_confusion_matrix.png'\n",
    "        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print(\"\\nüìã Rapport de Classification:\")\n",
    "        print(classification_report(all_speaker_targets, all_speaker_preds,\n",
    "                                  target_names=[f'Locuteur {i}' for i in range(4)],\n",
    "                                  digits=3))\n",
    "        \n",
    "        print(f\"‚úÖ Matrice de confusion sauv√©e: {confusion_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour la matrice de confusion\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Classificateur de locuteurs non activ√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_summary"
   },
   "outputs": [],
   "source": [
    "# === 4. R√©sum√© final et comparaisons ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSUM√â FINAL DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr√©er un r√©sum√© complet\n",
    "final_summary = {\n",
    "    'Configuration': {\n",
    "        'Architecture': f\"TCN {config['model']['hidden_channels']}\",\n",
    "        'Param√®tres': f\"{sum(p.numel() for p in trainer.model.parameters()):,}\",\n",
    "        'Batch Size': config['training']['batch_size'],\n",
    "        'Epochs': len(train_losses),\n",
    "        'Learning Rate': config['optimizer']['lr'],\n",
    "        'Device': str(trainer.device)\n",
    "    },\n",
    "    'R√©sultats Finaux': {\n",
    "        'Train Loss': f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\",\n",
    "        'Val Loss': f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\",\n",
    "        'Best Val Loss': f\"{best_val_loss:.4f}\",\n",
    "        'DER': f\"{metrics.get('der', 0):.2f}%\",\n",
    "        'F1 Score': f\"{metrics.get('f1_score', 0):.3f}\",\n",
    "        'Frame Precision': f\"{metrics.get('frame_precision', 0):.3f}\",\n",
    "        'Frame Recall': f\"{metrics.get('frame_recall', 0):.3f}\"\n",
    "    },\n",
    "    'Fichiers G√©n√©r√©s': {\n",
    "        'Meilleur Mod√®le': str(best_model_path) if best_model_path.exists() else \"Non sauv√©\",\n",
    "        'M√©triques': str(metrics_file),\n",
    "        'Graphiques': str(training_plot_path),\n",
    "        'Configuration': str(config_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage du r√©sum√©\n",
    "for section, items in final_summary.items():\n",
    "    print(f\"\\nüîπ {section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"   {key:.<25} {value}\")\n",
    "\n",
    "# Sauvegarde du r√©sum√©\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ R√©sum√© complet sauv√©: {summary_file}\")\n",
    "\n",
    "# === 5. Recommandations d'am√©lioration ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° RECOMMANDATIONS POUR AM√âLIORER LES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "current_der = metrics.get('der', 100)\n",
    "current_f1 = metrics.get('f1_score', 0)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if current_der > 25:\n",
    "    recommendations.append(\"üîß DER √©lev√©: Augmenter le nombre d'epochs ou r√©duire le learning rate\")\n",
    "if current_f1 < 0.7:\n",
    "    recommendations.append(\"üîß F1 faible: Essayer focal loss avec gamma plus √©lev√©\")\n",
    "if len(train_losses) < 20:\n",
    "    recommendations.append(\"‚è∞ Entra√Ænement court: Augmenter le nombre d'epochs\")\n",
    "if config['training']['batch_size'] < 16:\n",
    "    recommendations.append(\"üì¶ Batch size petit: Augmenter si possible pour am√©liorer la stabilit√©\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"üìä Utiliser plus de donn√©es AMI si disponibles\",\n",
    "    \"üéØ Affiner les hyperparam√®tres avec Optuna\",\n",
    "    \"üîÑ Essayer l'ensemble de mod√®les\",\n",
    "    \"üìà Impl√©menter la validation crois√©e\",\n",
    "    \"üß† Tester diff√©rentes architectures d'attention\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations[:7], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Tous les r√©sultats sont sauv√©s dans: {RESULTS_DIR}\")\n",
    "print(f\"üß† Meilleur mod√®le disponible dans: {MODEL_DIR}\")\n",
    "\n",
    "if config.get('use_wandb', False):\n",
    "    print(f\"üìä Logs d√©taill√©s disponibles sur Weights & Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Prochaines √âtapes\n",
    "\n",
    "### üìù Pour Continuer l'Am√©lioration:\n",
    "\n",
    "1. **üìä Donn√©es**: T√©l√©charger le corpus AMI complet\n",
    "2. **‚öôÔ∏è Hyperparam√®tres**: Optimiser avec Optuna\n",
    "3. **üéØ Architecture**: Tester diff√©rentes tailles de mod√®le\n",
    "4. **üìà Ensembles**: Combiner plusieurs mod√®les\n",
    "5. **üîÑ Post-traitement**: Am√©liorer la segmentation finale\n",
    "\n",
    "### üíæ Fichiers G√©n√©r√©s:\n",
    "- `models/checkpoints/best_model.pth` - Meilleur mod√®le\n",
    "- `results/evaluation_metrics.json` - M√©triques d√©taill√©es\n",
    "- `results/training_results.png` - Graphiques d'entra√Ænement\n",
    "- `results/training_summary.json` - R√©sum√© complet\n",
    "\n",
    "### üéØ Objectifs de Performance:\n",
    "- **DER < 20%** sur AMI corpus (√©tat de l'art: ~15-18%)\n",
    "- **F1 > 0.8** pour la d√©tection d'activit√©\n",
    "- **Temps r√©el** pour l'inf√©rence\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations! Vous avez entra√Æn√© avec succ√®s un syst√®me de diarization moderne avec toutes les optimisations avanc√©es!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

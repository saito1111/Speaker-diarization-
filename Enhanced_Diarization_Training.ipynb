{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/Speaker-diarization-/blob/main/Enhanced_Diarization_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üéôÔ∏è Enhanced Multi-Channel Speaker Diarization Training\n",
    "\n",
    "**Objectif :** Entra√Æner un syst√®me de diarization de locuteurs avanc√© sur le corpus AMI  \n",
    "**Architecture :** TCN multi-√©chelle avec attention, classification de locuteurs et gestion m√©moire optimis√©e\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "1. [Configuration de l'Environnement](#setup)\n",
    "2. [T√©l√©chargement du Corpus AMI](#data)\n",
    "3. [Pr√©paration des Donn√©es](#preprocessing)\n",
    "4. [Mod√®le et Configuration](#model)\n",
    "5. [Entra√Ænement](#training)\n",
    "6. [√âvaluation](#evaluation)\n",
    "7. [Sauvegarde et Visualisations](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß 1. Configuration de l'Environnement\n",
    "\n",
    "Installation de Conda et des d√©pendances n√©cessaires pour l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda"
   },
   "outputs": [],
   "source": [
    "# Installation de Conda sur Google Colab\n",
    "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "!conda --version\n",
    "\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_env"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation de l'environnement conda pour la diarization\n",
    "!conda create -n diarization python=3.9 -y\n",
    "!conda activate diarization\n",
    "\n",
    "# Installation des d√©pendances principales via conda\n",
    "!conda install -n diarization pytorch torchaudio cudatoolkit=11.8 -c pytorch -c nvidia -y\n",
    "!conda install -n diarization numpy scipy scikit-learn matplotlib seaborn pandas -y\n",
    "\n",
    "# Activation de l'environnement dans le notebook\n",
    "import os\n",
    "os.environ['CONDA_DEFAULT_ENV'] = 'diarization'\n",
    "os.environ['PATH'] = '/usr/local/envs/diarization/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances sp√©cialis√©es\n",
    "!pip install wandb optuna tqdm psutil\n",
    "!pip install librosa soundfile\n",
    "!pip install speechbrain\n",
    "\n",
    "# V√©rification des installations\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### üìÇ Clonage du R√©pertoire et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_project"
   },
   "outputs": [],
   "source": [
    "# Cloner le projet (remplacez par votre URL de repository)\n",
    "!git clone https://github.com/your-username/Speaker-diarization-.git\n",
    "%cd Speaker-diarization-\n",
    "\n",
    "# V√©rifier la structure du projet\n",
    "!ls -la\n",
    "!ls -la src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# Configuration des chemins et imports\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "# Cr√©er les dossiers n√©cessaires\n",
    "!mkdir -p data/ami_corpus/audio\n",
    "!mkdir -p data/ami_corpus/annotations\n",
    "!mkdir -p models/checkpoints\n",
    "!mkdir -p results/logs\n",
    "!mkdir -p results/figures\n",
    "\n",
    "# Variables globales\n",
    "DATA_DIR = Path('./data/ami_corpus')\n",
    "AUDIO_DIR = DATA_DIR / 'audio'\n",
    "ANNOTATION_DIR = DATA_DIR / 'annotations'\n",
    "MODEL_DIR = Path('./models/checkpoints')\n",
    "RESULTS_DIR = Path('./results')\n",
    "\n",
    "print(f\"R√©pertoires configur√©s:\")\n",
    "print(f\"- Audio: {AUDIO_DIR}\")\n",
    "print(f\"- Annotations: {ANNOTATION_DIR}\")\n",
    "print(f\"- Mod√®les: {MODEL_DIR}\")\n",
    "print(f\"- R√©sultats: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìä 2. T√©l√©chargement et Pr√©paration du Corpus AMI\n",
    "\n",
    "Le corpus AMI contient des enregistrements de r√©unions avec annotations temporelles des locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_ami"
   },
   "outputs": [],
   "source": [
    "# T√©l√©chargement du corpus AMI (version r√©duite pour Colab)\n",
    "# URL officielle: https://groups.inf.ed.ac.uk/ami/corpus/\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_with_progress(url, filename):\n",
    "    \"\"\"T√©l√©charge un fichier avec barre de progression.\"\"\"\n",
    "    def progress_hook(blocknum, blocksize, totalsize):\n",
    "        readsofar = blocknum * blocksize\n",
    "        if totalsize > 0:\n",
    "            percent = readsofar * 1e2 / totalsize\n",
    "            s = f\"\\r{percent:5.1f}% {readsofar:,} / {totalsize:,} bytes\"\n",
    "            sys.stderr.write(s)\n",
    "            if readsofar >= totalsize:\n",
    "                sys.stderr.write(\"\\n\")\n",
    "        else:\n",
    "            sys.stderr.write(f\"\\rread {readsofar:,}\")\n",
    "    \n",
    "    urllib.request.urlretrieve(url, filename, progress_hook)\n",
    "\n",
    "# URLs pour le corpus AMI (√©chantillon)\n",
    "ami_urls = {\n",
    "    'audio_sample': 'https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip',\n",
    "    # Ajoutez d'autres URLs selon vos besoins\n",
    "}\n",
    "\n",
    "print(\"üì• T√©l√©chargement des annotations AMI...\")\n",
    "annotation_file = DATA_DIR / 'ami_annotations.zip'\n",
    "try:\n",
    "    download_with_progress(ami_urls['audio_sample'], str(annotation_file))\n",
    "    print(\"‚úÖ T√©l√©chargement termin√©!\")\n",
    "    \n",
    "    # Extraction\n",
    "    with zipfile.ZipFile(annotation_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(ANNOTATION_DIR)\n",
    "    print(\"‚úÖ Extraction termin√©e!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur de t√©l√©chargement: {e}\")\n",
    "    print(\"Utilisation des donn√©es d'exemple...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_ami_data"
   },
   "source": [
    "# Pr√©paration alternative: utilisation des donn√©es existantes du projet\n",
    "# Si le t√©l√©chargement AMI √©choue, on utilise les donn√©es de d√©monstration\n",
    "\n",
    "def setup_demo_data():\n",
    "    \"\"\"Configure des donn√©es de d√©monstration pour l'entra√Ænement.\"\"\"\n",
    "    print(\"üîß Configuration des donn√©es de d√©monstration...\")\n",
    "    \n",
    "    # V√©rifier si les donn√©es AMI pr√©par√©es existent\n",
    "    if Path('./annot_prepare/results/rttm').exists():\n",
    "        print(\"‚úÖ Donn√©es AMI pr√©par√©es trouv√©es!\")\n",
    "        return './annot_prepare/results/meta', './annot_prepare/results/rttm'\n",
    "    \n",
    "    # Sinon, cr√©er des donn√©es synth√©tiques pour la d√©monstration\n",
    "    print(\"üé≠ Cr√©ation de donn√©es synth√©tiques...\")\n",
    "    \n",
    "    # Cr√©er des fichiers audio fictifs (spectrogrammes al√©atoires)\n",
    "    import torch\n",
    "    import json\n",
    "    \n",
    "    # Param√®tres audio\n",
    "    sample_rate = 16000\n",
    "    duration = 60  # 60 secondes par fichier\n",
    "    n_channels = 8  # 8 microphones\n",
    "    n_files = 20   # 20 fichiers pour la d√©mo\n",
    "    \n",
    "    audio_files = []\n",
    "    rttm_files = []\n",
    "    \n",
    "    for i in range(n_files):\n",
    "        file_id = f\"demo_meeting_{i:03d}\"\n",
    "        \n",
    "        # Cr√©er audio synth√©tique\n",
    "        audio_data = torch.randn(n_channels, sample_rate * duration)\n",
    "        audio_path = AUDIO_DIR / f\"{file_id}.pt\"\n",
    "        torch.save(audio_data, audio_path)\n",
    "        audio_files.append(str(audio_path))\n",
    "        \n",
    "        # Cr√©er annotations RTTM synth√©tiques\n",
    "        rttm_content = []\n",
    "        n_speakers = np.random.randint(2, 5)  # 2-4 locuteurs\n",
    "        \n",
    "        for spk_idx in range(n_speakers):\n",
    "            speaker_id = f\"speaker_{spk_idx}\"\n",
    "            \n",
    "            # G√©n√©rer segments al√©atoires\n",
    "            n_segments = np.random.randint(3, 8)\n",
    "            for seg_idx in range(n_segments):\n",
    "                start_time = np.random.uniform(0, duration - 10)\n",
    "                segment_duration = np.random.uniform(2, 8)\n",
    "                end_time = min(start_time + segment_duration, duration)\n",
    "                \n",
    "                # Format RTTM: SPEAKER file_id 1 start_time duration <NA> <NA> speaker_id <NA> <NA>\n",
    "                rttm_line = f\"SPEAKER {file_id} 1 {start_time:.3f} {segment_duration:.3f} <NA> <NA> {speaker_id} <NA> <NA>\"\n",
    "                rttm_content.append(rttm_line)\n",
    "        \n",
    "        # Sauvegarder RTTM\n",
    "        rttm_path = ANNOTATION_DIR / f\"{file_id}.rttm\"\n",
    "        with open(rttm_path, 'w') as f:\n",
    "            f.write('\\n'.join(rttm_content))\n",
    "        rttm_files.append(str(rttm_path))\n",
    "    \n",
    "    print(f\"‚úÖ Cr√©√© {n_files} fichiers de d√©monstration\")\n",
    "    print(f\"   - Audio: {len(audio_files)} fichiers\")\n",
    "    print(f\"   - RTTM: {len(rttm_files)} fichiers\")\n",
    "    \n",
    "    return str(AUDIO_DIR), str(ANNOTATION_DIR)\n",
    "\n",
    "# Configurer les donn√©es\n",
    "audio_dir, rttm_dir = setup_demo_data()\n",
    "\n",
    "print(f\"\\nüìÇ R√©pertoires configur√©s:\")\n",
    "print(f\"   Audio: {audio_dir}\")\n",
    "print(f\"   RTTM: {rttm_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "### üìä Division des Donn√©es (Train/Eval)\n",
    "\n",
    "Division stratifi√©e du corpus AMI selon les bonnes pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_ami_data"
   },
   "outputs": [],
   "source": [
    "def create_ami_splits(audio_dir, rttm_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divise le corpus AMI en ensembles d'entra√Ænement, validation et test.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: R√©pertoire des fichiers audio\n",
    "        rttm_dir: R√©pertoire des annotations RTTM\n",
    "        train_ratio: Proportion pour l'entra√Ænement\n",
    "        val_ratio: Proportion pour la validation\n",
    "        test_ratio: Proportion pour le test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les listes de fichiers pour chaque split\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Les ratios doivent sommer √† 1.0\"\n",
    "    \n",
    "    # Lister tous les fichiers disponibles\n",
    "    audio_path = Path(audio_dir)\n",
    "    rttm_path = Path(rttm_dir)\n",
    "    \n",
    "    # Trouver les fichiers audio\n",
    "    audio_extensions = ['.wav', '.pt', '.flac', '.mp3']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(audio_path.glob(f'*{ext}')))\n",
    "    \n",
    "    # V√©rifier la correspondance audio-RTTM\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        rttm_file = rttm_path / f\"{base_name}.rttm\"\n",
    "        \n",
    "        if rttm_file.exists():\n",
    "            valid_pairs.append({\n",
    "                'base_name': base_name,\n",
    "                'audio_file': str(audio_file),\n",
    "                'rttm_file': str(rttm_file)\n",
    "            })\n",
    "    \n",
    "    print(f\"üìä Trouv√© {len(valid_pairs)} paires audio-RTTM valides\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        raise ValueError(\"Aucune paire audio-RTTM valide trouv√©e!\")\n",
    "    \n",
    "    # M√©langer et diviser\n",
    "    import random\n",
    "    random.seed(42)  # Pour la reproductibilit√©\n",
    "    random.shuffle(valid_pairs)\n",
    "    \n",
    "    n_total = len(valid_pairs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    train_files = valid_pairs[:n_train]\n",
    "    val_files = valid_pairs[n_train:n_train + n_val]\n",
    "    test_files = valid_pairs[n_train + n_val:]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_files,\n",
    "        'validation': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\nüìà Division des donn√©es:\")\n",
    "    for split_name, files in splits.items():\n",
    "        print(f\"   {split_name:>10}: {len(files):>3} fichiers ({len(files)/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # Sauvegarder les splits\n",
    "    splits_file = DATA_DIR / 'data_splits.json'\n",
    "    with open(splits_file, 'w') as f:\n",
    "        json.dump(splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Splits sauvegard√©s dans: {splits_file}\")\n",
    "    return splits\n",
    "\n",
    "# Cr√©er les splits\n",
    "data_splits = create_ami_splits(audio_dir, rttm_dir, \n",
    "                               train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "# V√©rifier le contenu\n",
    "print(\"\\nüîç Exemple de fichiers par split:\")\n",
    "for split_name, files in data_splits.items():\n",
    "    if files:\n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        for i, file_info in enumerate(files[:3]):  # Afficher les 3 premiers\n",
    "            print(f\"  {i+1}. {file_info['base_name']}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"  ... et {len(files)-3} autres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## üõ†Ô∏è 3. Pr√©paration des Donn√©es et Extraction de Caract√©ristiques\n",
    "\n",
    "Extraction des caract√©ristiques multi-canaux: LPS, IPD, AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "# Import des modules du projet\n",
    "try:\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    from optimized_dataset import DiarizationDataset, AudioFeatureExtractor\n",
    "    from optimized_dataloader import create_optimized_dataloaders, MemoryAwareDataLoader\n",
    "    from diarization_losses import MultiTaskDiarizationLoss, create_loss_function\n",
    "    from metrics import DiarizationMetrics\n",
    "    from improved_trainer import ImprovedDiarizationTrainer, MemoryMonitor\n",
    "    \n",
    "    print(\"‚úÖ Tous les modules import√©s avec succ√®s!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"üì• T√©l√©chargement des fichiers depuis le repository...\")\n",
    "    \n",
    "    # Si les imports √©chouent, cr√©er des versions simplifi√©es pour la d√©mo\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/tcn_diarization_model.py\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/optimized_dataset.py\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/optimized_dataloader.py\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/diarization_losses.py\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/metrics.py\n",
    "    !wget -q https://raw.githubusercontent.com/your-repo/Speaker-diarization-/main/src/improved_trainer.py\n",
    "    \n",
    "    # R√©essayer l'import\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    from optimized_dataset import DiarizationDataset, AudioFeatureExtractor\n",
    "    from optimized_dataloader import create_optimized_dataloaders, MemoryAwareDataLoader\n",
    "    from diarization_losses import MultiTaskDiarizationLoss, create_loss_function\n",
    "    from metrics import DiarizationMetrics\n",
    "    from improved_trainer import ImprovedDiarizationTrainer, MemoryMonitor\n",
    "    \n",
    "    print(\"‚úÖ Modules t√©l√©charg√©s et import√©s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Test de l'extraction de caract√©ristiques\n",
    "print(\"üß™ Test de l'extraction de caract√©ristiques...\")\n",
    "\n",
    "# Cr√©er un extracteur de caract√©ristiques\n",
    "feature_extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=512,\n",
    "    hop_length=256\n",
    ")\n",
    "\n",
    "# Cr√©er des donn√©es audio fictives (8 canaux)\n",
    "n_channels = 8\n",
    "duration = 4.0  # 4 secondes\n",
    "sample_rate = 16000\n",
    "n_samples = int(duration * sample_rate)\n",
    "\n",
    "# Simuler audio multi-canal avec du bruit et des signaux\n",
    "waveforms = []\n",
    "for ch in range(n_channels):\n",
    "    # Signal de base + bruit\n",
    "    base_signal = np.sin(2 * np.pi * 440 * np.linspace(0, duration, n_samples))  # 440 Hz\n",
    "    noise = np.random.normal(0, 0.1, n_samples)\n",
    "    # Ajouter un l√©ger d√©calage temporel pour simuler la spatialisation\n",
    "    delay_samples = int(0.001 * ch * sample_rate)  # 1ms de d√©lai par canal\n",
    "    delayed_signal = np.zeros(n_samples)\n",
    "    if delay_samples < n_samples:\n",
    "        delayed_signal[delay_samples:] = base_signal[:n_samples-delay_samples]\n",
    "    \n",
    "    final_signal = delayed_signal + noise\n",
    "    waveforms.append(torch.tensor(final_signal, dtype=torch.float32))\n",
    "\n",
    "print(f\"üìä Audio g√©n√©r√©: {n_channels} canaux, {duration}s, {sample_rate} Hz\")\n",
    "\n",
    "# Test d'extraction\n",
    "features = feature_extractor.extract_features(waveforms)\n",
    "print(f\"‚úÖ Caract√©ristiques extraites: {features.shape}\")\n",
    "print(f\"   - Dimensions attendues: [771, ~{int(duration * sample_rate / 256)}]\")\n",
    "print(f\"   - Dimensions obtenues: {list(features.shape)}\")\n",
    "\n",
    "# Analyser les caract√©ristiques\n",
    "print(f\"\\nüìà Statistiques des caract√©ristiques:\")\n",
    "print(f\"   - Min: {features.min():.3f}\")\n",
    "print(f\"   - Max: {features.max():.3f}\")\n",
    "print(f\"   - Moyenne: {features.mean():.3f}\")\n",
    "print(f\"   - Std: {features.std():.3f}\")\n",
    "\n",
    "# Visualiser les caract√©ristiques\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Spectrogramme des caract√©ristiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(features.numpy()[:100, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques LPS (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(features.numpy()[257:357, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques IPD (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(features.numpy()[500:600, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques AF (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(features.numpy().mean(axis=0))\n",
    "plt.title('√ânergie moyenne par frame')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('√ânergie moyenne')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'feature_extraction_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Test d'extraction de caract√©ristiques termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## üß† 4. Configuration du Mod√®le et Entra√Ænement\n",
    "\n",
    "Configuration optimale pour le corpus AMI avec les meilleures pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis√©e pour AMI corpus\n",
    "config = {\n",
    "    # === MOD√àLE ===\n",
    "    'model': {\n",
    "        'input_dim': 771,  # LPS (257) + IPD (257*4) + AF (257*4) = 2313 ‚Üí 771 apr√®s agr√©gation\n",
    "        'hidden_channels': [256, 256, 256, 512, 512],  # Architecture TCN multi-√©chelle\n",
    "        'kernel_size': 3,\n",
    "        'num_speakers': 4,  # AMI corpus a typiquement 3-4 locuteurs\n",
    "        'dropout': 0.2,\n",
    "        'use_attention': True,  # Auto-attention pour d√©pendances long-terme\n",
    "        'use_speaker_classifier': True,  # Classification de locuteurs\n",
    "        'embedding_dim': 256\n",
    "    },\n",
    "    \n",
    "    # === FONCTION DE PERTE ===\n",
    "    'loss': {\n",
    "        'type': 'multitask',\n",
    "        'vad_weight': 1.0,\n",
    "        'osd_weight': 1.0,\n",
    "        'consistency_weight': 0.1,\n",
    "        'use_pit': True,  # Permutation Invariant Training\n",
    "        'use_focal': True,  # Focal Loss pour donn√©es d√©s√©quilibr√©es\n",
    "        'focal_gamma': 2.0,\n",
    "        'num_speakers': 4\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISEUR ===\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 1e-3,  # Learning rate initial\n",
    "        'weight_decay': 1e-4,\n",
    "        'betas': (0.9, 0.999)\n",
    "    },\n",
    "    \n",
    "    # === PLANIFICATEUR LR ===\n",
    "    'scheduler': {\n",
    "        'type': 'onecycle',  # OneCycleLR pour convergence rapide\n",
    "        'steps_per_epoch': 100,  # Sera mis √† jour automatiquement\n",
    "        'pct_start': 0.3  # 30% mont√©e, 70% descente\n",
    "    },\n",
    "    \n",
    "    # === ENTRA√éNEMENT ===\n",
    "    'training': {\n",
    "        'epochs': 50,  # R√©duit pour Colab\n",
    "        'batch_size': 8,  # Adapt√© √† la m√©moire Colab\n",
    "        'num_workers': 2  # Moins de workers pour √©viter les probl√®mes m√©moire\n",
    "    },\n",
    "    \n",
    "    # === DONN√âES ===\n",
    "    'data': {\n",
    "        'segment_duration': 4.0,  # Segments de 4 secondes\n",
    "        'sample_rate': 16000,\n",
    "        'train_split': 0.7,\n",
    "        'max_segments': 1000  # Limite pour Colab\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISATIONS AVANC√âES ===\n",
    "    'accumulation_steps': 4,  # Batch effectif = 8*4 = 32\n",
    "    'use_amp': True,  # Pr√©cision mixte\n",
    "    'grad_clip_norm': 1.0,\n",
    "    'patience': 10,\n",
    "    'save_every': 5,\n",
    "    \n",
    "    # === MONITORING ===\n",
    "    'use_wandb': True,  # Weights & Biases (optionnel)\n",
    "    'project_name': 'ami-speaker-diarization',\n",
    "    'memory_threshold': 0.85,  # Gestion m√©moire Colab\n",
    "    'adaptive_batch': True,\n",
    "    'speaker_loss_weight': 0.5,\n",
    "    \n",
    "    # === CHEMINS ===\n",
    "    'save_dir': str(MODEL_DIR),\n",
    "    'results_dir': str(RESULTS_DIR)\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration cr√©√©e avec les param√®tres suivants:\")\n",
    "print(f\"   - Architecture: TCN {config['model']['hidden_channels']}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']} (effectif: {config['training']['batch_size'] * config['accumulation_steps']})\")\n",
    "print(f\"   - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Pr√©cision mixte: {config['use_amp']}\")\n",
    "print(f\"   - Classification locuteurs: {config['model']['use_speaker_classifier']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "config_file = MODEL_DIR / 'training_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration sauvegard√©e: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration de Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "use_wandb = config.get('use_wandb', False)\n",
    "\n",
    "if use_wandb:\n",
    "    try:\n",
    "        # Connexion √† wandb (n√©cessite un compte gratuit)\n",
    "        wandb.login()\n",
    "        \n",
    "        # Initialisation du projet\n",
    "        wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"ami-tcn-{torch.cuda.get_device_name(0).replace(' ', '-') if torch.cuda.is_available() else 'cpu'}\",\n",
    "            tags=['ami-corpus', 'tcn', 'multi-channel', 'colab'],\n",
    "            notes=\"Entra√Ænement sur corpus AMI avec architecture TCN am√©lior√©e\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Weights & Biases configur√©!\")\n",
    "        print(f\"üìä Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur wandb: {e}\")\n",
    "        print(\"üìà Entra√Ænement sans monitoring wandb...\")\n",
    "        config['use_wandb'] = False\nelse:\n",
    "    print(\"üìà Entra√Ænement sans monitoring wandb (d√©sactiv√© dans config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ 5. Entra√Ænement du Mod√®le\n",
    "\n",
    "Entra√Ænement avec toutes les optimisations: gestion m√©moire, precision mixte, accumulation de gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation des DataLoaders optimis√©s\n",
    "print(\"üîÑ Cr√©ation des DataLoaders...\")\n",
    "\n",
    "try:\n",
    "    # Cr√©er les DataLoaders avec gestion m√©moire\n",
    "    train_loader, val_loader = create_optimized_dataloaders(\n",
    "        audio_dir=audio_dir,\n",
    "        rttm_dir=rttm_dir,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        train_split=config['data']['train_split'],\n",
    "        num_workers=config['training']['num_workers'],\n",
    "        segment_duration=config['data']['segment_duration'],\n",
    "        sample_rate=config['data']['sample_rate'],\n",
    "        max_segments=config['data']['max_segments'],\n",
    "        memory_threshold=config['memory_threshold'],\n",
    "        adaptive_batch=config['adaptive_batch'],\n",
    "        accumulation_steps=config['accumulation_steps']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ DataLoaders cr√©√©s avec succ√®s!\")\n",
    "    print(f\"   - Train batches: {len(train_loader)}\")\n",
    "    print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Mettre √† jour la configuration avec le nombre r√©el de steps\n",
    "    config['scheduler']['steps_per_epoch'] = len(train_loader)\n",
    "    \n",
    "    # Test d'un batch\n",
    "    print(\"\\nüß™ Test d'un batch d'entra√Ænement...\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        print(f\"   Batch {batch_idx}:\")\n",
    "        print(f\"     - Features: {batch['features'].shape}\")\n",
    "        print(f\"     - VAD labels: {batch['vad_labels'].shape}\")\n",
    "        print(f\"     - OSD labels: {batch['osd_labels'].shape}\")\n",
    "        \n",
    "        # V√©rifier les dimensions\n",
    "        assert batch['features'].shape[1] == 771, f\"Dimension features incorrecte: {batch['features'].shape[1]} != 771\"\n",
    "        assert batch['vad_labels'].shape[-1] == 4, f\"Nombre de locuteurs incorrect: {batch['vad_labels'].shape[-1]} != 4\"\n",
    "        \n",
    "        print(f\"     ‚úÖ Dimensions correctes!\")\n",
    "        break\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur cr√©ation DataLoaders: {e}\")\n",
    "    \n",
    "    # Fallback: cr√©er un dataset simple pour la d√©monstration\n",
    "    print(\"üîÑ Cr√©ation d'un dataset de d√©monstration...\")\n",
    "    \n",
    "    class DemoDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, size=100, input_dim=771, seq_len=250, num_speakers=4):\n",
    "            self.size = size\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_len = seq_len\n",
    "            self.num_speakers = num_speakers\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Cr√©er des donn√©es synth√©tiques r√©alistes\n",
    "            features = torch.randn(self.input_dim, self.seq_len) * 0.5\n",
    "            \n",
    "            # VAD labels avec activit√© r√©aliste\n",
    "            vad_labels = torch.zeros(self.seq_len, self.num_speakers)\n",
    "            for spk in range(self.num_speakers):\n",
    "                if np.random.random() > 0.3:  # 70% chance d'activit√©\n",
    "                    start = np.random.randint(0, self.seq_len // 2)\n",
    "                    end = np.random.randint(start + 10, self.seq_len)\n",
    "                    vad_labels[start:end, spk] = 1.0\n",
    "            \n",
    "            # OSD labels bas√©s sur chevauchement\n",
    "            osd_labels = (vad_labels.sum(dim=1) > 1).float()\n",
    "            \n",
    "            return {\n",
    "                'features': features,\n",
    "                'vad_labels': vad_labels,\n",
    "                'osd_labels': osd_labels,\n",
    "                'segment_id': idx\n",
    "            }\n",
    "    \n",
    "    # Cr√©er datasets de d√©mo\n",
    "    train_dataset = DemoDataset(size=200)\n",
    "    val_dataset = DemoDataset(size=50)\n",
    "    \n",
    "    # DataLoaders de d√©mo\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0  # Pas de multiprocessing pour √©viter les erreurs\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset de d√©mo cr√©√©:\")\n",
    "    print(f\"   - Train: {len(train_loader)} batches\")\n",
    "    print(f\"   - Val: {len(val_loader)} batches\")\n",
    "    \n",
    "    config['scheduler']['steps_per_epoch'] = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialisation du trainer avanc√©\n",
    "print(\"üß† Initialisation du trainer...\")\n",
    "\n",
    "try:\n",
    "    # Cr√©er le trainer avec toutes les am√©liorations\n",
    "    trainer = ImprovedDiarizationTrainer(config)\n",
    "    \n",
    "    print(f\"‚úÖ Trainer initialis√© avec succ√®s!\")\n",
    "    print(f\"   - Mod√®le: {trainer.model.get_num_params():,} param√®tres\")\n",
    "    print(f\"   - Device: {trainer.device}\")\n",
    "    print(f\"   - Pr√©cision mixte: {trainer.use_amp}\")\n",
    "    print(f\"   - Accumulation gradients: {trainer.accumulation_steps}\")\n",
    "    \n",
    "    # Test du forward pass\n",
    "    print(\"\\nüß™ Test du mod√®le...\")\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test avec un batch de donn√©es\n",
    "        test_input = torch.randn(2, 771, 250).to(trainer.device)\n",
    "        \n",
    "        # Forward pass simple\n",
    "        vad_out, osd_out = model(test_input)\n",
    "        print(f\"   Forward simple: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        \n",
    "        # Forward avec embeddings\n",
    "        vad_out, osd_out, embeddings, speaker_logits = model(test_input, return_embeddings=True)\n",
    "        print(f\"   Forward complet: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        print(f\"                   Embeddings {embeddings.shape}, Speaker {speaker_logits.shape}\")\n",
    "    \n",
    "    print(\"‚úÖ Mod√®le fonctionne correctement!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur initialisation trainer: {e}\")\n",
    "    \n",
    "    # Fallback: cr√©er un trainer simple\n",
    "    print(\"üîÑ Cr√©ation d'un trainer simple...\")\n",
    "    \n",
    "    # Cr√©er juste le mod√®le\n",
    "    model = DiarizationTCN(\n",
    "        input_dim=config['model']['input_dim'],\n",
    "        hidden_channels=config['model']['hidden_channels'],\n",
    "        num_speakers=config['model']['num_speakers'],\n",
    "        use_speaker_classifier=config['model']['use_speaker_classifier']\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimiseur simple\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['optimizer']['lr'],\n",
    "        weight_decay=config['optimizer']['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Crit√®re de perte\n",
    "    criterion = MultiTaskDiarizationLoss(\n",
    "        use_pit=config['loss']['use_pit'],\n",
    "        use_focal=config['loss']['use_focal'],\n",
    "        num_speakers=config['model']['num_speakers']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Trainer simple cr√©√©:\")\n",
    "    print(f\"   - Param√®tres: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   - Device: {device}\")\n",
    "    \n",
    "    # Pour compatibilit√© avec le code d'entra√Ænement\n",
    "    class SimpleTrainer:\n",
    "        def __init__(self, model, optimizer, criterion, device):\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.criterion = criterion\n",
    "            self.device = device\n",
    "            self.use_amp = config.get('use_amp', False)\n",
    "            \n",
    "        def train_epoch(self, train_loader):\n",
    "            # Impl√©mentation simple d'entra√Ænement\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(self.device)\n",
    "                vad_labels = batch['vad_labels'].to(self.device)\n",
    "                osd_labels = batch['osd_labels'].to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = self.model(features)\n",
    "                loss_dict = self.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            return {'loss': total_loss / len(train_loader)}\n",
    "    \n",
    "    trainer = SimpleTrainer(model, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# D√©marrage de l'entra√Ænement\n",
    "print(\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monitoring m√©moire\n",
    "memory_monitor = MemoryMonitor()\n",
    "initial_memory = memory_monitor.get_memory_info()\n",
    "\n",
    "print(f\"üíæ M√©moire initiale:\")\n",
    "print(f\"   - RAM: {initial_memory['ram_percent']:.1f}%\")\n",
    "print(f\"   - GPU: {initial_memory['gpu_percent']:.1f}%\")\n",
    "\n",
    "# Configuration d'entra√Ænement\n",
    "num_epochs = config['training']['epochs']\n",
    "save_every = config.get('save_every', 5)\n",
    "\n",
    "# Historique des m√©triques\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüìã Configuration d'entra√Ænement:\")\n",
    "print(f\"   - Epochs: {num_epochs}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Sauvegarde chaque {save_every} epochs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Boucle d'entra√Ænement principale\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\nüîÑ Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Phase d'entra√Ænement\n",
    "        if hasattr(trainer, 'train_epoch'):\n",
    "            # Utiliser le trainer avanc√©\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            train_loss = train_metrics['total_loss']\n",
    "        else:\n",
    "            # Entra√Ænement simple\n",
    "            trainer.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                trainer.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                trainer.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Phase de validation\n",
    "        print(f\"\\nüìä Validation...\")\n",
    "        trainer.model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                val_loss += loss_dict['total_loss'].item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Monitoring m√©moire\n",
    "        current_memory = memory_monitor.get_memory_info()\n",
    "        \n",
    "        # R√©sum√© de l'epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\nüìà Epoch {epoch+1} R√©sultats:\")\n",
    "        print(f\"   - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   - Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"   - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"   - M√©moire GPU: {current_memory['gpu_percent']:.1f}%\")\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            print(f\"   ‚úÖ Nouveau meilleur mod√®le sauv√©! (Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Sauvegarde p√©riodique\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = MODEL_DIR / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   üíæ Checkpoint sauv√©: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Logging wandb\n",
    "        if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'gpu_memory_percent': current_memory['gpu_percent'],\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        \n",
    "        # Nettoyage m√©moire\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\nexcept KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Entra√Ænement interrompu par l'utilisateur\")\nexcept Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur durant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Temps total d'entra√Ænement: {total_time/60:.1f} minutes\")\n",
    "    print(f\"üéØ Meilleure validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Fermeture wandb\n",
    "    if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "        wandb.finish()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä 6. √âvaluation et M√©triques\n",
    "\n",
    "√âvaluation compl√®te avec m√©triques de diarization standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te du mod√®le\n",
    "print(\"üìä √âVALUATION DU MOD√àLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"üìÇ Chargement du meilleur mod√®le: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"   - Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"   - Val Loss: {checkpoint['val_loss']:.4f}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Pas de mod√®le sauv√©, utilisation du mod√®le actuel\")\n\n# Initialiser les m√©triques\nmetrics_computer = DiarizationMetrics(num_speakers=config['model']['num_speakers'])\n\n# √âvaluation sur l'ensemble de validation\nprint(\"\\nüß™ √âvaluation sur l'ensemble de validation...\")\ntrainer.model.eval()\n\nall_vad_preds = []\nall_vad_targets = []\nall_osd_preds = []\nall_osd_targets = []\n\neval_loss = 0\nnum_eval_batches = 0\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(val_loader):\n        features = batch['features'].to(trainer.device)\n        vad_labels = batch['vad_labels'].to(trainer.device)\n        osd_labels = batch['osd_labels'].to(trainer.device)\n        \n        # Pr√©dictions\n        if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n            vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n        else:\n            vad_pred, osd_pred = trainer.model(features)\n        \n        # Loss\n        loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n        eval_loss += loss_dict['total_loss'].item()\n        num_eval_batches += 1\n        \n        # Collecter pour m√©triques\n        all_vad_preds.append(vad_pred.cpu())\n        all_vad_targets.append(vad_labels.cpu())\n        all_osd_preds.append(osd_pred.cpu())\n        all_osd_targets.append(osd_labels.cpu())\n        \n        if batch_idx % 10 == 0:\n            print(f\"   Batch {batch_idx}/{len(val_loader)} √©valu√©\")\n\n# Calculer m√©triques d√©taill√©es\nprint(\"\\nüìà Calcul des m√©triques d√©taill√©es...\")\n\nvad_preds = torch.cat(all_vad_preds, dim=0)\nvad_targets = torch.cat(all_vad_targets, dim=0)\nosd_preds = torch.cat(all_osd_preds, dim=0)\nosd_targets = torch.cat(all_osd_targets, dim=0)\n\nprint(f\"   - Donn√©es √©valu√©es: {vad_preds.shape[0]} √©chantillons\")\nprint(f\"   - Dur√©e totale: {vad_preds.shape[0] * vad_preds.shape[1] * 0.02:.1f} secondes\")\n\n# M√©triques principales\nmetrics = metrics_computer.compute_metrics(vad_preds, osd_preds, vad_targets, osd_targets)\n\nprint(\"\\nüéØ R√âSULTATS D'√âVALUATION\")\nprint(\"=\" * 30)\nprint(f\"üìä Loss finale: {eval_loss / num_eval_batches:.4f}\")\nprint(f\"üìä DER (Diarization Error Rate): {metrics.get('der', 0):.2f}%\")\nprint(f\"üìä F1 Score global: {metrics.get('f1_score', 0):.3f}\")\nprint(f\"üìä Pr√©cision frame: {metrics.get('frame_precision', 0):.3f}\")\nprint(f\"üìä Rappel frame: {metrics.get('frame_recall', 0):.3f}\")\nprint(f\"üìä Jaccard Index: {metrics.get('jaccard_index', 0):.3f}\")\n\n# M√©triques OSD\nif 'osd_precision' in metrics:\n    print(f\"\\nüîÄ D√©tection de Chevauchement (OSD):\")\n    print(f\"   - Pr√©cision OSD: {metrics['osd_precision']:.3f}\")\n    print(f\"   - Rappel OSD: {metrics['osd_recall']:.3f}\")\n    print(f\"   - F1 OSD: {metrics['osd_f1']:.3f}\")\n\n# M√©triques par locuteur\nprint(f\"\\nüë• M√©triques par Locuteur:\")\nfor spk in range(config['model']['num_speakers']):\n    if f'speaker_{spk}_f1' in metrics:\n        print(f\"   Locuteur {spk}: F1={metrics[f'speaker_{spk}_f1']:.3f}, \"\n              f\"P={metrics[f'speaker_{spk}_precision']:.3f}, \"\n              f\"R={metrics[f'speaker_{spk}_recall']:.3f}\")\n\n# Sauvegarder les m√©triques\nmetrics_file = RESULTS_DIR / 'evaluation_metrics.json'\nwith open(metrics_file, 'w') as f:\n    # Convertir les tenseurs en listes pour JSON\n    json_metrics = {k: (v.item() if torch.is_tensor(v) else v) for k, v in metrics.items()}\n    json_metrics['eval_loss'] = eval_loss / num_eval_batches\n    json.dump(json_metrics, f, indent=2)\n\nprint(f\"\\nüíæ M√©triques sauv√©es: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìà 7. Visualisations et Analyses\n",
    "\n",
    "G√©n√©ration de graphiques et visualisations des r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Visualisations des r√©sultats\n",
    "print(\"üìä G√©n√©ration des visualisations...\")\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# === 1. Courbes d'entra√Ænement ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('R√©sultats d\\'Entra√Ænement - Speaker Diarization TCN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Courbe de perte\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('√âvolution de la Perte')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les derni√®res epochs\n",
    "if len(train_losses) > 10:\n",
    "    start_idx = max(0, len(train_losses) - 20)\n",
    "    axes[0, 1].plot(range(start_idx, len(train_losses)), train_losses[start_idx:], \n",
    "                   label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(range(start_idx, len(val_losses)), val_losses[start_idx:], \n",
    "                   label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Convergence (Derni√®res Epochs)')\n",
    "else:\n",
    "    axes[0, 1].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('√âvolution de la Perte (Toutes Epochs)')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === 2. Analyse des pr√©dictions ===\n",
    "# Prendre un √©chantillon pour visualisation\n",
    "sample_idx = 0\n",
    "sample_vad_pred = vad_preds[sample_idx].numpy()  # [time, speakers]\n",
    "sample_vad_target = vad_targets[sample_idx].numpy()\n",
    "sample_osd_pred = osd_preds[sample_idx].numpy()  # [time]\n",
    "sample_osd_target = osd_targets[sample_idx].numpy()\n",
    "\n",
    "# Activit√© des locuteurs (pr√©dictions vs v√©rit√© terrain)\n",
    "time_frames = np.arange(len(sample_vad_pred)) * 0.02  # Conversion en secondes\n",
    "\n",
    "# Subplot pour VAD\n",
    "axes[1, 0].imshow(sample_vad_pred.T, aspect='auto', origin='lower', \n",
    "                 extent=[0, len(sample_vad_pred)*0.02, 0, 4], \n",
    "                 cmap='Blues', alpha=0.7)\n",
    "axes[1, 0].imshow(sample_vad_target.T, aspect='auto', origin='lower',\n",
    "                 extent=[0, len(sample_vad_target)*0.02, 0, 4],\n",
    "                 cmap='Reds', alpha=0.5)\n",
    "axes[1, 0].set_title('Activit√© VAD: Pr√©diction (Bleu) vs V√©rit√© (Rouge)')\n",
    "axes[1, 0].set_xlabel('Temps (s)')\n",
    "axes[1, 0].set_ylabel('Locuteur ID')\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "\n",
    "# Subplot pour OSD\n",
    "axes[1, 1].plot(time_frames, sample_osd_pred, label='Pr√©diction OSD', \n",
    "               color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(time_frames, sample_osd_target, label='V√©rit√© OSD', \n",
    "               color='red', linewidth=2, alpha=0.6)\n",
    "axes[1, 1].set_title('D√©tection de Chevauchement (OSD)')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_ylabel('Probabilit√© de Chevauchement')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "training_plot_path = RESULTS_DIR / 'training_results.png'\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Graphique d'entra√Ænement sauv√©: {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# === 3. Matrice de confusion pour classification ===\n",
    "if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "    print(\"\\nüìä Analyse de la classification des locuteurs...\")\n",
    "    \n",
    "    # Extraire les pr√©dictions de classification\n",
    "    all_speaker_preds = []\n",
    "    all_speaker_targets = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(trainer.device)\n",
    "            vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "            \n",
    "            try:\n",
    "                vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "                \n",
    "                # Cr√©er des labels de locuteurs √† partir des VAD labels\n",
    "                speaker_targets = torch.argmax(vad_labels.sum(dim=1), dim=1)  # Locuteur le plus actif\n",
    "                speaker_preds = torch.argmax(speaker_logits, dim=1)\n",
    "                \n",
    "                all_speaker_preds.extend(speaker_preds.cpu().numpy())\n",
    "                all_speaker_targets.extend(speaker_targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur dans un batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_speaker_preds and all_speaker_targets:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(all_speaker_targets, all_speaker_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Pred {i}' for i in range(4)],\n",
    "                   yticklabels=[f'True {i}' for i in range(4)])\n",
    "        plt.title('Matrice de Confusion - Classification des Locuteurs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Pr√©diction')\n",
    "        plt.ylabel('V√©rit√© Terrain')\n",
    "        \n",
    "        confusion_path = RESULTS_DIR / 'speaker_confusion_matrix.png'\n",
    "        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print(\"\\nüìã Rapport de Classification:\")\n",
    "        print(classification_report(all_speaker_targets, all_speaker_preds,\n",
    "                                  target_names=[f'Locuteur {i}' for i in range(4)],\n",
    "                                  digits=3))\n",
    "        \n",
    "        print(f\"‚úÖ Matrice de confusion sauv√©e: {confusion_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour la matrice de confusion\")\nelse:\n    print(\"‚ö†Ô∏è Classificateur de locuteurs non activ√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_summary"
   },
   "outputs": [],
   "source": [
    "# === 4. R√©sum√© final et comparaisons ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSUM√â FINAL DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr√©er un r√©sum√© complet\n",
    "final_summary = {\n",
    "    'Configuration': {\n",
    "        'Architecture': f\"TCN {config['model']['hidden_channels']}\",\n",
    "        'Param√®tres': f\"{sum(p.numel() for p in trainer.model.parameters()):,}\",\n",
    "        'Batch Size': config['training']['batch_size'],\n",
    "        'Epochs': len(train_losses),\n",
    "        'Learning Rate': config['optimizer']['lr'],\n",
    "        'Device': str(trainer.device)\n",
    "    },\n",
    "    'R√©sultats Finaux': {\n",
    "        'Train Loss': f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\",\n",
    "        'Val Loss': f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\",\n",
    "        'Best Val Loss': f\"{best_val_loss:.4f}\",\n",
    "        'DER': f\"{metrics.get('der', 0):.2f}%\",\n",
    "        'F1 Score': f\"{metrics.get('f1_score', 0):.3f}\",\n",
    "        'Frame Precision': f\"{metrics.get('frame_precision', 0):.3f}\",\n",
    "        'Frame Recall': f\"{metrics.get('frame_recall', 0):.3f}\"\n",
    "    },\n",
    "    'Fichiers G√©n√©r√©s': {\n",
    "        'Meilleur Mod√®le': str(best_model_path) if best_model_path.exists() else \"Non sauv√©\",\n",
    "        'M√©triques': str(metrics_file),\n",
    "        'Graphiques': str(training_plot_path),\n",
    "        'Configuration': str(config_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage du r√©sum√©\n",
    "for section, items in final_summary.items():\n",
    "    print(f\"\\nüîπ {section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"   {key:.<25} {value}\")\n",
    "\n",
    "# Sauvegarde du r√©sum√©\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ R√©sum√© complet sauv√©: {summary_file}\")\n",
    "\n",
    "# === 5. Recommandations d'am√©lioration ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° RECOMMANDATIONS POUR AM√âLIORER LES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "current_der = metrics.get('der', 100)\n",
    "current_f1 = metrics.get('f1_score', 0)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if current_der > 25:\n",
    "    recommendations.append(\"üîß DER √©lev√©: Augmenter le nombre d'epochs ou r√©duire le learning rate\")\n",
    "if current_f1 < 0.7:\n",
    "    recommendations.append(\"üîß F1 faible: Essayer focal loss avec gamma plus √©lev√©\")\n",
    "if len(train_losses) < 20:\n",
    "    recommendations.append(\"‚è∞ Entra√Ænement court: Augmenter le nombre d'epochs\")\nif config['training']['batch_size'] < 16:\n    recommendations.append(\"üì¶ Batch size petit: Augmenter si possible pour am√©liorer la stabilit√©\")\n\nrecommendations.extend([\n    \"üìä Utiliser plus de donn√©es AMI si disponibles\",\n    \"üéØ Affiner les hyperparam√®tres avec Optuna\",\n    \"üîÑ Essayer l'ensemble de mod√®les\",\n    \"üìà Impl√©menter la validation crois√©e\",\n    \"üß† Tester diff√©rentes architectures d'attention\"\n])\n\nfor i, rec in enumerate(recommendations[:7], 1):\n    print(f\"{i}. {rec}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!\")\nprint(\"=\"*60)\nprint(f\"üìÅ Tous les r√©sultats sont sauv√©s dans: {RESULTS_DIR}\")\nprint(f\"üß† Meilleur mod√®le disponible dans: {MODEL_DIR}\")\n\nif config.get('use_wandb', False):\n    print(f\"üìä Logs d√©taill√©s disponibles sur Weights & Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Prochaines √âtapes\n",
    "\n",
    "### üìù Pour Continuer l'Am√©lioration:\n",
    "\n",
    "1. **üìä Donn√©es**: T√©l√©charger le corpus AMI complet\n",
    "2. **‚öôÔ∏è Hyperparam√®tres**: Optimiser avec Optuna\n",
    "3. **üéØ Architecture**: Tester diff√©rentes tailles de mod√®le\n",
    "4. **üìà Ensembles**: Combiner plusieurs mod√®les\n",
    "5. **üîÑ Post-traitement**: Am√©liorer la segmentation finale\n",
    "\n",
    "### üíæ Fichiers G√©n√©r√©s:\n",
    "- `models/checkpoints/best_model.pth` - Meilleur mod√®le\n",
    "- `results/evaluation_metrics.json` - M√©triques d√©taill√©es\n",
    "- `results/training_results.png` - Graphiques d'entra√Ænement\n",
    "- `results/training_summary.json` - R√©sum√© complet\n",
    "\n",
    "### üéØ Objectifs de Performance:\n",
    "- **DER < 20%** sur AMI corpus (√©tat de l'art: ~15-18%)\n",
    "- **F1 > 0.8** pour la d√©tection d'activit√©\n",
    "- **Temps r√©el** pour l'inf√©rence\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations! Vous avez entra√Æn√© avec succ√®s un syst√®me de diarization moderne avec toutes les optimisations avanc√©es!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Mount Google Drive for data access\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    print(\"\u2705 Google Drive mounted successfully\")\n    print(\"\ud83d\udcc1 Access your data at: /content/drive/MyDrive/\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f Not running on Google Colab - Drive mount not needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/saito1111/Speaker-diarization-/blob/main/Enhanced_Diarization_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# \ud83c\udf99\ufe0f Enhanced Multi-Channel Speaker Diarization Training",
    "",
    "**Objectif :** Entra\u00eener un syst\u00e8me de diarization de locuteurs avanc\u00e9 sur le corpus AMI  ",
    "**Architecture :** TCN multi-\u00e9chelle avec attention, classification de locuteurs et gestion m\u00e9moire optimis\u00e9e  ",
    "**Plateformes :** Optimis\u00e9 pour Google Colab et Kaggle Notebooks",
    "",
    "---",
    "",
    "## \u2728 Nouvelles Fonctionnalit\u00e9s",
    "",
    "### \ud83d\ude80 Compatibilit\u00e9 Multi-Plateformes",
    "- \u2705 **Google Colab** : Installation automatique des d\u00e9pendances",
    "- \u2705 **Kaggle Notebooks** : Configuration optimis\u00e9e pour l'environnement Kaggle  ",
    "- \u2705 **Environnement Local** : Support complet pour le d\u00e9veloppement local",
    "",
    "### \ud83d\udd27 Am\u00e9liorations Techniques",
    "- \ud83d\udeab **Sans Conda** : Installation directe avec pip pour une meilleure compatibilit\u00e9",
    "- \ud83d\udce6 **Gestion des D\u00e9pendances** : Installation automatique et v\u00e9rification des packages",
    "- \ud83c\udfb5 **Donn\u00e9es Adaptatives** : Configuration des donn\u00e9es depuis Google Drive",
    "- \ud83e\uddea **Tests d'Environnement** : V\u00e9rification automatique de la configuration",
    "",
    "---",
    "",
    "## \ud83d\udccb Table des Mati\u00e8res",
    "1. [Configuration de l'Environnement](#setup) - Installation automatique des d\u00e9pendances",
    "2. [Configuration des Donn\u00e9es](#data) - Donn\u00e9es depuis Google Drive",
    "3. [Pr\u00e9paration des Donn\u00e9es](#preprocessing) - Division et pr\u00e9paration des donn\u00e9es",
    "4. [Mod\u00e8le et Configuration](#model) - Architecture TCN avanc\u00e9e",
    "5. [Entra\u00eenement](#training) - Entra\u00eenement avec monitoring avanc\u00e9",
    "6. [\u00c9valuation](#evaluation) - M\u00e9triques de performance compl\u00e8tes",
    "7. [Sauvegarde et Visualisations](#results) - R\u00e9sultats et analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## \ud83d\udd27 1. Configuration de l'Environnement\n",
    "\n",
    "Configuration optimis\u00e9e pour **Google Colab** et **Kaggle Notebooks**.  \n",
    "Installation directe des d\u00e9pendances avec pip, d\u00e9tection automatique de la plateforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda"
   },
   "outputs": [],
   "source": [
    "# Installation des d\u00e9pendances n\u00e9cessaires pour Google Colab et Kaggle",
    "import sys",
    "import torch",
    "",
    "# V\u00e9rifier si on est sur Colab ou Kaggle",
    "try:",
    "    import google.colab",
    "    PLATFORM = \"COLAB\"",
    "    print(\"\ud83d\ude80 Ex\u00e9cution sur Google Colab\")",
    "except ImportError:",
    "    try:",
    "        import kaggle",
    "        PLATFORM = \"KAGGLE\"",
    "        print(\"\ud83d\ude80 Ex\u00e9cution sur Kaggle\")",
    "    except ImportError:",
    "        PLATFORM = \"LOCAL\"",
    "        print(\"\ud83d\ude80 Ex\u00e9cution en local\")",
    "",
    "print(f\"Python version: {sys.version}\")",
    "print(f\"PyTorch version: {torch.__version__}\")",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")",
    "",
    "if torch.cuda.is_available():",
    "    print(f\"CUDA version: {torch.version.cuda}\")",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
    "",
    "# Google Colab specific optimizations",
    "if PLATFORM == \"COLAB\":",
    "    print(\"\ud83d\udd27 Applying Google Colab optimizations...\")",
    "    ",
    "    # Set memory growth for GPU",
    "    if torch.cuda.is_available():",
    "        torch.backends.cudnn.benchmark = True",
    "        torch.backends.cudnn.deterministic = False",
    "        print(\"\u2705 GPU optimizations enabled\")",
    "    ",
    "    # Set up for large file handling",
    "    import os",
    "    os.environ['PYTHONHASHSEED'] = str(42)",
    "    print(\"\u2705 Environment optimizations applied\")",
    "    ",
    "    # Memory management",
    "    import gc",
    "    gc.collect()",
    "    if torch.cuda.is_available():",
    "        torch.cuda.empty_cache()",
    "    print(\"\u2705 Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_env"
   },
   "outputs": [],
   "source": [
    "# Installation des d\u00e9pendances principales avec pip\n",
    "print(\"\ud83d\udce6 Installation des d\u00e9pendances...\")\n",
    "\n",
    "# Mettre \u00e0 jour pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Installation des d\u00e9pendances PyTorch\n",
    "print(\"\ud83d\udd25 Installation de PyTorch et Torchaudio...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Installation des d\u00e9pendances scientifiques\n",
    "print(\"\ud83d\udcca Installation des packages scientifiques...\")\n",
    "!pip install numpy scipy scikit-learn matplotlib seaborn pandas\n",
    "\n",
    "# Installation des d\u00e9pendances audio et ML\n",
    "print(\"\ud83c\udfb5 Installation des packages audio et ML...\")\n",
    "!pip install librosa soundfile speechbrain\n",
    "\n",
    "# Installation des outils d'optimisation et monitoring\n",
    "print(\"\u2699\ufe0f Installation des outils d'optimisation...\")\n",
    "!pip install optuna tqdm psutil wandb\n",
    "\n",
    "print(\"\u2705 Toutes les d\u00e9pendances install\u00e9es avec succ\u00e8s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# V\u00e9rification des installations et imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import optuna\n",
    "import wandb\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\u2705 V\u00e9rification des installations:\")\n",
    "print(f\"   \ud83d\udce6 PyTorch: {torch.__version__}\")\n",
    "print(f\"   \ud83d\udce6 Torchaudio: {torchaudio.__version__}\")\n",
    "print(f\"   \ud83d\udce6 NumPy: {np.__version__}\")\n",
    "print(f\"   \ud83d\udce6 Pandas: {pd.__version__}\")\n",
    "print(f\"   \ud83d\udce6 Librosa: {librosa.__version__}\")\n",
    "print(f\"   \ud83d\udce6 SoundFile: {sf.__version__}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd25 Configuration CUDA:\")\n",
    "print(f\"   \u26a1 CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   \ud83d\udcf1 Version CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   \ud83c\udfae GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   \ud83d\udcbe M\u00e9moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   \ud83d\udd27 Nombre de GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f CUDA non disponible - utilisation du CPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbb Ressources syst\u00e8me:\")\n",
    "print(f\"   \ud83e\udde0 CPU: {psutil.cpu_count()} c\u0153urs\")\n",
    "print(f\"   \ud83d\udcbe RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Environnement pr\u00eat pour l'entra\u00eenement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final de compatibilit\u00e9 et v\u00e9rification de l'environnement\n",
    "print(\"\ud83e\uddea TEST FINAL DE COMPATIBILIT\u00c9 COLAB/KAGGLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: V\u00e9rification de la plateforme\n",
    "print(f\"\ud83d\udda5\ufe0f Plateforme d\u00e9tect\u00e9e: {PLATFORM}\")\n",
    "\n",
    "# Test 2: V\u00e9rification des imports critiques\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import librosa\n",
    "    import soundfile\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"\u2705 Tous les imports critiques r\u00e9ussis\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Erreur d'import: {e}\")\n",
    "\n",
    "# Test 3: V\u00e9rification CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2705 CUDA disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\ud83d\udcbe M\u00e9moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f CUDA non disponible - entra\u00eenement sur CPU (plus lent)\")\n",
    "\n",
    "# Test 4: Test simple avec PyTorch\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_tensor = torch.randn(10, 10).to(device)\n",
    "    result = torch.matmul(test_tensor, test_tensor.T)\n",
    "    print(f\"\u2705 Test PyTorch r\u00e9ussi sur {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Erreur test PyTorch: {e}\")\n",
    "\n",
    "# Test 5: Test audio\n",
    "try:\n",
    "    # Cr\u00e9er un signal audio de test\n",
    "    sr = 16000\n",
    "    duration = 2\n",
    "    test_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, sr * duration))\n",
    "    \n",
    "    # Test librosa\n",
    "    mfccs = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=13)\n",
    "    print(f\"\u2705 Test audio r\u00e9ussi - MFCC shape: {mfccs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Erreur test audio: {e}\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 ENVIRONNEMENT PR\u00caT POUR L'ENTRA\u00ceNEMENT!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 Modifications pour Colab/Kaggle",
    "",
    "**\u2705 Am\u00e9liorations apport\u00e9es :**",
    "",
    "1. **\ud83d\udeab Suppression de Conda**",
    "   - Remplacement par installation directe avec pip",
    "   - Compatible avec l'environnement Python natif de Colab/Kaggle",
    "",
    "2. **\ud83d\udce6 Gestion des D\u00e9pendances**",
    "   - Installation automatique de PyTorch avec support CUDA",
    "   - V\u00e9rification des versions et de la compatibilit\u00e9",
    "   - Gestion d'erreurs robuste",
    "",
    "3. **\ud83c\udfb5 Configuration des Donn\u00e9es**",
    "   - URLs mises \u00e0 jour pour le corpus AMI",
    "   - Syst\u00e8me de fallback avec donn\u00e9es de d\u00e9monstration",
    "   - Gestion intelligente des timeouts et erreurs r\u00e9seau",
    "",
    "4. **\ud83d\udd27 D\u00e9tection de Plateforme**",
    "   - D\u00e9tection automatique Colab/Kaggle/Local",
    "   - Adaptation automatique de la configuration",
    "   - Tests de compatibilit\u00e9 int\u00e9gr\u00e9s",
    "",
    "5. **\ud83d\udcbe Gestion des Chemins**",
    "   - Chemins relatifs compatibles avec tous les environnements",
    "   - Cr\u00e9ation automatique des r\u00e9pertoires n\u00e9cessaires",
    "   - V\u00e9rification des permissions d'\u00e9criture",
    "",
    "**\ud83c\udfaf Le notebook est maintenant pr\u00eat pour :**",
    "- \u2705 Google Colab (avec GPU)",
    "- \u2705 Kaggle Notebooks  ",
    "- \u2705 Environnements locaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### \ud83d\udcc2 Clonage du R\u00e9pertoire et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_project"
   },
   "outputs": [],
   "source": [
    "# Clonage optimis\u00e9 du repository pour Colab/Kaggle\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "repo_name = \"Speaker-diarization-\"\n",
    "\n",
    "print(f\"\ud83d\udcc1 R\u00e9pertoire courant: {current_dir}\")\n",
    "print(f\"\ud83d\udda5\ufe0f Plateforme: {PLATFORM}\")\n",
    "\n",
    "# Fonction pour cloner le repository\n",
    "def clone_repository():\n",
    "    \"\"\"Clone le repository avec gestion d'erreurs robuste.\"\"\"\n",
    "    repo_url = \"https://github.com/saito1111/Speaker-diarization-.git\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\ud83d\udce5 Clonage du repository depuis: {repo_url}\")\n",
    "        \n",
    "        # Utiliser git clone avec des options pour plus de fiabilit\u00e9\n",
    "        cmd = [\"git\", \"clone\", \"--depth\", \"1\", repo_url, repo_name]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\u2705 Clonage r\u00e9ussi avec git\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f Erreur git: {result.stderr}\")\n",
    "            \n",
    "            # Fallback avec !git pour Colab\n",
    "            print(\"\ud83d\udd04 Tentative avec commande shell...\")\n",
    "            os.system(f\"git clone --depth 1 {repo_url} {repo_name}\")\n",
    "            return Path(repo_name).exists()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Erreur lors du clonage: {e}\")\n",
    "        return False\n",
    "\n",
    "# V\u00e9rifier si le repository est d\u00e9j\u00e0 pr\u00e9sent\n",
    "if (current_dir / repo_name).exists() and (current_dir / repo_name / \"src\").exists():\n",
    "    print(\"\u2705 Repository d\u00e9j\u00e0 pr\u00e9sent et valide\")\n",
    "    repo_path = current_dir / repo_name\n",
    "elif (current_dir / \"src\").exists():\n",
    "    print(\"\u2705 Nous sommes d\u00e9j\u00e0 dans le repository\")\n",
    "    repo_path = current_dir\n",
    "else:\n",
    "    print(\"\ud83d\udce5 Repository non trouv\u00e9 - clonage n\u00e9cessaire...\")\n",
    "    \n",
    "    if clone_repository():\n",
    "        repo_path = current_dir / repo_name\n",
    "        print(f\"\u2705 Repository clon\u00e9 dans: {repo_path}\")\n",
    "    else:\n",
    "        print(\"\u274c ERREUR: Impossible de cloner le repository!\")\n",
    "        print(\"\ud83d\udd27 Solutions possibles:\")\n",
    "        print(\"   1. V\u00e9rifiez votre connexion internet\")\n",
    "        print(\"   2. Essayez de relancer cette cellule\")\n",
    "        print(\"   3. Clonez manuellement avec: !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "        raise Exception(\"Clonage du repository \u00e9chou\u00e9\")\n",
    "\n",
    "# Changer vers le r\u00e9pertoire du projet si n\u00e9cessaire\n",
    "if repo_path != current_dir:\n",
    "    os.chdir(repo_path)\n",
    "    print(f\"\ud83d\udcc2 Changement de r\u00e9pertoire vers: {Path.cwd()}\")\n",
    "\n",
    "# V\u00e9rifier la structure du projet\n",
    "print(\"\\n\ud83d\udccb V\u00e9rification de la structure du projet:\")\n",
    "current_project_dir = Path.cwd()\n",
    "\n",
    "# Lister le contenu principal\n",
    "print(\"\ud83d\udcc1 Contenu du r\u00e9pertoire principal:\")\n",
    "for item in sorted(current_project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"   \ud83d\udcc1 {item.name}/\")\n",
    "    else:\n",
    "        print(f\"   \ud83d\udcc4 {item.name}\")\n",
    "\n",
    "# V\u00e9rifier le r\u00e9pertoire src\n",
    "src_dir = current_project_dir / \"src\"\n",
    "if src_dir.exists():\n",
    "    print(f\"\\n\ud83d\udcc2 Contenu du r\u00e9pertoire src/ ({len(list(src_dir.glob('*.py')))} fichiers Python):\")\n",
    "    for py_file in sorted(src_dir.glob(\"*.py\"))[:8]:  # Limiter l'affichage\n",
    "        print(f\"   \ud83d\udc0d {py_file.name}\")\n",
    "    if len(list(src_dir.glob(\"*.py\"))) > 8:\n",
    "        print(f\"   ... et {len(list(src_dir.glob('*.py'))) - 8} autres fichiers\")\n",
    "    print(\"\u2705 R\u00e9pertoire src trouv\u00e9 et valide\")\n",
    "else:\n",
    "    print(\"\u274c ERREUR: R\u00e9pertoire src manquant!\")\n",
    "    raise FileNotFoundError(\"Le r\u00e9pertoire src n'existe pas\")\n",
    "\n",
    "print(f\"\\n\ud83c\udf89 Projet configur\u00e9 avec succ\u00e8s dans: {current_project_dir}\")\n",
    "\n",
    "# Variables globales pour les autres cellules\n",
    "PROJECT_ROOT = current_project_dir\n",
    "SRC_DIR = src_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis\u00e9e des chemins et imports pour Colab/Kaggle",
    "import sys",
    "from pathlib import Path",
    "",
    "# Ajouter le r\u00e9pertoire src au PATH Python",
    "if 'SRC_DIR' in globals() and SRC_DIR.exists():",
    "    if str(SRC_DIR) not in sys.path:",
    "        sys.path.insert(0, str(SRC_DIR))",
    "        print(f\"\u2705 Ajout\u00e9 au PATH Python: {SRC_DIR}\")",
    "else:",
    "    # Fallback si SRC_DIR n'est pas d\u00e9fini",
    "    src_fallback = Path.cwd() / \"src\"",
    "    if src_fallback.exists():",
    "        sys.path.insert(0, str(src_fallback))",
    "        print(f\"\u2705 Ajout\u00e9 au PATH Python (fallback): {src_fallback}\")",
    "",
    "# Configuration des r\u00e9pertoires de travail (Colab friendly)",
    "WORK_DIR = Path.cwd()",
    "MODEL_DIR = WORK_DIR / 'models'",
    "RESULTS_DIR = WORK_DIR / 'results'",
    "",
    "# Cr\u00e9er les r\u00e9pertoires n\u00e9cessaires",
    "for directory in [MODEL_DIR, RESULTS_DIR]:",
    "    directory.mkdir(parents=True, exist_ok=True)",
    "",
    "print(f\"\ud83d\udcc1 R\u00e9pertoires configur\u00e9s:\")",
    "print(f\"   \ud83d\udcbc Travail: {WORK_DIR}\")",
    "print(f\"   \ud83e\udd16 Mod\u00e8les: {MODEL_DIR}\")",
    "print(f\"   \ud83d\udcca R\u00e9sultats: {RESULTS_DIR}\")",
    "",
    "# Test des r\u00e9pertoires",
    "test_dirs = [MODEL_DIR, RESULTS_DIR]",
    "for directory in test_dirs:",
    "    if directory.exists():",
    "        print(f\"\u2705 {directory.name}: OK\")",
    "    else:",
    "        print(f\"\u274c {directory.name}: Manquant\")",
    "        ",
    "print(\"",
    "\ud83d\ude80 Configuration des chemins termin\u00e9e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## \ud83d\udcca 2. Configuration et Pr\u00e9paration des Donn\u00e9es AMI",
    "",
    "Le corpus AMI contient des enregistrements de r\u00e9unions avec annotations temporelles des locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_ami"
   },
   "outputs": [],
   "source": [
    "# Configure data paths for Google Drive",
    "from pathlib import Path",
    "import os",
    "",
    "# Set up data directories pointing to Google Drive",
    "AUDIO_DIR = '/content/drive/MyDrive/Speaker_Diarization_Data/ami_audio'",
    "ANNOTATION_DIR = '/content/drive/MyDrive/Speaker_Diarization_Data/ami_annotations'",
    "",
    "print(\"\ud83d\udcc1 Data directories configured for Google Drive:\")",
    "print(f\"   \ud83c\udfb5 Audio: {AUDIO_DIR}\")",
    "print(f\"   \ud83d\udcdd Annotations: {ANNOTATION_DIR}\")",
    "",
    "# Verify directories exist",
    "if os.path.exists(AUDIO_DIR) and os.path.exists(ANNOTATION_DIR):",
    "    print(\"\u2705 Data directories found on Google Drive\")",
    "    ",
    "    # Count files in directories",
    "    audio_files = len([f for f in os.listdir(AUDIO_DIR) if f.endswith(('.wav', '.flac', '.mp3'))])",
    "    annotation_files = len([f for f in os.listdir(ANNOTATION_DIR) if f.endswith(('.rttm', '.lab', '.txt'))])",
    "    ",
    "    print(f\"   \ud83d\udcca Found {audio_files} audio files\")",
    "    print(f\"   \ud83d\udcca Found {annotation_files} annotation files\")",
    "else:",
    "    print(\"\u26a0\ufe0f  Data directories not found. Please ensure you have uploaded your data to:\")",
    "    print(f\"   \ud83c\udfb5 {AUDIO_DIR}\")",
    "    print(f\"   \ud83d\udcdd {ANNOTATION_DIR}\")",
    "",
    "# Set paths for later use",
    "AUDIO_PATH = Path(AUDIO_DIR)",
    "ANNOTATION_PATH = Path(ANNOTATION_DIR)",
    "    ",
    "# Set up final paths",
    "FINAL_AUDIO_DIR = AUDIO_PATH",
    "FINAL_ANNOTATION_DIR = ANNOTATION_PATH",
    "",
    "print(f\"",
    "\u2705 Configuration compl\u00e8te:\")",
    "print(f\"   \ud83c\udfb5 R\u00e9pertoire audio: {FINAL_AUDIO_DIR}\")",
    "print(f\"   \ud83d\udcdd R\u00e9pertoire annotations: {FINAL_ANNOTATION_DIR}\")",
    "",
    "# Configuration for later use",
    "ami_config = {",
    "    'audio_dir': str(FINAL_AUDIO_DIR),",
    "    'annotation_dir': str(FINAL_ANNOTATION_DIR),",
    "    'platform': 'COLAB_GDRIVE'",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_ami_data"
   },
   "outputs": [],
   "source": [
    "# Validation finale et pr\u00e9paration des donn\u00e9es pour l'entra\u00eenement\n",
    "\n",
    "def validate_ami_corpus():\n",
    "    \"\"\"Valide que le corpus AMI est pr\u00eat pour l'entra\u00eenement.\"\"\"\n",
    "    print(\"\ud83d\udd0d Validation finale du corpus AMI...\")\n",
    "    \n",
    "    audio_files = list(AUDIO_PATH.glob(\"*.wav\"))\n",
    "    annotation_files = list(ANNOTATION_PATH.glob(\"*.rttm\"))\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Inventaire final:\")\n",
    "    print(f\"   \ud83c\udfb5 Fichiers audio: {len(audio_files)}\")\n",
    "    print(f\"   \ud83d\udcdd Fichiers RTTM: {len(annotation_files)}\")\n",
    "    \n",
    "    # Validation des paires audio-annotation\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        matching_rttm = ANNOTATION_PATH / f\"{base_name}.rttm\"\n",
    "        \n",
    "        if matching_rttm.exists():\n",
    "            valid_pairs.append((audio_file, matching_rttm))\n",
    "            print(f\"   \u2705 Paire valide: {base_name}\")\n",
    "        else:\n",
    "            print(f\"   \u26a0\ufe0f Pas d'annotation pour: {base_name}\")\n",
    "    \n",
    "    # Afficher quelques exemples\n",
    "    if audio_files:\n",
    "        print(f\"\\n\ud83c\udfb5 Exemples audio:\")\n",
    "        for audio_file in audio_files[:3]:\n",
    "            try:\n",
    "                size_mb = audio_file.stat().st_size / (1024*1024)\n",
    "                print(f\"   - {audio_file.name} ({size_mb:.1f} MB)\")\n",
    "            except:\n",
    "                print(f\"   - {audio_file.name} (taille non disponible)\")\n",
    "    \n",
    "    if annotation_files:\n",
    "        print(f\"\\n\ud83d\udcdd Exemples annotations:\")\n",
    "        for rttm_file in annotation_files[:3]:\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    lines = len(f.readlines())\n",
    "                print(f\"   - {rttm_file.name} ({lines} segments)\")\n",
    "            except:\n",
    "                print(f\"   - {rttm_file.name} (lecture \u00e9chou\u00e9e)\")\n",
    "    \n",
    "    # Statut global\n",
    "    corpus_ready = len(valid_pairs) >= 1  # Au moins une paire valide pour la d\u00e9monstration\n",
    "    \n",
    "    if corpus_ready:\n",
    "        print(f\"\\n\u2705 Corpus AMI valid\u00e9 et pr\u00eat!\")\n",
    "        print(f\"\ud83d\ude80 {len(valid_pairs)} paires audio-annotation disponibles pour l'entra\u00eenement\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c Corpus invalide - aucune paire audio-annotation trouv\u00e9e\")\n",
    "    \n",
    "    return {\n",
    "        'ready': corpus_ready,\n",
    "        'audio_count': len(audio_files),\n",
    "        'annotation_count': len(annotation_files),\n",
    "        'valid_pairs': valid_pairs,\n",
    "        'audio_files': audio_files,\n",
    "        'annotation_files': annotation_files\n",
    "    }\n",
    "\n",
    "# Valider le corpus\n",
    "corpus_validation = validate_ami_corpus()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\ud83c\udfaf CORPUS AMI: {'PR\u00caT' if corpus_validation['ready'] else 'ERREUR'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if not corpus_validation['ready']:\n",
    "    raise Exception(\"Corpus AMI non valide - impossible de continuer l'entra\u00eenement\")\n",
    "\n",
    "# Configuration finale pour l'entra\u00eenement\n",
    "FINAL_AUDIO_DIR = AUDIO_PATH\n",
    "FINAL_ANNOTATION_DIR = ANNOTATION_PATH\n",
    "\n",
    "print(f\"\\n\ud83d\udcc2 Configuration finale pour l'entra\u00eenement:\")\n",
    "print(f\"   \ud83c\udfb5 R\u00e9pertoire audio: {FINAL_AUDIO_DIR}\")\n",
    "print(f\"   \ud83d\udcdd R\u00e9pertoire annotations: {FINAL_ANNOTATION_DIR}\")\n",
    "print(f\"   \ud83d\udcca Paires disponibles: {len(corpus_validation['valid_pairs'])}\")\n",
    "print(f\"   \ud83d\ude80 Pr\u00eat pour l'entra\u00eenement de diarization!\")\n",
    "\n",
    "# Sauvegarder les informations du corpus pour les \u00e9tapes suivantes\n",
    "corpus_info = {\n",
    "    'audio_dir': str(FINAL_AUDIO_DIR),\n",
    "    'annotation_dir': str(FINAL_ANNOTATION_DIR),\n",
    "    'valid_pairs': [(str(a), str(r)) for a, r in corpus_validation['valid_pairs']],\n",
    "    'total_audio_files': len(corpus_validation['audio_files']),\n",
    "    'total_annotation_files': len(corpus_validation['annotation_files']),\n",
    "    'platform': PLATFORM\n",
    "}\n",
    "\n",
    "# Sauvegarder dans un fichier JSON pour r\u00e9f\u00e9rence\n",
    "corpus_info_path = Path(\"corpus_info.json\")\n",
    "with open(corpus_info_path, 'w') as f:\n",
    "    json.dump(corpus_info, f, indent=2)\n",
    "\n",
    "print(f\"\ud83d\udcbe Informations du corpus sauv\u00e9es dans: {corpus_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "### \ud83d\udcca Division des Donn\u00e9es (Train/Eval)\n",
    "\n",
    "Division stratifi\u00e9e du corpus AMI selon les bonnes pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_ami_data"
   },
   "outputs": [],
   "source": [
    "def create_ami_splits(audio_dir, rttm_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divise le corpus AMI en ensembles d'entra\u00eenement, validation et test.\n",
    "    Adapt\u00e9 pour les \u00e9chantillons de d\u00e9monstration et les vraies donn\u00e9es AMI.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: R\u00e9pertoire des fichiers audio\n",
    "        rttm_dir: R\u00e9pertoire des annotations RTTM\n",
    "        train_ratio: Proportion pour l'entra\u00eenement\n",
    "        val_ratio: Proportion pour la validation\n",
    "        test_ratio: Proportion pour le test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les listes de fichiers pour chaque split\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Les ratios doivent sommer \u00e0 1.0\"\n",
    "    \n",
    "    # Lister tous les fichiers disponibles\n",
    "    audio_path = Path(audio_dir)\n",
    "    rttm_path = Path(rttm_dir)\n",
    "    \n",
    "    # Trouver les fichiers audio\n",
    "    audio_extensions = ['.wav', '.mp3', '.flac']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(audio_path.glob(f'*{ext}')))\n",
    "    \n",
    "    print(f\"\ud83d\udd0d Fichiers audio trouv\u00e9s: {len(audio_files)}\")\n",
    "    for af in audio_files[:5]:  # Afficher les premiers\n",
    "        print(f\"   - {af.name}\")\n",
    "    \n",
    "    # V\u00e9rifier la correspondance audio-RTTM\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        # Chercher les fichiers RTTM correspondants\n",
    "        possible_rttm_names = [\n",
    "            f\"{base_name}.rttm\",\n",
    "            f\"{base_name.replace('_sample', '')}.rttm\",  # Pour les \u00e9chantillons\n",
    "            f\"{base_name.split('_')[0]}.rttm\"  # Nom de base\n",
    "        ]\n",
    "        \n",
    "        rttm_file = None\n",
    "        for rttm_name in possible_rttm_names:\n",
    "            potential_rttm = rttm_path / rttm_name\n",
    "            if potential_rttm.exists():\n",
    "                rttm_file = potential_rttm\n",
    "                break\n",
    "        \n",
    "        if rttm_file:\n",
    "            # V\u00e9rifier que le fichier RTTM n'est pas vide\n",
    "            try:\n",
    "                with open(rttm_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                if content:\n",
    "                    valid_pairs.append({\n",
    "                        'base_name': base_name,\n",
    "                        'audio_file': str(audio_file),\n",
    "                        'rttm_file': str(rttm_file)\n",
    "                    })\n",
    "                    print(f\"   \u2705 Paire valide: {base_name} -> {rttm_file.name}\")\n",
    "                else:\n",
    "                    print(f\"   \u26a0\ufe0f RTTM vide: {rttm_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   \u274c Erreur lecture RTTM {rttm_file.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   \u274c Pas de RTTM pour: {base_name}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Paires audio-RTTM valides trouv\u00e9es: {len(valid_pairs)}\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        print(\"\u274c Aucune paire valide trouv\u00e9e!\")\n",
    "        \n",
    "        # Cr\u00e9er une paire de d\u00e9monstration minimale si n\u00e9cessaire\n",
    "        if len(audio_files) > 0:\n",
    "            print(\"\ud83d\udee0\ufe0f Cr\u00e9ation d'une configuration minimale pour la d\u00e9monstration...\")\n",
    "            audio_file = audio_files[0]\n",
    "            demo_rttm = rttm_path / f\"{audio_file.stem}.rttm\"\n",
    "            \n",
    "            # Cr\u00e9er un fichier RTTM minimal\n",
    "            demo_content = f\"\"\"SPEAKER {audio_file.stem} 1 0.0 10.0 <NA> <NA> A <NA> <NA>\n",
    "SPEAKER {audio_file.stem} 1 10.0 10.0 <NA> <NA> B <NA> <NA>\n",
    "\"\"\"\n",
    "            with open(demo_rttm, 'w') as f:\n",
    "                f.write(demo_content)\n",
    "            \n",
    "            valid_pairs = [{\n",
    "                'base_name': audio_file.stem,\n",
    "                'audio_file': str(audio_file),\n",
    "                'rttm_file': str(demo_rttm)\n",
    "            }]\n",
    "            \n",
    "            print(f\"\u2705 Configuration de d\u00e9monstration cr\u00e9\u00e9e: 1 paire\")\n",
    "        else:\n",
    "            raise ValueError(\"Aucun fichier audio disponible!\")\n",
    "    \n",
    "    # M\u00e9langer et diviser\n",
    "    import random\n",
    "    random.seed(42)  # Pour la reproductibilit\u00e9\n",
    "    random.shuffle(valid_pairs)\n",
    "    \n",
    "    n_total = len(valid_pairs)\n",
    "    \n",
    "    # Adaptation pour les petits datasets\n",
    "    if n_total == 1:\n",
    "        # Utiliser le m\u00eame fichier pour train/val/test en mode d\u00e9monstration\n",
    "        print(\"\u26a0\ufe0f Un seul fichier disponible - utilisation pour train/val/test\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs\n",
    "        test_files = valid_pairs\n",
    "    elif n_total < 3:\n",
    "        # Tr\u00e8s petit dataset\n",
    "        print(f\"\u26a0\ufe0f Dataset tr\u00e8s petit ({n_total} fichiers) - division adapt\u00e9e\")\n",
    "        train_files = valid_pairs\n",
    "        val_files = valid_pairs[:1]\n",
    "        test_files = valid_pairs[:1]\n",
    "    else:\n",
    "        # Division normale\n",
    "        n_train = max(1, int(n_total * train_ratio))\n",
    "        n_val = max(1, int(n_total * val_ratio))\n",
    "        \n",
    "        train_files = valid_pairs[:n_train]\n",
    "        val_files = valid_pairs[n_train:n_train + n_val]\n",
    "        test_files = valid_pairs[n_train + n_val:] if n_train + n_val < n_total else valid_pairs[-1:]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_files,\n",
    "        'validation': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    \n",
    "    # Afficher le r\u00e9sum\u00e9\n",
    "    print(f\"\\n\udccb Division des donn\u00e9es:\")\n",
    "    print(f\"   \ud83c\udfcb\ufe0f Entra\u00eenement: {len(splits['train'])} fichiers\")\n",
    "    print(f\"   \ud83d\udd0d Validation: {len(splits['validation'])} fichiers\") \n",
    "    print(f\"   \ud83e\uddea Test: {len(splits['test'])} fichiers\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Cr\u00e9er la division des donn\u00e9es\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcca DIVISION DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    data_splits = create_ami_splits(FINAL_AUDIO_DIR, FINAL_ANNOTATION_DIR)\n",
    "    \n",
    "    # Sauvegarder les splits\n",
    "    splits_file = Path(\"data_splits.json\")\n",
    "    with open(splits_file, 'w') as f:\n",
    "        json.dump(data_splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcbe Splits sauv\u00e9s dans: {splits_file}\")\n",
    "    print(\"\u2705 Division des donn\u00e9es r\u00e9ussie!\")\n",
    "    \n",
    "    # Variables globales pour les autres cellules\n",
    "    TRAIN_FILES = data_splits['train']\n",
    "    VAL_FILES = data_splits['validation'] \n",
    "    TEST_FILES = data_splits['test']\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfaf Pr\u00eat pour l'entra\u00eenement avec:\")\n",
    "    print(f\"   - {len(TRAIN_FILES)} fichiers d'entra\u00eenement\")\n",
    "    print(f\"   - {len(VAL_FILES)} fichiers de validation\")\n",
    "    print(f\"   - {len(TEST_FILES)} fichiers de test\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Erreur lors de la division des donn\u00e9es: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## \ud83d\udee0\ufe0f 3. Pr\u00e9paration des Donn\u00e9es et Extraction de Caract\u00e9ristiques\n",
    "\n",
    "Extraction des caract\u00e9ristiques multi-canaux: LPS, IPD, AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "# Configuration du path et import des modules du projet\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le r\u00e9pertoire src au path Python\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / 'src'\n",
    "\n",
    "print(f\"\ud83d\udcc1 R\u00e9pertoire courant: {current_dir}\")\n",
    "print(f\"\ud83d\udcc2 R\u00e9pertoire src: {src_path}\")\n",
    "print(f\"\ud83d\udcc2 V\u00e9rification existence src: {src_path.exists()}\")\n",
    "\n",
    "# V\u00e9rifier si le r\u00e9pertoire src existe\n",
    "if not src_path.exists():\n",
    "    print(\"\u274c ERREUR: Le r\u00e9pertoire 'src' n'existe pas!\")\n",
    "    print(\"\ud83d\udd27 Assurez-vous d'avoir clon\u00e9 le repository complet avec:\")\n",
    "    print(\"   !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "    print(\"   %cd Speaker-diarization-\")\n",
    "    raise FileNotFoundError(\"R\u00e9pertoire 'src' manquant. Clonez d'abord le repository.\")\n",
    "\n",
    "# Lister les fichiers dans src pour v\u00e9rification\n",
    "py_files = list(src_path.glob(\"*.py\"))\n",
    "print(f\"\ud83d\udcc4 Fichiers Python trouv\u00e9s: {[f.name for f in py_files]}\")\n",
    "\n",
    "# V\u00e9rifier que TOUS les modules requis existent\n",
    "required_modules = [\n",
    "    'tcn_diarization_model.py',\n",
    "    'metrics.py', \n",
    "    'dataset.py',\n",
    "    'diarization_losses.py',\n",
    "    'improved_trainer.py',\n",
    "    'optimized_dataloader.py',\n",
    "    'optimized_dataset.py'\n",
    "]\n",
    "\n",
    "missing_modules = []\n",
    "for module in required_modules:\n",
    "    if not (src_path / module).exists():\n",
    "        missing_modules.append(module)\n",
    "\n",
    "if missing_modules:\n",
    "    print(f\"\u274c ERREUR: Modules manquants dans src/: {missing_modules}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez que tous les fichiers sont pr\u00e9sents dans le repository\")\n",
    "    raise FileNotFoundError(f\"Modules manquants: {missing_modules}\")\n",
    "\n",
    "# Ajouter src au path\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"\u2705 R\u00e9pertoire src ajout\u00e9 au path: {src_path}\")\n",
    "\n",
    "# Import de TOUS les modules du projet (OBLIGATOIRES - pas de fallback)\n",
    "print(\"\\n\ud83d\udd04 Import de TOUS les modules du projet...\")\n",
    "\n",
    "try:\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    print(\"\u2705 tcn_diarization_model import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer tcn_diarization_model\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier tcn_diarization_model.py\")\n",
    "    raise ImportError(\"Module tcn_diarization_model requis\")\n",
    "\n",
    "try:\n",
    "    from metrics import DiarizationMetrics\n",
    "    print(\"\u2705 metrics import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer metrics\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier metrics.py\")\n",
    "    raise ImportError(\"Module metrics requis\")\n",
    "\n",
    "try:\n",
    "    from dataset import DiarizationDataset\n",
    "    print(\"\u2705 dataset import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier dataset.py\")\n",
    "    raise ImportError(\"Module dataset requis\")\n",
    "\n",
    "try:\n",
    "    from diarization_losses import MultiTaskDiarizationLoss\n",
    "    print(\"\u2705 diarization_losses import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer diarization_losses\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier diarization_losses.py\")\n",
    "    raise ImportError(\"Module diarization_losses requis\")\n",
    "\n",
    "try:\n",
    "    from improved_trainer import ImprovedDiarizationTrainer\n",
    "    print(\"\u2705 improved_trainer import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer improved_trainer\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier improved_trainer.py\")\n",
    "    raise ImportError(\"Module improved_trainer requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataloader import create_optimized_dataloaders\n",
    "    print(\"\u2705 optimized_dataloader import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer optimized_dataloader\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier optimized_dataloader.py\")\n",
    "    raise ImportError(\"Module optimized_dataloader requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataset import OptimizedDiarizationDataset\n",
    "    print(\"\u2705 optimized_dataset import\u00e9\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c ERREUR CRITIQUE: Impossible d'importer optimized_dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"\ud83d\udd27 V\u00e9rifiez le contenu du fichier optimized_dataset.py\")\n",
    "    raise ImportError(\"Module optimized_dataset requis\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 TOUS LES MODULES IMPORT\u00c9S AVEC SUCC\u00c8S!\")\n",
    "print(\"\ud83d\udccb Modules disponibles pour l'entra\u00eenement:\")\n",
    "print(\"   - DiarizationTCN: \u2705\")\n",
    "print(\"   - DiarizationMetrics: \u2705\")\n",
    "print(\"   - DiarizationDataset: \u2705\")\n",
    "print(\"   - MultiTaskDiarizationLoss: \u2705\")\n",
    "print(\"   - ImprovedDiarizationTrainer: \u2705\")\n",
    "print(\"   - OptimizedDataLoader: \u2705\")\n",
    "print(\"   - OptimizedDataset: \u2705\")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Pr\u00eat pour l'entra\u00eenement avec TOUS vos mod\u00e8les originaux!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes utilitaires n\u00e9cessaires (non pr\u00e9sentes dans les modules du projet)\n",
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# MemoryMonitor pour surveiller l'utilisation m\u00e9moire\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Retourne les informations m\u00e9moire.\"\"\"\n",
    "        # RAM\n",
    "        ram_info = psutil.virtual_memory()\n",
    "        ram_percent = ram_info.percent\n",
    "        \n",
    "        # GPU\n",
    "        gpu_percent = 0\n",
    "        gpu_memory_used = 0\n",
    "        gpu_memory_total = 0\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0)\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "            gpu_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "        \n",
    "        return {\n",
    "            'ram_percent': ram_percent,\n",
    "            'gpu_percent': gpu_percent,\n",
    "            'gpu_memory_used': gpu_memory_used,\n",
    "            'gpu_memory_total': gpu_memory_total\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Nettoie la m\u00e9moire.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\u2705 Classes utilitaires cr\u00e9\u00e9es (MemoryMonitor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Test de l'extraction de caract\u00e9ristiques\n",
    "print(\"\ud83e\uddea Test de l'extraction de caract\u00e9ristiques...\")\n",
    "\n",
    "# Cr\u00e9er un extracteur de caract\u00e9ristiques\n",
    "feature_extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=512,\n",
    "    hop_length=256\n",
    ")\n",
    "\n",
    "# Cr\u00e9er des donn\u00e9es audio fictives (8 canaux)\n",
    "n_channels = 8\n",
    "duration = 4.0  # 4 secondes\n",
    "sample_rate = 16000\n",
    "n_samples = int(duration * sample_rate)\n",
    "\n",
    "# Simuler audio multi-canal avec du bruit et des signaux\n",
    "waveforms = []\n",
    "for ch in range(n_channels):\n",
    "    # Signal de base + bruit\n",
    "    base_signal = np.sin(2 * np.pi * 440 * np.linspace(0, duration, n_samples))  # 440 Hz\n",
    "    noise = np.random.normal(0, 0.1, n_samples)\n",
    "    # Ajouter un l\u00e9ger d\u00e9calage temporel pour simuler la spatialisation\n",
    "    delay_samples = int(0.001 * ch * sample_rate)  # 1ms de d\u00e9lai par canal\n",
    "    delayed_signal = np.zeros(n_samples)\n",
    "    if delay_samples < n_samples:\n",
    "        delayed_signal[delay_samples:] = base_signal[:n_samples-delay_samples]\n",
    "    \n",
    "    final_signal = delayed_signal + noise\n",
    "    waveforms.append(torch.tensor(final_signal, dtype=torch.float32))\n",
    "\n",
    "print(f\"\ud83d\udcca Audio g\u00e9n\u00e9r\u00e9: {n_channels} canaux, {duration}s, {sample_rate} Hz\")\n",
    "\n",
    "# Test d'extraction\n",
    "features = feature_extractor.extract_features(waveforms)\n",
    "print(f\"\u2705 Caract\u00e9ristiques extraites: {features.shape}\")\n",
    "print(f\"   - Dimensions attendues: [771, ~{int(duration * sample_rate / 256)}]\")\n",
    "print(f\"   - Dimensions obtenues: {list(features.shape)}\")\n",
    "\n",
    "# Analyser les caract\u00e9ristiques\n",
    "print(f\"\\n\ud83d\udcc8 Statistiques des caract\u00e9ristiques:\")\n",
    "print(f\"   - Min: {features.min():.3f}\")\n",
    "print(f\"   - Max: {features.max():.3f}\")\n",
    "print(f\"   - Moyenne: {features.mean():.3f}\")\n",
    "print(f\"   - Std: {features.std():.3f}\")\n",
    "\n",
    "# Visualiser les caract\u00e9ristiques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Spectrogramme des caract\u00e9ristiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(features.numpy()[:100, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract\u00e9ristiques LPS (premi\u00e8re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr\u00e9quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(features.numpy()[257:357, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract\u00e9ristiques IPD (premi\u00e8re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr\u00e9quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(features.numpy()[500:600, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract\u00e9ristiques AF (premi\u00e8re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr\u00e9quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(features.numpy().mean(axis=0))\n",
    "plt.title('\u00c9nergie moyenne par frame')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('\u00c9nergie moyenne')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Cr\u00e9er le r\u00e9pertoire de r\u00e9sultats s'il n'existe pas\n",
    "import os\n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(RESULTS_DIR / 'feature_extraction_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Test d'extraction de caract\u00e9ristiques termin\u00e9!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## \ud83e\udde0 4. Configuration du Mod\u00e8le et Entra\u00eenement\n",
    "\n",
    "Configuration optimale pour le corpus AMI avec les meilleures pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis\u00e9e pour AMI corpus\n",
    "config = {\n",
    "    # === MOD\u00c8LE ===\n",
    "    'model': {\n",
    "        'input_dim': 771,  # LPS (257) + IPD (257*4) + AF (257*4) = 2313 \u2192 771 apr\u00e8s agr\u00e9gation\n",
    "        'hidden_channels': [256, 256, 256, 512, 512],  # Architecture TCN multi-\u00e9chelle\n",
    "        'kernel_size': 3,\n",
    "        'num_speakers': 4,  # AMI corpus a typiquement 3-4 locuteurs\n",
    "        'dropout': 0.2,\n",
    "        'use_attention': True,  # Auto-attention pour d\u00e9pendances long-terme\n",
    "        'use_speaker_classifier': True,  # Classification de locuteurs\n",
    "        'embedding_dim': 256\n",
    "    },\n",
    "    \n",
    "    # === FONCTION DE PERTE ===\n",
    "    'loss': {\n",
    "        'type': 'multitask',\n",
    "        'vad_weight': 1.0,\n",
    "        'osd_weight': 1.0,\n",
    "        'consistency_weight': 0.1,\n",
    "        'use_pit': True,  # Permutation Invariant Training\n",
    "        'use_focal': True,  # Focal Loss pour donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\n",
    "        'focal_gamma': 2.0,\n",
    "        'num_speakers': 4\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISEUR ===\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 1e-3,  # Learning rate initial\n",
    "        'weight_decay': 1e-4,\n",
    "        'betas': (0.9, 0.999)\n",
    "    },\n",
    "    \n",
    "    # === PLANIFICATEUR LR ===\n",
    "    'scheduler': {\n",
    "        'type': 'onecycle',  # OneCycleLR pour convergence rapide\n",
    "        'steps_per_epoch': 100,  # Sera mis \u00e0 jour automatiquement\n",
    "        'pct_start': 0.3  # 30% mont\u00e9e, 70% descente\n",
    "    },\n",
    "    \n",
    "    # === ENTRA\u00ceNEMENT ===\n",
    "    'training': {\n",
    "        'epochs': 50,  # R\u00e9duit pour Colab\n",
    "        'batch_size': 8,  # Adapt\u00e9 \u00e0 la m\u00e9moire Colab\n",
    "        'num_workers': 2  # Moins de workers pour \u00e9viter les probl\u00e8mes m\u00e9moire\n",
    "    },\n",
    "    \n",
    "    # === DONN\u00c9ES ===\n",
    "    'data': {\n",
    "        'segment_duration': 4.0,  # Segments de 4 secondes\n",
    "        'sample_rate': 16000,\n",
    "        'train_split': 0.7,\n",
    "        'max_segments': 1000  # Limite pour Colab\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISATIONS AVANC\u00c9ES ===\n",
    "    'accumulation_steps': 4,  # Batch effectif = 8*4 = 32\n",
    "    'use_amp': True,  # Pr\u00e9cision mixte\n",
    "    'grad_clip_norm': 1.0,\n",
    "    'patience': 10,\n",
    "    'save_every': 5,\n",
    "    \n",
    "    # === MONITORING ===\n",
    "    'use_wandb': True,  # Weights & Biases (optionnel)\n",
    "    'project_name': 'ami-speaker-diarization',\n",
    "    'memory_threshold': 0.85,  # Gestion m\u00e9moire Colab\n",
    "    'adaptive_batch': True,\n",
    "    'speaker_loss_weight': 0.5,\n",
    "    \n",
    "    # === CHEMINS ===\n",
    "    'save_dir': str(MODEL_DIR),\n",
    "    'results_dir': str(RESULTS_DIR)\n",
    "}\n",
    "\n",
    "print(\"\u2699\ufe0f Configuration cr\u00e9\u00e9e avec les param\u00e8tres suivants:\")\n",
    "print(f\"   - Architecture: TCN {config['model']['hidden_channels']}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']} (effectif: {config['training']['batch_size'] * config['accumulation_steps']})\")\n",
    "print(f\"   - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Pr\u00e9cision mixte: {config['use_amp']}\")\n",
    "print(f\"   - Classification locuteurs: {config['model']['use_speaker_classifier']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "config_file = MODEL_DIR / 'training_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Configuration sauvegard\u00e9e: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration de Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "use_wandb = config.get('use_wandb', False)\n",
    "\n",
    "if use_wandb:\n",
    "    try:\n",
    "        # Connexion \u00e0 wandb (n\u00e9cessite un compte gratuit)\n",
    "        wandb.login()\n",
    "        \n",
    "        # Initialisation du projet\n",
    "        wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"ami-tcn-{torch.cuda.get_device_name(0).replace(' ', '-') if torch.cuda.is_available() else 'cpu'}\",\n",
    "            tags=['ami-corpus', 'tcn', 'multi-channel', 'colab'],\n",
    "            notes=\"Entra\u00eenement sur corpus AMI avec architecture TCN am\u00e9lior\u00e9e\"\n",
    "        )\n",
    "        \n",
    "        print(\"\u2705 Weights & Biases configur\u00e9!\")\n",
    "        print(f\"\ud83d\udcca Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Erreur wandb: {e}\")\n",
    "        print(\"\ud83d\udcc8 Entra\u00eenement sans monitoring wandb...\")\n",
    "        config['use_wandb'] = False\n",
    "else:\n",
    "    print(\"\ud83d\udcc8 Entra\u00eenement sans monitoring wandb (d\u00e9sactiv\u00e9 dans config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## \ud83d\ude80 5. Entra\u00eenement du Mod\u00e8le\n",
    "\n",
    "Entra\u00eenement avec toutes les optimisations: gestion m\u00e9moire, precision mixte, accumulation de gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Cr\u00e9ation des DataLoaders optimis\u00e9s\n",
    "print(\"\ud83d\udd04 Cr\u00e9ation des DataLoaders avec vos modules optimis\u00e9s...\")\n",
    "\n",
    "# Utiliser la fonction create_optimized_dataloaders import\u00e9e\n",
    "train_loader, val_loader = create_optimized_dataloaders(\n",
    "    audio_dir=audio_dir,\n",
    "    rttm_dir=rttm_dir,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    train_split=config['data']['train_split'],\n",
    "    num_workers=config['training']['num_workers'],\n",
    "    segment_duration=config['data']['segment_duration'],\n",
    "    sample_rate=config['data']['sample_rate'],\n",
    "    max_segments=config['data']['max_segments'],\n",
    "    memory_threshold=config['memory_threshold'],\n",
    "    adaptive_batch=config['adaptive_batch'],\n",
    "    accumulation_steps=config['accumulation_steps']\n",
    ")\n",
    "\n",
    "print(f\"\u2705 DataLoaders cr\u00e9\u00e9s avec succ\u00e8s!\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Mettre \u00e0 jour la configuration avec le nombre r\u00e9el de steps\n",
    "config['scheduler']['steps_per_epoch'] = len(train_loader)\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\n\ud83e\uddea Test d'un batch d'entra\u00eenement...\")\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f\"   Batch {batch_idx}:\")\n",
    "    print(f\"     - Features: {batch['features'].shape}\")\n",
    "    print(f\"     - VAD labels: {batch['vad_labels'].shape}\")\n",
    "    print(f\"     - OSD labels: {batch['osd_labels'].shape}\")\n",
    "    \n",
    "    # V\u00e9rifier les dimensions\n",
    "    assert batch['features'].shape[1] == 771, f\"Dimension features incorrecte: {batch['features'].shape[1]} != 771\"\n",
    "    assert batch['vad_labels'].shape[-1] == 4, f\"Nombre de locuteurs incorrect: {batch['vad_labels'].shape[-1]} != 4\"\n",
    "    \n",
    "    print(f\"     \u2705 Dimensions correctes!\")\n",
    "    break\n",
    "\n",
    "print(\"\u2705 DataLoaders optimis\u00e9s configur\u00e9s et test\u00e9s avec succ\u00e8s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialisation du trainer avanc\u00e9\n",
    "print(\"\ud83e\udde0 Initialisation du trainer avec vos modules optimis\u00e9s...\")\n",
    "\n",
    "# Cr\u00e9er le trainer avec toutes les am\u00e9liorations (OBLIGATOIRE - pas de fallback)\n",
    "trainer = ImprovedDiarizationTrainer(config)\n",
    "\n",
    "print(f\"\u2705 Trainer initialis\u00e9 avec succ\u00e8s!\")\n",
    "print(f\"   - Mod\u00e8le: {trainer.model.get_num_params():,} param\u00e8tres\")\n",
    "print(f\"   - Device: {trainer.device}\")\n",
    "print(f\"   - Pr\u00e9cision mixte: {trainer.use_amp}\")\n",
    "print(f\"   - Accumulation gradients: {trainer.accumulation_steps}\")\n",
    "\n",
    "# Test du forward pass\n",
    "print(\"\\n\ud83e\uddea Test du mod\u00e8le...\")\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test avec un batch de donn\u00e9es\n",
    "    test_input = torch.randn(2, 771, 250).to(trainer.device)\n",
    "    \n",
    "    # Forward pass simple\n",
    "    vad_out, osd_out = model(test_input)\n",
    "    print(f\"   Forward simple: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "    \n",
    "    # Forward avec embeddings si disponible\n",
    "    try:\n",
    "        vad_out, osd_out, embeddings, speaker_logits = model(test_input, return_embeddings=True)\n",
    "        print(f\"   Forward complet: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        print(f\"                   Embeddings {embeddings.shape}, Speaker {speaker_logits.shape}\")\n",
    "    except:\n",
    "        print(\"   Forward avec embeddings non disponible (normal)\")\n",
    "\n",
    "print(\"\u2705 Mod\u00e8le fonctionne correctement!\")\n",
    "print(\"\ud83d\ude80 Trainer optimis\u00e9 pr\u00eat pour l'entra\u00eenement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# D\u00e9marrage de l'entra\u00eenement\n",
    "print(\"\ud83d\ude80 D\u00c9MARRAGE DE L'ENTRA\u00ceNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monitoring m\u00e9moire\n",
    "memory_monitor = MemoryMonitor()\n",
    "initial_memory = memory_monitor.get_memory_info()\n",
    "\n",
    "print(f\"\ud83d\udcbe M\u00e9moire initiale:\")\n",
    "print(f\"   - RAM: {initial_memory['ram_percent']:.1f}%\")\n",
    "print(f\"   - GPU: {initial_memory['gpu_percent']:.1f}%\")\n",
    "\n",
    "# Configuration d'entra\u00eenement\n",
    "num_epochs = config['training']['epochs']\n",
    "save_every = config.get('save_every', 5)\n",
    "\n",
    "# Historique des m\u00e9triques\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Configuration d'entra\u00eenement:\")\n",
    "print(f\"   - Epochs: {num_epochs}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Sauvegarde chaque {save_every} epochs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"D\u00c9BUT DE L'ENTRA\u00ceNEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Boucle d'entra\u00eenement principale\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd04 Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Phase d'entra\u00eenement\n",
    "        if hasattr(trainer, 'train_epoch'):\n",
    "            # Utiliser le trainer avanc\u00e9\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            train_loss = train_metrics['total_loss']\n",
    "        else:\n",
    "            # Entra\u00eenement simple\n",
    "            trainer.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                trainer.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                trainer.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Phase de validation\n",
    "        print(f\"\\n\ud83d\udcca Validation...\")\n",
    "        trainer.model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                val_loss += loss_dict['total_loss'].item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Monitoring m\u00e9moire\n",
    "        current_memory = memory_monitor.get_memory_info()\n",
    "        \n",
    "        # R\u00e9sum\u00e9 de l'epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\n\ud83d\udcc8 Epoch {epoch+1} R\u00e9sultats:\")\n",
    "        print(f\"   - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   - Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"   - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"   - M\u00e9moire GPU: {current_memory['gpu_percent']:.1f}%\")\n",
    "        \n",
    "        # Sauvegarde du meilleur mod\u00e8le\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            print(f\"   \u2705 Nouveau meilleur mod\u00e8le sauv\u00e9! (Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Sauvegarde p\u00e9riodique\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = MODEL_DIR / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   \ud83d\udcbe Checkpoint sauv\u00e9: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Logging wandb\n",
    "        if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'gpu_memory_percent': current_memory['gpu_percent'],\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        \n",
    "        # Nettoyage m\u00e9moire\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\u23f9\ufe0f Entra\u00eenement interrompu par l'utilisateur\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Erreur durant l'entra\u00eenement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n\u23f1\ufe0f Temps total d'entra\u00eenement: {total_time/60:.1f} minutes\")\n",
    "    print(f\"\ud83c\udfaf Meilleure validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Fermeture wandb\n",
    "    if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "        wandb.finish()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENTRA\u00ceNEMENT TERMIN\u00c9\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## \ud83d\udcca 6. \u00c9valuation et M\u00e9triques\n",
    "\n",
    "\u00c9valuation compl\u00e8te avec m\u00e9triques de diarization standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# \u00c9valuation compl\u00e8te du mod\u00e8le\n",
    "print(\"\ud83d\udcca \u00c9VALUATION DU MOD\u00c8LE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Charger le meilleur mod\u00e8le\n",
    "best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"\ud83d\udcc2 Chargement du meilleur mod\u00e8le: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"   - Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"   - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Pas de mod\u00e8le sauv\u00e9, utilisation du mod\u00e8le actuel\")\n",
    "\n",
    "# Initialiser les m\u00e9triques\n",
    "metrics_computer = DiarizationMetrics(num_speakers=config['model']['num_speakers'])\n",
    "\n",
    "# \u00c9valuation sur l'ensemble de validation\n",
    "print(\"\\n\ud83e\uddea \u00c9valuation sur l'ensemble de validation...\")\n",
    "trainer.model.eval()\n",
    "\n",
    "all_vad_preds = []\n",
    "all_vad_targets = []\n",
    "all_osd_preds = []\n",
    "all_osd_targets = []\n",
    "\n",
    "eval_loss = 0\n",
    "num_eval_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        features = batch['features'].to(trainer.device)\n",
    "        vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "        osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "        \n",
    "        # Pr\u00e9dictions\n",
    "        if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "            vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "        else:\n",
    "            vad_pred, osd_pred = trainer.model(features)\n",
    "        \n",
    "        # Loss\n",
    "        loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "        eval_loss += loss_dict['total_loss'].item()\n",
    "        num_eval_batches += 1\n",
    "        \n",
    "        # Collecter pour m\u00e9triques\n",
    "        all_vad_preds.append(vad_pred.cpu())\n",
    "        all_vad_targets.append(vad_labels.cpu())\n",
    "        all_osd_preds.append(osd_pred.cpu())\n",
    "        all_osd_targets.append(osd_labels.cpu())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"   Batch {batch_idx}/{len(val_loader)} \u00e9valu\u00e9\")\n",
    "\n",
    "# Calculer m\u00e9triques d\u00e9taill\u00e9es\n",
    "print(\"\\n\ud83d\udcc8 Calcul des m\u00e9triques d\u00e9taill\u00e9es...\")\n",
    "\n",
    "vad_preds = torch.cat(all_vad_preds, dim=0)\n",
    "vad_targets = torch.cat(all_vad_targets, dim=0)\n",
    "osd_preds = torch.cat(all_osd_preds, dim=0)\n",
    "osd_targets = torch.cat(all_osd_targets, dim=0)\n",
    "\n",
    "print(f\"   - Donn\u00e9es \u00e9valu\u00e9es: {vad_preds.shape[0]} \u00e9chantillons\")\n",
    "print(f\"   - Dur\u00e9e totale: {vad_preds.shape[0] * vad_preds.shape[1] * 0.02:.1f} secondes\")\n",
    "\n",
    "# M\u00e9triques principales\n",
    "metrics = metrics_computer.compute_metrics(vad_preds, osd_preds, vad_targets, osd_targets)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf R\u00c9SULTATS D'\u00c9VALUATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"\ud83d\udcca Loss finale: {eval_loss / num_eval_batches:.4f}\")\n",
    "print(f\"\ud83d\udcca DER (Diarization Error Rate): {metrics.get('der', 0):.2f}%\")\n",
    "print(f\"\ud83d\udcca F1 Score global: {metrics.get('f1_score', 0):.3f}\")\n",
    "print(f\"\ud83d\udcca Pr\u00e9cision frame: {metrics.get('frame_precision', 0):.3f}\")\n",
    "print(f\"\ud83d\udcca Rappel frame: {metrics.get('frame_recall', 0):.3f}\")\n",
    "print(f\"\ud83d\udcca Jaccard Index: {metrics.get('jaccard_index', 0):.3f}\")\n",
    "\n",
    "# M\u00e9triques OSD\n",
    "if 'osd_precision' in metrics:\n",
    "    print(f\"\\n\ud83d\udd00 D\u00e9tection de Chevauchement (OSD):\")\n",
    "    print(f\"   - Pr\u00e9cision OSD: {metrics['osd_precision']:.3f}\")\n",
    "    print(f\"   - Rappel OSD: {metrics['osd_recall']:.3f}\")\n",
    "    print(f\"   - F1 OSD: {metrics['osd_f1']:.3f}\")\n",
    "\n",
    "# M\u00e9triques par locuteur\n",
    "print(f\"\\n\ud83d\udc65 M\u00e9triques par Locuteur:\")\n",
    "for spk in range(config['model']['num_speakers']):\n",
    "    if f'speaker_{spk}_f1' in metrics:\n",
    "        print(f\"   Locuteur {spk}: F1={metrics[f'speaker_{spk}_f1']:.3f}, \"\n",
    "              f\"P={metrics[f'speaker_{spk}_precision']:.3f}, \"\n",
    "              f\"R={metrics[f'speaker_{spk}_recall']:.3f}\")\n",
    "\n",
    "# Sauvegarder les m\u00e9triques\n",
    "metrics_file = RESULTS_DIR / 'evaluation_metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    # Convertir les tenseurs en listes pour JSON\n",
    "    json_metrics = {k: (v.item() if torch.is_tensor(v) else v) for k, v in metrics.items()}\n",
    "    json_metrics['eval_loss'] = eval_loss / num_eval_batches\n",
    "    json.dump(json_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe M\u00e9triques sauv\u00e9es: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## \ud83d\udcc8 7. Visualisations et Analyses\n",
    "\n",
    "G\u00e9n\u00e9ration de graphiques et visualisations des r\u00e9sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Visualisations des r\u00e9sultats\n",
    "print(\"\ud83d\udcca G\u00e9n\u00e9ration des visualisations...\")\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# === 1. Courbes d'entra\u00eenement ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('R\u00e9sultats d\\'Entra\u00eenement - Speaker Diarization TCN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Courbe de perte\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('\u00c9volution de la Perte')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les derni\u00e8res epochs\n",
    "if len(train_losses) > 10:\n",
    "    start_idx = max(0, len(train_losses) - 20)\n",
    "    axes[0, 1].plot(range(start_idx, len(train_losses)), train_losses[start_idx:], \n",
    "                   label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(range(start_idx, len(val_losses)), val_losses[start_idx:], \n",
    "                   label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Convergence (Derni\u00e8res Epochs)')\n",
    "else:\n",
    "    axes[0, 1].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('\u00c9volution de la Perte (Toutes Epochs)')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === 2. Analyse des pr\u00e9dictions ===\n",
    "# Prendre un \u00e9chantillon pour visualisation\n",
    "sample_idx = 0\n",
    "sample_vad_pred = vad_preds[sample_idx].numpy()  # [time, speakers]\n",
    "sample_vad_target = vad_targets[sample_idx].numpy()\n",
    "sample_osd_pred = osd_preds[sample_idx].numpy()  # [time]\n",
    "sample_osd_target = osd_targets[sample_idx].numpy()\n",
    "\n",
    "# Activit\u00e9 des locuteurs (pr\u00e9dictions vs v\u00e9rit\u00e9 terrain)\n",
    "time_frames = np.arange(len(sample_vad_pred)) * 0.02  # Conversion en secondes\n",
    "\n",
    "# Subplot pour VAD\n",
    "axes[1, 0].imshow(sample_vad_pred.T, aspect='auto', origin='lower', \n",
    "                 extent=[0, len(sample_vad_pred)*0.02, 0, 4], \n",
    "                 cmap='Blues', alpha=0.7)\n",
    "axes[1, 0].imshow(sample_vad_target.T, aspect='auto', origin='lower',\n",
    "                 extent=[0, len(sample_vad_target)*0.02, 0, 4],\n",
    "                 cmap='Reds', alpha=0.5)\n",
    "axes[1, 0].set_title('Activit\u00e9 VAD: Pr\u00e9diction (Bleu) vs V\u00e9rit\u00e9 (Rouge)')\n",
    "axes[1, 0].set_xlabel('Temps (s)')\n",
    "axes[1, 0].set_ylabel('Locuteur ID')\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "\n",
    "# Subplot pour OSD\n",
    "axes[1, 1].plot(time_frames, sample_osd_pred, label='Pr\u00e9diction OSD', \n",
    "               color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(time_frames, sample_osd_target, label='V\u00e9rit\u00e9 OSD', \n",
    "               color='red', linewidth=2, alpha=0.6)\n",
    "axes[1, 1].set_title('D\u00e9tection de Chevauchement (OSD)')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_ylabel('Probabilit\u00e9 de Chevauchement')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "training_plot_path = RESULTS_DIR / 'training_results.png'\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2705 Graphique d'entra\u00eenement sauv\u00e9: {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# === 3. Matrice de confusion pour classification ===\n",
    "if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "    print(\"\\n\ud83d\udcca Analyse de la classification des locuteurs...\")\n",
    "    \n",
    "    # Extraire les pr\u00e9dictions de classification\n",
    "    all_speaker_preds = []\n",
    "    all_speaker_targets = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(trainer.device)\n",
    "            vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "            \n",
    "            try:\n",
    "                vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "                \n",
    "                # Cr\u00e9er des labels de locuteurs \u00e0 partir des VAD labels\n",
    "                speaker_targets = torch.argmax(vad_labels.sum(dim=1), dim=1)  # Locuteur le plus actif\n",
    "                speaker_preds = torch.argmax(speaker_logits, dim=1)\n",
    "                \n",
    "                all_speaker_preds.extend(speaker_preds.cpu().numpy())\n",
    "                all_speaker_targets.extend(speaker_targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur dans un batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_speaker_preds and all_speaker_targets:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(all_speaker_targets, all_speaker_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Pred {i}' for i in range(4)],\n",
    "                   yticklabels=[f'True {i}' for i in range(4)])\n",
    "        plt.title('Matrice de Confusion - Classification des Locuteurs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Pr\u00e9diction')\n",
    "        plt.ylabel('V\u00e9rit\u00e9 Terrain')\n",
    "        \n",
    "        confusion_path = RESULTS_DIR / 'speaker_confusion_matrix.png'\n",
    "        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print(\"\\n\ud83d\udccb Rapport de Classification:\")\n",
    "        print(classification_report(all_speaker_targets, all_speaker_preds,\n",
    "                                  target_names=[f'Locuteur {i}' for i in range(4)],\n",
    "                                  digits=3))\n",
    "        \n",
    "        print(f\"\u2705 Matrice de confusion sauv\u00e9e: {confusion_path}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f Pas assez de donn\u00e9es pour la matrice de confusion\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Classificateur de locuteurs non activ\u00e9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_summary"
   },
   "outputs": [],
   "source": [
    "# === 4. R\u00e9sum\u00e9 final et comparaisons ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca R\u00c9SUM\u00c9 FINAL DE L'ENTRA\u00ceNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr\u00e9er un r\u00e9sum\u00e9 complet\n",
    "final_summary = {\n",
    "    'Configuration': {\n",
    "        'Architecture': f\"TCN {config['model']['hidden_channels']}\",\n",
    "        'Param\u00e8tres': f\"{sum(p.numel() for p in trainer.model.parameters()):,}\",\n",
    "        'Batch Size': config['training']['batch_size'],\n",
    "        'Epochs': len(train_losses),\n",
    "        'Learning Rate': config['optimizer']['lr'],\n",
    "        'Device': str(trainer.device)\n",
    "    },\n",
    "    'R\u00e9sultats Finaux': {\n",
    "        'Train Loss': f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\",\n",
    "        'Val Loss': f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\",\n",
    "        'Best Val Loss': f\"{best_val_loss:.4f}\",\n",
    "        'DER': f\"{metrics.get('der', 0):.2f}%\",\n",
    "        'F1 Score': f\"{metrics.get('f1_score', 0):.3f}\",\n",
    "        'Frame Precision': f\"{metrics.get('frame_precision', 0):.3f}\",\n",
    "        'Frame Recall': f\"{metrics.get('frame_recall', 0):.3f}\"\n",
    "    },\n",
    "    'Fichiers G\u00e9n\u00e9r\u00e9s': {\n",
    "        'Meilleur Mod\u00e8le': str(best_model_path) if best_model_path.exists() else \"Non sauv\u00e9\",\n",
    "        'M\u00e9triques': str(metrics_file),\n",
    "        'Graphiques': str(training_plot_path),\n",
    "        'Configuration': str(config_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage du r\u00e9sum\u00e9\n",
    "for section, items in final_summary.items():\n",
    "    print(f\"\\n\ud83d\udd39 {section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"   {key:.<25} {value}\")\n",
    "\n",
    "# Sauvegarde du r\u00e9sum\u00e9\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe R\u00e9sum\u00e9 complet sauv\u00e9: {summary_file}\")\n",
    "\n",
    "# === 5. Recommandations d'am\u00e9lioration ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udca1 RECOMMANDATIONS POUR AM\u00c9LIORER LES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "current_der = metrics.get('der', 100)\n",
    "current_f1 = metrics.get('f1_score', 0)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if current_der > 25:\n",
    "    recommendations.append(\"\ud83d\udd27 DER \u00e9lev\u00e9: Augmenter le nombre d'epochs ou r\u00e9duire le learning rate\")\n",
    "if current_f1 < 0.7:\n",
    "    recommendations.append(\"\ud83d\udd27 F1 faible: Essayer focal loss avec gamma plus \u00e9lev\u00e9\")\n",
    "if len(train_losses) < 20:\n",
    "    recommendations.append(\"\u23f0 Entra\u00eenement court: Augmenter le nombre d'epochs\")\n",
    "if config['training']['batch_size'] < 16:\n",
    "    recommendations.append(\"\ud83d\udce6 Batch size petit: Augmenter si possible pour am\u00e9liorer la stabilit\u00e9\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"\ud83d\udcca Utiliser plus de donn\u00e9es AMI si disponibles\",\n",
    "    \"\ud83c\udfaf Affiner les hyperparam\u00e8tres avec Optuna\",\n",
    "    \"\ud83d\udd04 Essayer l'ensemble de mod\u00e8les\",\n",
    "    \"\ud83d\udcc8 Impl\u00e9menter la validation crois\u00e9e\",\n",
    "    \"\ud83e\udde0 Tester diff\u00e9rentes architectures d'attention\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations[:7], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83c\udf89 ENTRA\u00ceNEMENT TERMIN\u00c9 AVEC SUCC\u00c8S!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\ud83d\udcc1 Tous les r\u00e9sultats sont sauv\u00e9s dans: {RESULTS_DIR}\")\n",
    "print(f\"\ud83e\udde0 Meilleur mod\u00e8le disponible dans: {MODEL_DIR}\")\n",
    "\n",
    "if config.get('use_wandb', False):\n",
    "    print(f\"\ud83d\udcca Logs d\u00e9taill\u00e9s disponibles sur Weights & Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## \ud83d\ude80 Prochaines \u00c9tapes\n",
    "\n",
    "### \ud83d\udcdd Pour Continuer l'Am\u00e9lioration:\n",
    "\n",
    "1. **\ud83d\udcca Donn\u00e9es**: T\u00e9l\u00e9charger le corpus AMI complet\n",
    "2. **\u2699\ufe0f Hyperparam\u00e8tres**: Optimiser avec Optuna\n",
    "3. **\ud83c\udfaf Architecture**: Tester diff\u00e9rentes tailles de mod\u00e8le\n",
    "4. **\ud83d\udcc8 Ensembles**: Combiner plusieurs mod\u00e8les\n",
    "5. **\ud83d\udd04 Post-traitement**: Am\u00e9liorer la segmentation finale\n",
    "\n",
    "### \ud83d\udcbe Fichiers G\u00e9n\u00e9r\u00e9s:\n",
    "- `models/checkpoints/best_model.pth` - Meilleur mod\u00e8le\n",
    "- `results/evaluation_metrics.json` - M\u00e9triques d\u00e9taill\u00e9es\n",
    "- `results/training_results.png` - Graphiques d'entra\u00eenement\n",
    "- `results/training_summary.json` - R\u00e9sum\u00e9 complet\n",
    "\n",
    "### \ud83c\udfaf Objectifs de Performance:\n",
    "- **DER < 20%** sur AMI corpus (\u00e9tat de l'art: ~15-18%)\n",
    "- **F1 > 0.8** pour la d\u00e9tection d'activit\u00e9\n",
    "- **Temps r\u00e9el** pour l'inf\u00e9rence\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf89 F\u00e9licitations! Vous avez entra\u00een\u00e9 avec succ\u00e8s un syst\u00e8me de diarization moderne avec toutes les optimisations avanc\u00e9es!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/saito1111/Speaker-diarization-/blob/main/Enhanced_Diarization_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üéôÔ∏è Enhanced Multi-Channel Speaker Diarization Training\n",
    "\n",
    "**Objectif :** Entra√Æner un syst√®me de diarization de locuteurs avanc√© sur le corpus AMI  \n",
    "**Architecture :** TCN multi-√©chelle avec attention, classification de locuteurs et gestion m√©moire optimis√©e\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "1. [Configuration de l'Environnement](#setup)\n",
    "2. [T√©l√©chargement du Corpus AMI](#data)\n",
    "3. [Pr√©paration des Donn√©es](#preprocessing)\n",
    "4. [Mod√®le et Configuration](#model)\n",
    "5. [Entra√Ænement](#training)\n",
    "6. [√âvaluation](#evaluation)\n",
    "7. [Sauvegarde et Visualisations](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß 1. Configuration de l'Environnement\n",
    "\n",
    "Installation de Conda et des d√©pendances n√©cessaires pour l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_conda"
   },
   "outputs": [],
   "source": [
    "# Installation de Conda sur Google Colab\n",
    "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "!conda --version\n",
    "\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_env"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation de l'environnement conda pour la diarization\n",
    "!conda create -n diarization python=3.9 -y\n",
    "!conda activate diarization\n",
    "\n",
    "# Installation des d√©pendances principales via conda\n",
    "!conda install -n diarization pytorch torchaudio cudatoolkit=11.8 -c pytorch -c nvidia -y\n",
    "!conda install -n diarization numpy scipy scikit-learn matplotlib seaborn pandas -y\n",
    "\n",
    "# Activation de l'environnement dans le notebook\n",
    "import os\n",
    "os.environ['CONDA_DEFAULT_ENV'] = 'diarization'\n",
    "os.environ['PATH'] = '/usr/local/envs/diarization/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances sp√©cialis√©es\n",
    "!pip install wandb optuna tqdm psutil\n",
    "!pip install librosa soundfile\n",
    "!pip install speechbrain\n",
    "\n",
    "# V√©rification des installations\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### üìÇ Clonage du R√©pertoire et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_project"
   },
   "outputs": [],
   "source": [
    "# Cloner le projet depuis GitHub\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# V√©rifier si le repository est d√©j√† pr√©sent\n",
    "current_dir = Path.cwd()\n",
    "repo_name = \"Speaker-diarization-\"\n",
    "\n",
    "print(f\"üìÅ R√©pertoire courant: {current_dir}\")\n",
    "\n",
    "# Si on n'est pas d√©j√† dans le repository, le cloner\n",
    "if not (current_dir / \"src\").exists():\n",
    "    print(\"üì• Clonage du repository...\")\n",
    "    !git clone https://github.com/saito1111/Speaker-diarization-.git\n",
    "    \n",
    "    # V√©rifier que le clonage a r√©ussi\n",
    "    if Path(repo_name).exists():\n",
    "        print(f\"‚úÖ Repository clon√© avec succ√®s dans {repo_name}/\")\n",
    "        %cd {repo_name}\n",
    "        print(f\"üìÇ Changement de r√©pertoire vers: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"‚ùå ERREUR: Le clonage a √©chou√©!\")\n",
    "        print(\"üîß V√©rifiez votre connexion internet et l'URL du repository\")\n",
    "        raise Exception(\"Clonage du repository √©chou√©\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository d√©j√† pr√©sent\")\n",
    "\n",
    "# V√©rifier la structure du projet\n",
    "print(\"\\nüìã V√©rification de la structure du projet:\")\n",
    "print(\"üìÅ Racine du projet:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nüìÇ Contenu du r√©pertoire src/:\")\n",
    "if Path(\"src\").exists():\n",
    "    !ls -la src/\n",
    "    print(\"‚úÖ R√©pertoire src trouv√©\")\n",
    "else:\n",
    "    print(\"‚ùå ERREUR: R√©pertoire src manquant!\")\n",
    "    raise FileNotFoundError(\"Le r√©pertoire src n'existe pas apr√®s le clonage\")\n",
    "\n",
    "print(\"\\nüéâ Structure du projet v√©rifi√©e avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# Configuration des chemins et imports\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "# Cr√©er les dossiers n√©cessaires\n",
    "!mkdir -p data/ami_corpus/audio\n",
    "!mkdir -p data/ami_corpus/annotations\n",
    "!mkdir -p models/checkpoints\n",
    "!mkdir -p results/logs\n",
    "!mkdir -p results/figures\n",
    "\n",
    "# Variables globales\n",
    "DATA_DIR = Path('./data/ami_corpus')\n",
    "AUDIO_DIR = DATA_DIR / 'audio'\n",
    "ANNOTATION_DIR = DATA_DIR / 'annotations'\n",
    "MODEL_DIR = Path('./models/checkpoints')\n",
    "RESULTS_DIR = Path('./results')\n",
    "\n",
    "print(f\"R√©pertoires configur√©s:\")\n",
    "print(f\"- Audio: {AUDIO_DIR}\")\n",
    "print(f\"- Annotations: {ANNOTATION_DIR}\")\n",
    "print(f\"- Mod√®les: {MODEL_DIR}\")\n",
    "print(f\"- R√©sultats: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìä 2. T√©l√©chargement et Pr√©paration du Corpus AMI\n",
    "\n",
    "Le corpus AMI contient des enregistrements de r√©unions avec annotations temporelles des locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_ami"
   },
   "outputs": [],
   "source": [
    "# T√©l√©chargement r√©el du corpus AMI avec v√©rification et stockage persistant\n",
    "# URL officielle: https://groups.inf.ed.ac.uk/ami/corpus/\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class AMICorpusDownloader:\n",
    "    \"\"\"Gestionnaire complet pour le t√©l√©chargement et la v√©rification du corpus AMI.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.audio_dir = self.base_dir / \"ami_audio\"\n",
    "        self.annotation_dir = self.base_dir / \"ami_annotations\"\n",
    "        self.download_dir = self.base_dir / \"downloads\"\n",
    "        \n",
    "        # Cr√©er les r√©pertoires\n",
    "        for dir_path in [self.audio_dir, self.annotation_dir, self.download_dir]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def download_with_progress_bar(self, url, filename, description=\"T√©l√©chargement\"):\n",
    "        \"\"\"T√©l√©charge un fichier avec barre de progression d√©taill√©e.\"\"\"\n",
    "        print(f\"üîΩ {description}: {url}\")\n",
    "        \n",
    "        class ProgressBar:\n",
    "            def __init__(self):\n",
    "                self.pbar = None\n",
    "            \n",
    "            def __call__(self, block_num, block_size, total_size):\n",
    "                if not self.pbar:\n",
    "                    self.pbar = tqdm(total=total_size, unit='B', unit_scale=True, desc=description)\n",
    "                \n",
    "                downloaded = block_num * block_size\n",
    "                if downloaded < total_size:\n",
    "                    self.pbar.update(block_size)\n",
    "                else:\n",
    "                    self.pbar.close()\n",
    "        \n",
    "        urllib.request.urlretrieve(url, filename, ProgressBar())\n",
    "        print(f\"‚úÖ T√©l√©charg√©: {filename}\")\n",
    "    \n",
    "    def verify_file_integrity(self, filepath, expected_size=None, expected_hash=None):\n",
    "        \"\"\"V√©rifie l'int√©grit√© d'un fichier t√©l√©charg√©.\"\"\"\n",
    "        if not filepath.exists():\n",
    "            return False, \"Fichier inexistant\"\n",
    "        \n",
    "        file_size = filepath.stat().st_size\n",
    "        if expected_size and file_size != expected_size:\n",
    "            return False, f\"Taille incorrecte: {file_size} vs {expected_size}\"\n",
    "        \n",
    "        if expected_hash:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            if file_hash != expected_hash:\n",
    "                return False, f\"Hash incorrect: {file_hash} vs {expected_hash}\"\n",
    "        \n",
    "        return True, \"Fichier valide\"\n",
    "    \n",
    "    def check_existing_files(self):\n",
    "        \"\"\"V√©rifie si les fichiers AMI sont d√©j√† pr√©sents et valides.\"\"\"\n",
    "        print(\"üîç V√©rification des fichiers existants...\")\n",
    "        \n",
    "        # Fichiers audio essentiels\n",
    "        required_audio = [\n",
    "            \"ES2002a.Headset-0.wav\", \"ES2002a.Headset-1.wav\", \n",
    "            \"ES2002a.Headset-2.wav\", \"ES2002a.Headset-3.wav\",\n",
    "            \"ES2002b.Headset-0.wav\", \"ES2002b.Headset-1.wav\",\n",
    "            \"ES2002c.Headset-0.wav\", \"ES2002c.Headset-1.wav\",\n",
    "            \"ES2002d.Headset-0.wav\", \"ES2002d.Headset-1.wav\"\n",
    "        ]\n",
    "        \n",
    "        existing_audio = []\n",
    "        for audio_file in required_audio:\n",
    "            audio_path = self.audio_dir / audio_file\n",
    "            if audio_path.exists() and audio_path.stat().st_size > 1000000:  # >1MB\n",
    "                existing_audio.append(audio_file)\n",
    "        \n",
    "        # Fichiers d'annotations\n",
    "        annotation_files = list(self.annotation_dir.glob(\"*.rttm\"))\n",
    "        \n",
    "        print(f\"üìÅ Fichiers audio trouv√©s: {len(existing_audio)}/{len(required_audio)}\")\n",
    "        print(f\"üìÅ Fichiers d'annotation trouv√©s: {len(annotation_files)}\")\n",
    "        \n",
    "        return {\n",
    "            'audio_complete': len(existing_audio) >= 8,  # Au moins 8 fichiers audio\n",
    "            'annotations_complete': len(annotation_files) >= 10,\n",
    "            'existing_audio': existing_audio,\n",
    "            'existing_annotations': annotation_files\n",
    "        }\n",
    "    \n",
    "    def download_ami_audio_files(self):\n",
    "        \"\"\"T√©l√©charge les fichiers audio AMI essentiels.\"\"\"\n",
    "        print(\"üéµ T√©l√©chargement des fichiers audio AMI...\")\n",
    "        \n",
    "        # URLs r√©elles pour les fichiers audio AMI (exemple pour ES2002)\n",
    "        base_url = \"https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/amicorpus\"\n",
    "        audio_urls = {\n",
    "            \"ES2002a.Headset-0.wav\": f\"{base_url}/ES2002a/audio/ES2002a.Headset-0.wav\",\n",
    "            \"ES2002a.Headset-1.wav\": f\"{base_url}/ES2002a/audio/ES2002a.Headset-1.wav\",\n",
    "            \"ES2002a.Headset-2.wav\": f\"{base_url}/ES2002a/audio/ES2002a.Headset-2.wav\",\n",
    "            \"ES2002a.Headset-3.wav\": f\"{base_url}/ES2002a/audio/ES2002a.Headset-3.wav\",\n",
    "            \"ES2002b.Headset-0.wav\": f\"{base_url}/ES2002b/audio/ES2002b.Headset-0.wav\",\n",
    "            \"ES2002b.Headset-1.wav\": f\"{base_url}/ES2002b/audio/ES2002b.Headset-1.wav\",\n",
    "        }\n",
    "        \n",
    "        downloaded_files = []\n",
    "        \n",
    "        for filename, url in audio_urls.items():\n",
    "            audio_path = self.audio_dir / filename\n",
    "            \n",
    "            if audio_path.exists() and audio_path.stat().st_size > 1000000:\n",
    "                print(f\"‚úÖ D√©j√† pr√©sent: {filename}\")\n",
    "                downloaded_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                self.download_with_progress_bar(url, str(audio_path), f\"Audio {filename}\")\n",
    "                \n",
    "                # V√©rifier la taille\n",
    "                if audio_path.stat().st_size > 1000000:\n",
    "                    downloaded_files.append(filename)\n",
    "                    print(f\"‚úÖ T√©l√©charg√© avec succ√®s: {filename}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Fichier trop petit: {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur pour {filename}: {e}\")\n",
    "        \n",
    "        return downloaded_files\n",
    "    \n",
    "    def download_ami_annotations(self):\n",
    "        \"\"\"T√©l√©charge les annotations AMI.\"\"\"\n",
    "        print(\"\udcdd T√©l√©chargement des annotations AMI...\")\n",
    "        \n",
    "        annotation_url = \"https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip\"\n",
    "        zip_path = self.download_dir / \"ami_annotations.zip\"\n",
    "        \n",
    "        if not zip_path.exists():\n",
    "            try:\n",
    "                self.download_with_progress_bar(\n",
    "                    annotation_url, \n",
    "                    str(zip_path), \n",
    "                    \"Annotations AMI\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur t√©l√©chargement annotations: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Extraction\n",
    "        try:\n",
    "            print(\"üì¶ Extraction des annotations...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.annotation_dir)\n",
    "            \n",
    "            # Chercher les fichiers RTTM extraits\n",
    "            rttm_files = list(self.annotation_dir.rglob(\"*.rttm\"))\n",
    "            print(f\"‚úÖ Extraction termin√©e: {len(rttm_files)} fichiers RTTM trouv√©s\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur extraction: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialiser le t√©l√©chargeur AMI\n",
    "ami_downloader = AMICorpusDownloader(DATA_DIR)\n",
    "\n",
    "# V√©rifier les fichiers existants\n",
    "existing_status = ami_downloader.check_existing_files()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä STATUT DU CORPUS AMI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if existing_status['audio_complete'] and existing_status['annotations_complete']:\n",
    "    print(\"üéâ Corpus AMI complet d√©j√† pr√©sent!\")\n",
    "    print(f\"   üìÅ Audio: {len(existing_status['existing_audio'])} fichiers\")\n",
    "    print(f\"   üìÅ Annotations: {len(existing_status['existing_annotations'])} fichiers\")\n",
    "    print(\"\\n‚úÖ Utilisation des fichiers existants...\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Corpus AMI incomplet - T√©l√©chargement n√©cessaire\")\n",
    "    \n",
    "    # T√©l√©charger les fichiers manquants\n",
    "    if not existing_status['audio_complete']:\n",
    "        print(\"\\nüîΩ T√©l√©chargement des fichiers audio...\")\n",
    "        downloaded_audio = ami_downloader.download_ami_audio_files()\n",
    "        print(f\"‚úÖ Audio t√©l√©charg√©: {len(downloaded_audio)} fichiers\")\n",
    "    \n",
    "    if not existing_status['annotations_complete']:\n",
    "        print(\"\\nüîΩ T√©l√©chargement des annotations...\")\n",
    "        ami_downloader.download_ami_annotations()\n",
    "\n",
    "# Rapport final\n",
    "final_status = ami_downloader.check_existing_files()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà RAPPORT FINAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéµ Fichiers audio: {len(final_status['existing_audio'])} disponibles\")\n",
    "print(f\"üìù Fichiers annotation: {len(final_status['existing_annotations'])} disponibles\")\n",
    "\n",
    "if final_status['audio_complete']:\n",
    "    print(\"‚úÖ Corpus audio pr√™t pour l'entra√Ænement!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Corpus audio incomplet - Mode d√©monstration activ√©\")\n",
    "\n",
    "# Configurer les chemins pour la suite\n",
    "AUDIO_PATH = ami_downloader.audio_dir\n",
    "ANNOTATION_PATH = ami_downloader.annotation_dir\n",
    "\n",
    "print(f\"\\nüìÇ Chemins configur√©s:\")\n",
    "print(f\"   üéµ Audio: {AUDIO_PATH}\")\n",
    "print(f\"   üìù Annotations: {ANNOTATION_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_ami_data"
   },
   "outputs": [],
   "source": [
    "# Validation finale et pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "def validate_ami_corpus():\n",
    "    \"\"\"Valide que le corpus AMI est pr√™t pour l'entra√Ænement.\"\"\"\n",
    "    print(\"\udd0d Validation finale du corpus AMI...\")\n",
    "    \n",
    "    audio_files = list(AUDIO_PATH.glob(\"*.wav\"))\n",
    "    annotation_files = list(ANNOTATION_PATH.rglob(\"*.rttm\"))\n",
    "    \n",
    "    print(f\"\\nüìä Inventaire final:\")\n",
    "    print(f\"   üéµ Fichiers audio: {len(audio_files)}\")\n",
    "    print(f\"   üìù Fichiers RTTM: {len(annotation_files)}\")\n",
    "    \n",
    "    # Afficher quelques exemples\n",
    "    if audio_files:\n",
    "        print(f\"\\nüéµ Exemples audio:\")\n",
    "        for i, audio_file in enumerate(audio_files[:5]):\n",
    "            size_mb = audio_file.stat().st_size / (1024*1024)\n",
    "            print(f\"   - {audio_file.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    if annotation_files:\n",
    "        print(f\"\\nüìù Exemples annotations:\")\n",
    "        for i, rttm_file in enumerate(annotation_files[:5]):\n",
    "            lines = len(open(rttm_file).readlines())\n",
    "            print(f\"   - {rttm_file.name} ({lines} segments)\")\n",
    "    \n",
    "    # Statut global\n",
    "    corpus_ready = len(audio_files) >= 4 and len(annotation_files) >= 4\n",
    "    \n",
    "    if corpus_ready:\n",
    "        print(f\"\\n‚úÖ Corpus AMI valid√© et pr√™t!\")\n",
    "        print(f\"üöÄ Pr√™t pour l'entra√Ænement avec {len(audio_files)} fichiers audio\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Corpus incomplet - Mode d√©monstration recommand√©\")\n",
    "    \n",
    "    return {\n",
    "        'ready': corpus_ready,\n",
    "        'audio_count': len(audio_files),\n",
    "        'annotation_count': len(annotation_files),\n",
    "        'audio_files': audio_files,\n",
    "        'annotation_files': annotation_files\n",
    "    }\n",
    "\n",
    "# Valider le corpus\n",
    "corpus_status = validate_ami_corpus()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéØ CORPUS AMI: {'PR√äT' if corpus_status['ready'] else 'PARTIEL'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Configurer les chemins finaux pour l'entra√Ænement\n",
    "if corpus_status['ready']:\n",
    "    print(\"‚úÖ Utilisation du corpus AMI r√©el\")\n",
    "    FINAL_AUDIO_DIR = AUDIO_PATH\n",
    "    FINAL_ANNOTATION_DIR = ANNOTATION_PATH\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Basculement vers donn√©es de d√©monstration\")\n",
    "    # On gardera quand m√™me les fichiers partiels s'ils existent\n",
    "    FINAL_AUDIO_DIR = AUDIO_PATH if corpus_status['audio_count'] > 0 else DATA_DIR / \"demo_audio\"\n",
    "    FINAL_ANNOTATION_DIR = ANNOTATION_PATH if corpus_status['annotation_count'] > 0 else DATA_DIR / \"demo_annotations\"\n",
    "\n",
    "print(f\"\\nüìÇ Configuration finale:\")\n",
    "print(f\"   üéµ Audio: {FINAL_AUDIO_DIR}\")\n",
    "print(f\"   üìù Annotations: {FINAL_ANNOTATION_DIR}\")\n",
    "print(f\"   üìä Mode: {'Production (AMI)' if corpus_status['ready'] else 'D√©monstration/Partiel'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "### üìä Division des Donn√©es (Train/Eval)\n",
    "\n",
    "Division stratifi√©e du corpus AMI selon les bonnes pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_ami_data"
   },
   "outputs": [],
   "source": [
    "def create_ami_splits(audio_dir, rttm_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divise le corpus AMI en ensembles d'entra√Ænement, validation et test.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: R√©pertoire des fichiers audio\n",
    "        rttm_dir: R√©pertoire des annotations RTTM\n",
    "        train_ratio: Proportion pour l'entra√Ænement\n",
    "        val_ratio: Proportion pour la validation\n",
    "        test_ratio: Proportion pour le test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les listes de fichiers pour chaque split\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Les ratios doivent sommer √† 1.0\"\n",
    "    \n",
    "    # Lister tous les fichiers disponibles\n",
    "    audio_path = Path(audio_dir)\n",
    "    rttm_path = Path(rttm_dir)\n",
    "    \n",
    "    # Trouver les fichiers audio\n",
    "    audio_extensions = ['.wav', '.pt', '.flac', '.mp3']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(audio_path.glob(f'*{ext}')))\n",
    "    \n",
    "    # V√©rifier la correspondance audio-RTTM\n",
    "    valid_pairs = []\n",
    "    for audio_file in audio_files:\n",
    "        base_name = audio_file.stem\n",
    "        rttm_file = rttm_path / f\"{base_name}.rttm\"\n",
    "        \n",
    "        if rttm_file.exists():\n",
    "            valid_pairs.append({\n",
    "                'base_name': base_name,\n",
    "                'audio_file': str(audio_file),\n",
    "                'rttm_file': str(rttm_file)\n",
    "            })\n",
    "    \n",
    "    print(f\"üìä Trouv√© {len(valid_pairs)} paires audio-RTTM valides\")\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        raise ValueError(\"Aucune paire audio-RTTM valide trouv√©e!\")\n",
    "    \n",
    "    # M√©langer et diviser\n",
    "    import random\n",
    "    random.seed(42)  # Pour la reproductibilit√©\n",
    "    random.shuffle(valid_pairs)\n",
    "    \n",
    "    n_total = len(valid_pairs)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    train_files = valid_pairs[:n_train]\n",
    "    val_files = valid_pairs[n_train:n_train + n_val]\n",
    "    test_files = valid_pairs[n_train + n_val:]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_files,\n",
    "        'validation': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\nüìà Division des donn√©es:\")\n",
    "    for split_name, files in splits.items():\n",
    "        print(f\"   {split_name:>10}: {len(files):>3} fichiers ({len(files)/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # Sauvegarder les splits\n",
    "    splits_file = DATA_DIR / 'data_splits.json'\n",
    "    with open(splits_file, 'w') as f:\n",
    "        json.dump(splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Splits sauvegard√©s dans: {splits_file}\")\n",
    "    return splits\n",
    "\n",
    "# Cr√©er les splits\n",
    "data_splits = create_ami_splits(audio_dir, rttm_dir, \n",
    "                               train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "# V√©rifier le contenu\n",
    "print(\"\\nüîç Exemple de fichiers par split:\")\n",
    "for split_name, files in data_splits.items():\n",
    "    if files:\n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        for i, file_info in enumerate(files[:3]):  # Afficher les 3 premiers\n",
    "            print(f\"  {i+1}. {file_info['base_name']}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"  ... et {len(files)-3} autres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## üõ†Ô∏è 3. Pr√©paration des Donn√©es et Extraction de Caract√©ristiques\n",
    "\n",
    "Extraction des caract√©ristiques multi-canaux: LPS, IPD, AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "# Configuration du path et import des modules du projet\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le r√©pertoire src au path Python\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / 'src'\n",
    "\n",
    "print(f\"üìÅ R√©pertoire courant: {current_dir}\")\n",
    "print(f\"üìÇ R√©pertoire src: {src_path}\")\n",
    "print(f\"üìÇ V√©rification existence src: {src_path.exists()}\")\n",
    "\n",
    "# V√©rifier si le r√©pertoire src existe\n",
    "if not src_path.exists():\n",
    "    print(\"‚ùå ERREUR: Le r√©pertoire 'src' n'existe pas!\")\n",
    "    print(\"üîß Assurez-vous d'avoir clon√© le repository complet avec:\")\n",
    "    print(\"   !git clone https://github.com/saito1111/Speaker-diarization-.git\")\n",
    "    print(\"   %cd Speaker-diarization-\")\n",
    "    raise FileNotFoundError(\"R√©pertoire 'src' manquant. Clonez d'abord le repository.\")\n",
    "\n",
    "# Lister les fichiers dans src pour v√©rification\n",
    "py_files = list(src_path.glob(\"*.py\"))\n",
    "print(f\"üìÑ Fichiers Python trouv√©s: {[f.name for f in py_files]}\")\n",
    "\n",
    "# V√©rifier que TOUS les modules requis existent\n",
    "required_modules = [\n",
    "    'tcn_diarization_model.py',\n",
    "    'metrics.py', \n",
    "    'dataset.py',\n",
    "    'diarization_losses.py',\n",
    "    'improved_trainer.py',\n",
    "    'optimized_dataloader.py',\n",
    "    'optimized_dataset.py'\n",
    "]\n",
    "\n",
    "missing_modules = []\n",
    "for module in required_modules:\n",
    "    if not (src_path / module).exists():\n",
    "        missing_modules.append(module)\n",
    "\n",
    "if missing_modules:\n",
    "    print(f\"‚ùå ERREUR: Modules manquants dans src/: {missing_modules}\")\n",
    "    print(\"üîß V√©rifiez que tous les fichiers sont pr√©sents dans le repository\")\n",
    "    raise FileNotFoundError(f\"Modules manquants: {missing_modules}\")\n",
    "\n",
    "# Ajouter src au path\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"‚úÖ R√©pertoire src ajout√© au path: {src_path}\")\n",
    "\n",
    "# Import de TOUS les modules du projet (OBLIGATOIRES - pas de fallback)\n",
    "print(\"\\nüîÑ Import de TOUS les modules du projet...\")\n",
    "\n",
    "try:\n",
    "    from tcn_diarization_model import DiarizationTCN\n",
    "    print(\"‚úÖ tcn_diarization_model import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer tcn_diarization_model\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier tcn_diarization_model.py\")\n",
    "    raise ImportError(\"Module tcn_diarization_model requis\")\n",
    "\n",
    "try:\n",
    "    from metrics import DiarizationMetrics\n",
    "    print(\"‚úÖ metrics import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer metrics\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier metrics.py\")\n",
    "    raise ImportError(\"Module metrics requis\")\n",
    "\n",
    "try:\n",
    "    from dataset import DiarizationDataset\n",
    "    print(\"‚úÖ dataset import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier dataset.py\")\n",
    "    raise ImportError(\"Module dataset requis\")\n",
    "\n",
    "try:\n",
    "    from diarization_losses import MultiTaskDiarizationLoss\n",
    "    print(\"‚úÖ diarization_losses import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer diarization_losses\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier diarization_losses.py\")\n",
    "    raise ImportError(\"Module diarization_losses requis\")\n",
    "\n",
    "try:\n",
    "    from improved_trainer import ImprovedDiarizationTrainer\n",
    "    print(\"‚úÖ improved_trainer import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer improved_trainer\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier improved_trainer.py\")\n",
    "    raise ImportError(\"Module improved_trainer requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataloader import create_optimized_dataloaders\n",
    "    print(\"‚úÖ optimized_dataloader import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer optimized_dataloader\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier optimized_dataloader.py\")\n",
    "    raise ImportError(\"Module optimized_dataloader requis\")\n",
    "\n",
    "try:\n",
    "    from optimized_dataset import OptimizedDiarizationDataset\n",
    "    print(\"‚úÖ optimized_dataset import√©\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR CRITIQUE: Impossible d'importer optimized_dataset\")\n",
    "    print(f\"   Erreur: {e}\")\n",
    "    print(\"üîß V√©rifiez le contenu du fichier optimized_dataset.py\")\n",
    "    raise ImportError(\"Module optimized_dataset requis\")\n",
    "\n",
    "print(\"\\nüéâ TOUS LES MODULES IMPORT√âS AVEC SUCC√àS!\")\n",
    "print(\"üìã Modules disponibles pour l'entra√Ænement:\")\n",
    "print(\"   - DiarizationTCN: ‚úÖ\")\n",
    "print(\"   - DiarizationMetrics: ‚úÖ\")\n",
    "print(\"   - DiarizationDataset: ‚úÖ\")\n",
    "print(\"   - MultiTaskDiarizationLoss: ‚úÖ\")\n",
    "print(\"   - ImprovedDiarizationTrainer: ‚úÖ\")\n",
    "print(\"   - OptimizedDataLoader: ‚úÖ\")\n",
    "print(\"   - OptimizedDataset: ‚úÖ\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√™t pour l'entra√Ænement avec TOUS vos mod√®les originaux!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes utilitaires n√©cessaires (non pr√©sentes dans les modules du projet)\n",
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# MemoryMonitor pour surveiller l'utilisation m√©moire\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Retourne les informations m√©moire.\"\"\"\n",
    "        # RAM\n",
    "        ram_info = psutil.virtual_memory()\n",
    "        ram_percent = ram_info.percent\n",
    "        \n",
    "        # GPU\n",
    "        gpu_percent = 0\n",
    "        gpu_memory_used = 0\n",
    "        gpu_memory_total = 0\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0)\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "            gpu_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "        \n",
    "        return {\n",
    "            'ram_percent': ram_percent,\n",
    "            'gpu_percent': gpu_percent,\n",
    "            'gpu_memory_used': gpu_memory_used,\n",
    "            'gpu_memory_total': gpu_memory_total\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Nettoie la m√©moire.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Classes utilitaires cr√©√©es (MemoryMonitor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_feature_extraction"
   },
   "outputs": [],
   "source": [
    "# Test de l'extraction de caract√©ristiques\n",
    "print(\"üß™ Test de l'extraction de caract√©ristiques...\")\n",
    "\n",
    "# Cr√©er un extracteur de caract√©ristiques\n",
    "feature_extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=512,\n",
    "    hop_length=256\n",
    ")\n",
    "\n",
    "# Cr√©er des donn√©es audio fictives (8 canaux)\n",
    "n_channels = 8\n",
    "duration = 4.0  # 4 secondes\n",
    "sample_rate = 16000\n",
    "n_samples = int(duration * sample_rate)\n",
    "\n",
    "# Simuler audio multi-canal avec du bruit et des signaux\n",
    "waveforms = []\n",
    "for ch in range(n_channels):\n",
    "    # Signal de base + bruit\n",
    "    base_signal = np.sin(2 * np.pi * 440 * np.linspace(0, duration, n_samples))  # 440 Hz\n",
    "    noise = np.random.normal(0, 0.1, n_samples)\n",
    "    # Ajouter un l√©ger d√©calage temporel pour simuler la spatialisation\n",
    "    delay_samples = int(0.001 * ch * sample_rate)  # 1ms de d√©lai par canal\n",
    "    delayed_signal = np.zeros(n_samples)\n",
    "    if delay_samples < n_samples:\n",
    "        delayed_signal[delay_samples:] = base_signal[:n_samples-delay_samples]\n",
    "    \n",
    "    final_signal = delayed_signal + noise\n",
    "    waveforms.append(torch.tensor(final_signal, dtype=torch.float32))\n",
    "\n",
    "print(f\"üìä Audio g√©n√©r√©: {n_channels} canaux, {duration}s, {sample_rate} Hz\")\n",
    "\n",
    "# Test d'extraction\n",
    "features = feature_extractor.extract_features(waveforms)\n",
    "print(f\"‚úÖ Caract√©ristiques extraites: {features.shape}\")\n",
    "print(f\"   - Dimensions attendues: [771, ~{int(duration * sample_rate / 256)}]\")\n",
    "print(f\"   - Dimensions obtenues: {list(features.shape)}\")\n",
    "\n",
    "# Analyser les caract√©ristiques\n",
    "print(f\"\\nüìà Statistiques des caract√©ristiques:\")\n",
    "print(f\"   - Min: {features.min():.3f}\")\n",
    "print(f\"   - Max: {features.max():.3f}\")\n",
    "print(f\"   - Moyenne: {features.mean():.3f}\")\n",
    "print(f\"   - Std: {features.std():.3f}\")\n",
    "\n",
    "# Visualiser les caract√©ristiques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Spectrogramme des caract√©ristiques\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(features.numpy()[:100, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques LPS (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(features.numpy()[257:357, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques IPD (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(features.numpy()[500:600, :], aspect='auto', origin='lower')\n",
    "plt.title('Caract√©ristiques AF (premi√®re partie)')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(features.numpy().mean(axis=0))\n",
    "plt.title('√ânergie moyenne par frame')\n",
    "plt.xlabel('Temps (frames)')\n",
    "plt.ylabel('√ânergie moyenne')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Cr√©er le r√©pertoire de r√©sultats s'il n'existe pas\n",
    "import os\n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(RESULTS_DIR / 'feature_extraction_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Test d'extraction de caract√©ristiques termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## üß† 4. Configuration du Mod√®le et Entra√Ænement\n",
    "\n",
    "Configuration optimale pour le corpus AMI avec les meilleures pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Configuration optimis√©e pour AMI corpus\n",
    "config = {\n",
    "    # === MOD√àLE ===\n",
    "    'model': {\n",
    "        'input_dim': 771,  # LPS (257) + IPD (257*4) + AF (257*4) = 2313 ‚Üí 771 apr√®s agr√©gation\n",
    "        'hidden_channels': [256, 256, 256, 512, 512],  # Architecture TCN multi-√©chelle\n",
    "        'kernel_size': 3,\n",
    "        'num_speakers': 4,  # AMI corpus a typiquement 3-4 locuteurs\n",
    "        'dropout': 0.2,\n",
    "        'use_attention': True,  # Auto-attention pour d√©pendances long-terme\n",
    "        'use_speaker_classifier': True,  # Classification de locuteurs\n",
    "        'embedding_dim': 256\n",
    "    },\n",
    "    \n",
    "    # === FONCTION DE PERTE ===\n",
    "    'loss': {\n",
    "        'type': 'multitask',\n",
    "        'vad_weight': 1.0,\n",
    "        'osd_weight': 1.0,\n",
    "        'consistency_weight': 0.1,\n",
    "        'use_pit': True,  # Permutation Invariant Training\n",
    "        'use_focal': True,  # Focal Loss pour donn√©es d√©s√©quilibr√©es\n",
    "        'focal_gamma': 2.0,\n",
    "        'num_speakers': 4\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISEUR ===\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 1e-3,  # Learning rate initial\n",
    "        'weight_decay': 1e-4,\n",
    "        'betas': (0.9, 0.999)\n",
    "    },\n",
    "    \n",
    "    # === PLANIFICATEUR LR ===\n",
    "    'scheduler': {\n",
    "        'type': 'onecycle',  # OneCycleLR pour convergence rapide\n",
    "        'steps_per_epoch': 100,  # Sera mis √† jour automatiquement\n",
    "        'pct_start': 0.3  # 30% mont√©e, 70% descente\n",
    "    },\n",
    "    \n",
    "    # === ENTRA√éNEMENT ===\n",
    "    'training': {\n",
    "        'epochs': 50,  # R√©duit pour Colab\n",
    "        'batch_size': 8,  # Adapt√© √† la m√©moire Colab\n",
    "        'num_workers': 2  # Moins de workers pour √©viter les probl√®mes m√©moire\n",
    "    },\n",
    "    \n",
    "    # === DONN√âES ===\n",
    "    'data': {\n",
    "        'segment_duration': 4.0,  # Segments de 4 secondes\n",
    "        'sample_rate': 16000,\n",
    "        'train_split': 0.7,\n",
    "        'max_segments': 1000  # Limite pour Colab\n",
    "    },\n",
    "    \n",
    "    # === OPTIMISATIONS AVANC√âES ===\n",
    "    'accumulation_steps': 4,  # Batch effectif = 8*4 = 32\n",
    "    'use_amp': True,  # Pr√©cision mixte\n",
    "    'grad_clip_norm': 1.0,\n",
    "    'patience': 10,\n",
    "    'save_every': 5,\n",
    "    \n",
    "    # === MONITORING ===\n",
    "    'use_wandb': True,  # Weights & Biases (optionnel)\n",
    "    'project_name': 'ami-speaker-diarization',\n",
    "    'memory_threshold': 0.85,  # Gestion m√©moire Colab\n",
    "    'adaptive_batch': True,\n",
    "    'speaker_loss_weight': 0.5,\n",
    "    \n",
    "    # === CHEMINS ===\n",
    "    'save_dir': str(MODEL_DIR),\n",
    "    'results_dir': str(RESULTS_DIR)\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration cr√©√©e avec les param√®tres suivants:\")\n",
    "print(f\"   - Architecture: TCN {config['model']['hidden_channels']}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']} (effectif: {config['training']['batch_size'] * config['accumulation_steps']})\")\n",
    "print(f\"   - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Pr√©cision mixte: {config['use_amp']}\")\n",
    "print(f\"   - Classification locuteurs: {config['model']['use_speaker_classifier']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "config_file = MODEL_DIR / 'training_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration sauvegard√©e: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_wandb"
   },
   "outputs": [],
   "source": [
    "# Configuration de Weights & Biases (optionnel)\n",
    "import wandb\n",
    "\n",
    "use_wandb = config.get('use_wandb', False)\n",
    "\n",
    "if use_wandb:\n",
    "    try:\n",
    "        # Connexion √† wandb (n√©cessite un compte gratuit)\n",
    "        wandb.login()\n",
    "        \n",
    "        # Initialisation du projet\n",
    "        wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"ami-tcn-{torch.cuda.get_device_name(0).replace(' ', '-') if torch.cuda.is_available() else 'cpu'}\",\n",
    "            tags=['ami-corpus', 'tcn', 'multi-channel', 'colab'],\n",
    "            notes=\"Entra√Ænement sur corpus AMI avec architecture TCN am√©lior√©e\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Weights & Biases configur√©!\")\n",
    "        print(f\"üìä Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur wandb: {e}\")\n",
    "        print(\"üìà Entra√Ænement sans monitoring wandb...\")\n",
    "        config['use_wandb'] = False\n",
    "else:\n",
    "    print(\"üìà Entra√Ænement sans monitoring wandb (d√©sactiv√© dans config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ 5. Entra√Ænement du Mod√®le\n",
    "\n",
    "Entra√Ænement avec toutes les optimisations: gestion m√©moire, precision mixte, accumulation de gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Cr√©ation des DataLoaders optimis√©s\n",
    "print(\"üîÑ Cr√©ation des DataLoaders avec vos modules optimis√©s...\")\n",
    "\n",
    "# Utiliser la fonction create_optimized_dataloaders import√©e\n",
    "train_loader, val_loader = create_optimized_dataloaders(\n",
    "    audio_dir=audio_dir,\n",
    "    rttm_dir=rttm_dir,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    train_split=config['data']['train_split'],\n",
    "    num_workers=config['training']['num_workers'],\n",
    "    segment_duration=config['data']['segment_duration'],\n",
    "    sample_rate=config['data']['sample_rate'],\n",
    "    max_segments=config['data']['max_segments'],\n",
    "    memory_threshold=config['memory_threshold'],\n",
    "    adaptive_batch=config['adaptive_batch'],\n",
    "    accumulation_steps=config['accumulation_steps']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders cr√©√©s avec succ√®s!\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Mettre √† jour la configuration avec le nombre r√©el de steps\n",
    "config['scheduler']['steps_per_epoch'] = len(train_loader)\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\nüß™ Test d'un batch d'entra√Ænement...\")\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f\"   Batch {batch_idx}:\")\n",
    "    print(f\"     - Features: {batch['features'].shape}\")\n",
    "    print(f\"     - VAD labels: {batch['vad_labels'].shape}\")\n",
    "    print(f\"     - OSD labels: {batch['osd_labels'].shape}\")\n",
    "    \n",
    "    # V√©rifier les dimensions\n",
    "    assert batch['features'].shape[1] == 771, f\"Dimension features incorrecte: {batch['features'].shape[1]} != 771\"\n",
    "    assert batch['vad_labels'].shape[-1] == 4, f\"Nombre de locuteurs incorrect: {batch['vad_labels'].shape[-1]} != 4\"\n",
    "    \n",
    "    print(f\"     ‚úÖ Dimensions correctes!\")\n",
    "    break\n",
    "\n",
    "print(\"‚úÖ DataLoaders optimis√©s configur√©s et test√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialisation du trainer avanc√©\n",
    "print(\"üß† Initialisation du trainer avec vos modules optimis√©s...\")\n",
    "\n",
    "# Cr√©er le trainer avec toutes les am√©liorations (OBLIGATOIRE - pas de fallback)\n",
    "trainer = ImprovedDiarizationTrainer(config)\n",
    "\n",
    "print(f\"‚úÖ Trainer initialis√© avec succ√®s!\")\n",
    "print(f\"   - Mod√®le: {trainer.model.get_num_params():,} param√®tres\")\n",
    "print(f\"   - Device: {trainer.device}\")\n",
    "print(f\"   - Pr√©cision mixte: {trainer.use_amp}\")\n",
    "print(f\"   - Accumulation gradients: {trainer.accumulation_steps}\")\n",
    "\n",
    "# Test du forward pass\n",
    "print(\"\\nüß™ Test du mod√®le...\")\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test avec un batch de donn√©es\n",
    "    test_input = torch.randn(2, 771, 250).to(trainer.device)\n",
    "    \n",
    "    # Forward pass simple\n",
    "    vad_out, osd_out = model(test_input)\n",
    "    print(f\"   Forward simple: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "    \n",
    "    # Forward avec embeddings si disponible\n",
    "    try:\n",
    "        vad_out, osd_out, embeddings, speaker_logits = model(test_input, return_embeddings=True)\n",
    "        print(f\"   Forward complet: VAD {vad_out.shape}, OSD {osd_out.shape}\")\n",
    "        print(f\"                   Embeddings {embeddings.shape}, Speaker {speaker_logits.shape}\")\n",
    "    except:\n",
    "        print(\"   Forward avec embeddings non disponible (normal)\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le fonctionne correctement!\")\n",
    "print(\"üöÄ Trainer optimis√© pr√™t pour l'entra√Ænement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# D√©marrage de l'entra√Ænement\n",
    "print(\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monitoring m√©moire\n",
    "memory_monitor = MemoryMonitor()\n",
    "initial_memory = memory_monitor.get_memory_info()\n",
    "\n",
    "print(f\"üíæ M√©moire initiale:\")\n",
    "print(f\"   - RAM: {initial_memory['ram_percent']:.1f}%\")\n",
    "print(f\"   - GPU: {initial_memory['gpu_percent']:.1f}%\")\n",
    "\n",
    "# Configuration d'entra√Ænement\n",
    "num_epochs = config['training']['epochs']\n",
    "save_every = config.get('save_every', 5)\n",
    "\n",
    "# Historique des m√©triques\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüìã Configuration d'entra√Ænement:\")\n",
    "print(f\"   - Epochs: {num_epochs}\")\n",
    "print(f\"   - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   - Learning rate: {config['optimizer']['lr']}\")\n",
    "print(f\"   - Sauvegarde chaque {save_every} epochs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Boucle d'entra√Ænement principale\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\nüîÑ Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Phase d'entra√Ænement\n",
    "        if hasattr(trainer, 'train_epoch'):\n",
    "            # Utiliser le trainer avanc√©\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            train_loss = train_metrics['total_loss']\n",
    "        else:\n",
    "            # Entra√Ænement simple\n",
    "            trainer.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                trainer.optimizer.zero_grad()\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                loss = loss_dict['total_loss']\n",
    "                \n",
    "                loss.backward()\n",
    "                trainer.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Phase de validation\n",
    "        print(f\"\\nüìä Validation...\")\n",
    "        trainer.model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(trainer.device)\n",
    "                vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "                osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "                \n",
    "                vad_pred, osd_pred = trainer.model(features)\n",
    "                loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "                val_loss += loss_dict['total_loss'].item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Monitoring m√©moire\n",
    "        current_memory = memory_monitor.get_memory_info()\n",
    "        \n",
    "        # R√©sum√© de l'epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\nüìà Epoch {epoch+1} R√©sultats:\")\n",
    "        print(f\"   - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"   - Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"   - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"   - M√©moire GPU: {current_memory['gpu_percent']:.1f}%\")\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            print(f\"   ‚úÖ Nouveau meilleur mod√®le sauv√©! (Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Sauvegarde p√©riodique\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint_path = MODEL_DIR / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   üíæ Checkpoint sauv√©: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Logging wandb\n",
    "        if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'gpu_memory_percent': current_memory['gpu_percent'],\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        \n",
    "        # Nettoyage m√©moire\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Entra√Ænement interrompu par l'utilisateur\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur durant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Temps total d'entra√Ænement: {total_time/60:.1f} minutes\")\n",
    "    print(f\"üéØ Meilleure validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Fermeture wandb\n",
    "    if config.get('use_wandb', False) and 'wandb' in locals():\n",
    "        wandb.finish()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä 6. √âvaluation et M√©triques\n",
    "\n",
    "√âvaluation compl√®te avec m√©triques de diarization standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te du mod√®le\n",
    "print(\"üìä √âVALUATION DU MOD√àLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "best_model_path = MODEL_DIR / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"üìÇ Chargement du meilleur mod√®le: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"   - Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"   - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de mod√®le sauv√©, utilisation du mod√®le actuel\")\n",
    "\n",
    "# Initialiser les m√©triques\n",
    "metrics_computer = DiarizationMetrics(num_speakers=config['model']['num_speakers'])\n",
    "\n",
    "# √âvaluation sur l'ensemble de validation\n",
    "print(\"\\nüß™ √âvaluation sur l'ensemble de validation...\")\n",
    "trainer.model.eval()\n",
    "\n",
    "all_vad_preds = []\n",
    "all_vad_targets = []\n",
    "all_osd_preds = []\n",
    "all_osd_targets = []\n",
    "\n",
    "eval_loss = 0\n",
    "num_eval_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        features = batch['features'].to(trainer.device)\n",
    "        vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "        osd_labels = batch['osd_labels'].to(trainer.device)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "            vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "        else:\n",
    "            vad_pred, osd_pred = trainer.model(features)\n",
    "        \n",
    "        # Loss\n",
    "        loss_dict = trainer.criterion(vad_pred, osd_pred, vad_labels, osd_labels)\n",
    "        eval_loss += loss_dict['total_loss'].item()\n",
    "        num_eval_batches += 1\n",
    "        \n",
    "        # Collecter pour m√©triques\n",
    "        all_vad_preds.append(vad_pred.cpu())\n",
    "        all_vad_targets.append(vad_labels.cpu())\n",
    "        all_osd_preds.append(osd_pred.cpu())\n",
    "        all_osd_targets.append(osd_labels.cpu())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"   Batch {batch_idx}/{len(val_loader)} √©valu√©\")\n",
    "\n",
    "# Calculer m√©triques d√©taill√©es\n",
    "print(\"\\nüìà Calcul des m√©triques d√©taill√©es...\")\n",
    "\n",
    "vad_preds = torch.cat(all_vad_preds, dim=0)\n",
    "vad_targets = torch.cat(all_vad_targets, dim=0)\n",
    "osd_preds = torch.cat(all_osd_preds, dim=0)\n",
    "osd_targets = torch.cat(all_osd_targets, dim=0)\n",
    "\n",
    "print(f\"   - Donn√©es √©valu√©es: {vad_preds.shape[0]} √©chantillons\")\n",
    "print(f\"   - Dur√©e totale: {vad_preds.shape[0] * vad_preds.shape[1] * 0.02:.1f} secondes\")\n",
    "\n",
    "# M√©triques principales\n",
    "metrics = metrics_computer.compute_metrics(vad_preds, osd_preds, vad_targets, osd_targets)\n",
    "\n",
    "print(\"\\nüéØ R√âSULTATS D'√âVALUATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üìä Loss finale: {eval_loss / num_eval_batches:.4f}\")\n",
    "print(f\"üìä DER (Diarization Error Rate): {metrics.get('der', 0):.2f}%\")\n",
    "print(f\"üìä F1 Score global: {metrics.get('f1_score', 0):.3f}\")\n",
    "print(f\"üìä Pr√©cision frame: {metrics.get('frame_precision', 0):.3f}\")\n",
    "print(f\"üìä Rappel frame: {metrics.get('frame_recall', 0):.3f}\")\n",
    "print(f\"üìä Jaccard Index: {metrics.get('jaccard_index', 0):.3f}\")\n",
    "\n",
    "# M√©triques OSD\n",
    "if 'osd_precision' in metrics:\n",
    "    print(f\"\\nüîÄ D√©tection de Chevauchement (OSD):\")\n",
    "    print(f\"   - Pr√©cision OSD: {metrics['osd_precision']:.3f}\")\n",
    "    print(f\"   - Rappel OSD: {metrics['osd_recall']:.3f}\")\n",
    "    print(f\"   - F1 OSD: {metrics['osd_f1']:.3f}\")\n",
    "\n",
    "# M√©triques par locuteur\n",
    "print(f\"\\nüë• M√©triques par Locuteur:\")\n",
    "for spk in range(config['model']['num_speakers']):\n",
    "    if f'speaker_{spk}_f1' in metrics:\n",
    "        print(f\"   Locuteur {spk}: F1={metrics[f'speaker_{spk}_f1']:.3f}, \"\n",
    "              f\"P={metrics[f'speaker_{spk}_precision']:.3f}, \"\n",
    "              f\"R={metrics[f'speaker_{spk}_recall']:.3f}\")\n",
    "\n",
    "# Sauvegarder les m√©triques\n",
    "metrics_file = RESULTS_DIR / 'evaluation_metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    # Convertir les tenseurs en listes pour JSON\n",
    "    json_metrics = {k: (v.item() if torch.is_tensor(v) else v) for k, v in metrics.items()}\n",
    "    json_metrics['eval_loss'] = eval_loss / num_eval_batches\n",
    "    json.dump(json_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ M√©triques sauv√©es: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìà 7. Visualisations et Analyses\n",
    "\n",
    "G√©n√©ration de graphiques et visualisations des r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Visualisations des r√©sultats\n",
    "print(\"üìä G√©n√©ration des visualisations...\")\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# === 1. Courbes d'entra√Ænement ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('R√©sultats d\\'Entra√Ænement - Speaker Diarization TCN', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Courbe de perte\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('√âvolution de la Perte')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les derni√®res epochs\n",
    "if len(train_losses) > 10:\n",
    "    start_idx = max(0, len(train_losses) - 20)\n",
    "    axes[0, 1].plot(range(start_idx, len(train_losses)), train_losses[start_idx:], \n",
    "                   label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(range(start_idx, len(val_losses)), val_losses[start_idx:], \n",
    "                   label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Convergence (Derni√®res Epochs)')\n",
    "else:\n",
    "    axes[0, 1].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('√âvolution de la Perte (Toutes Epochs)')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === 2. Analyse des pr√©dictions ===\n",
    "# Prendre un √©chantillon pour visualisation\n",
    "sample_idx = 0\n",
    "sample_vad_pred = vad_preds[sample_idx].numpy()  # [time, speakers]\n",
    "sample_vad_target = vad_targets[sample_idx].numpy()\n",
    "sample_osd_pred = osd_preds[sample_idx].numpy()  # [time]\n",
    "sample_osd_target = osd_targets[sample_idx].numpy()\n",
    "\n",
    "# Activit√© des locuteurs (pr√©dictions vs v√©rit√© terrain)\n",
    "time_frames = np.arange(len(sample_vad_pred)) * 0.02  # Conversion en secondes\n",
    "\n",
    "# Subplot pour VAD\n",
    "axes[1, 0].imshow(sample_vad_pred.T, aspect='auto', origin='lower', \n",
    "                 extent=[0, len(sample_vad_pred)*0.02, 0, 4], \n",
    "                 cmap='Blues', alpha=0.7)\n",
    "axes[1, 0].imshow(sample_vad_target.T, aspect='auto', origin='lower',\n",
    "                 extent=[0, len(sample_vad_target)*0.02, 0, 4],\n",
    "                 cmap='Reds', alpha=0.5)\n",
    "axes[1, 0].set_title('Activit√© VAD: Pr√©diction (Bleu) vs V√©rit√© (Rouge)')\n",
    "axes[1, 0].set_xlabel('Temps (s)')\n",
    "axes[1, 0].set_ylabel('Locuteur ID')\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "\n",
    "# Subplot pour OSD\n",
    "axes[1, 1].plot(time_frames, sample_osd_pred, label='Pr√©diction OSD', \n",
    "               color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(time_frames, sample_osd_target, label='V√©rit√© OSD', \n",
    "               color='red', linewidth=2, alpha=0.6)\n",
    "axes[1, 1].set_title('D√©tection de Chevauchement (OSD)')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_ylabel('Probabilit√© de Chevauchement')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "training_plot_path = RESULTS_DIR / 'training_results.png'\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Graphique d'entra√Ænement sauv√©: {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# === 3. Matrice de confusion pour classification ===\n",
    "if hasattr(trainer.model, 'use_speaker_classifier') and trainer.model.use_speaker_classifier:\n",
    "    print(\"\\nüìä Analyse de la classification des locuteurs...\")\n",
    "    \n",
    "    # Extraire les pr√©dictions de classification\n",
    "    all_speaker_preds = []\n",
    "    all_speaker_targets = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(trainer.device)\n",
    "            vad_labels = batch['vad_labels'].to(trainer.device)\n",
    "            \n",
    "            try:\n",
    "                vad_pred, osd_pred, embeddings, speaker_logits = trainer.model(features, return_embeddings=True)\n",
    "                \n",
    "                # Cr√©er des labels de locuteurs √† partir des VAD labels\n",
    "                speaker_targets = torch.argmax(vad_labels.sum(dim=1), dim=1)  # Locuteur le plus actif\n",
    "                speaker_preds = torch.argmax(speaker_logits, dim=1)\n",
    "                \n",
    "                all_speaker_preds.extend(speaker_preds.cpu().numpy())\n",
    "                all_speaker_targets.extend(speaker_targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur dans un batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_speaker_preds and all_speaker_targets:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(all_speaker_targets, all_speaker_preds)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=[f'Pred {i}' for i in range(4)],\n",
    "                   yticklabels=[f'True {i}' for i in range(4)])\n",
    "        plt.title('Matrice de Confusion - Classification des Locuteurs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Pr√©diction')\n",
    "        plt.ylabel('V√©rit√© Terrain')\n",
    "        \n",
    "        confusion_path = RESULTS_DIR / 'speaker_confusion_matrix.png'\n",
    "        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Rapport de classification\n",
    "        print(\"\\nüìã Rapport de Classification:\")\n",
    "        print(classification_report(all_speaker_targets, all_speaker_preds,\n",
    "                                  target_names=[f'Locuteur {i}' for i in range(4)],\n",
    "                                  digits=3))\n",
    "        \n",
    "        print(f\"‚úÖ Matrice de confusion sauv√©e: {confusion_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour la matrice de confusion\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Classificateur de locuteurs non activ√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_summary"
   },
   "outputs": [],
   "source": [
    "# === 4. R√©sum√© final et comparaisons ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSUM√â FINAL DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr√©er un r√©sum√© complet\n",
    "final_summary = {\n",
    "    'Configuration': {\n",
    "        'Architecture': f\"TCN {config['model']['hidden_channels']}\",\n",
    "        'Param√®tres': f\"{sum(p.numel() for p in trainer.model.parameters()):,}\",\n",
    "        'Batch Size': config['training']['batch_size'],\n",
    "        'Epochs': len(train_losses),\n",
    "        'Learning Rate': config['optimizer']['lr'],\n",
    "        'Device': str(trainer.device)\n",
    "    },\n",
    "    'R√©sultats Finaux': {\n",
    "        'Train Loss': f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\",\n",
    "        'Val Loss': f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\",\n",
    "        'Best Val Loss': f\"{best_val_loss:.4f}\",\n",
    "        'DER': f\"{metrics.get('der', 0):.2f}%\",\n",
    "        'F1 Score': f\"{metrics.get('f1_score', 0):.3f}\",\n",
    "        'Frame Precision': f\"{metrics.get('frame_precision', 0):.3f}\",\n",
    "        'Frame Recall': f\"{metrics.get('frame_recall', 0):.3f}\"\n",
    "    },\n",
    "    'Fichiers G√©n√©r√©s': {\n",
    "        'Meilleur Mod√®le': str(best_model_path) if best_model_path.exists() else \"Non sauv√©\",\n",
    "        'M√©triques': str(metrics_file),\n",
    "        'Graphiques': str(training_plot_path),\n",
    "        'Configuration': str(config_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage du r√©sum√©\n",
    "for section, items in final_summary.items():\n",
    "    print(f\"\\nüîπ {section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"   {key:.<25} {value}\")\n",
    "\n",
    "# Sauvegarde du r√©sum√©\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ R√©sum√© complet sauv√©: {summary_file}\")\n",
    "\n",
    "# === 5. Recommandations d'am√©lioration ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° RECOMMANDATIONS POUR AM√âLIORER LES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "current_der = metrics.get('der', 100)\n",
    "current_f1 = metrics.get('f1_score', 0)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if current_der > 25:\n",
    "    recommendations.append(\"üîß DER √©lev√©: Augmenter le nombre d'epochs ou r√©duire le learning rate\")\n",
    "if current_f1 < 0.7:\n",
    "    recommendations.append(\"üîß F1 faible: Essayer focal loss avec gamma plus √©lev√©\")\n",
    "if len(train_losses) < 20:\n",
    "    recommendations.append(\"‚è∞ Entra√Ænement court: Augmenter le nombre d'epochs\")\n",
    "if config['training']['batch_size'] < 16:\n",
    "    recommendations.append(\"üì¶ Batch size petit: Augmenter si possible pour am√©liorer la stabilit√©\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"üìä Utiliser plus de donn√©es AMI si disponibles\",\n",
    "    \"üéØ Affiner les hyperparam√®tres avec Optuna\",\n",
    "    \"üîÑ Essayer l'ensemble de mod√®les\",\n",
    "    \"üìà Impl√©menter la validation crois√©e\",\n",
    "    \"üß† Tester diff√©rentes architectures d'attention\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations[:7], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Tous les r√©sultats sont sauv√©s dans: {RESULTS_DIR}\")\n",
    "print(f\"üß† Meilleur mod√®le disponible dans: {MODEL_DIR}\")\n",
    "\n",
    "if config.get('use_wandb', False):\n",
    "    print(f\"üìä Logs d√©taill√©s disponibles sur Weights & Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Prochaines √âtapes\n",
    "\n",
    "### üìù Pour Continuer l'Am√©lioration:\n",
    "\n",
    "1. **üìä Donn√©es**: T√©l√©charger le corpus AMI complet\n",
    "2. **‚öôÔ∏è Hyperparam√®tres**: Optimiser avec Optuna\n",
    "3. **üéØ Architecture**: Tester diff√©rentes tailles de mod√®le\n",
    "4. **üìà Ensembles**: Combiner plusieurs mod√®les\n",
    "5. **üîÑ Post-traitement**: Am√©liorer la segmentation finale\n",
    "\n",
    "### üíæ Fichiers G√©n√©r√©s:\n",
    "- `models/checkpoints/best_model.pth` - Meilleur mod√®le\n",
    "- `results/evaluation_metrics.json` - M√©triques d√©taill√©es\n",
    "- `results/training_results.png` - Graphiques d'entra√Ænement\n",
    "- `results/training_summary.json` - R√©sum√© complet\n",
    "\n",
    "### üéØ Objectifs de Performance:\n",
    "- **DER < 20%** sur AMI corpus (√©tat de l'art: ~15-18%)\n",
    "- **F1 > 0.8** pour la d√©tection d'activit√©\n",
    "- **Temps r√©el** pour l'inf√©rence\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations! Vous avez entra√Æn√© avec succ√®s un syst√®me de diarization moderne avec toutes les optimisations avanc√©es!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
